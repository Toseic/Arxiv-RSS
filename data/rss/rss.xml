<?xml version="1.0" ?>
<rss>
 <channel>
  <title>arxiv-rss</title>
  <link></link>
  <description></description>
  <docs></docs>
  <language>en-us</language>
  <lastBuildDate>Wed, 02 Jul 2025 03:14:31 </lastBuildDate>
  <managingEditor></managingEditor>
  <pubDate>Wed, 02 Jul 2025 03:14:31 </pubDate>
  <item>
   <title>Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models</title>
   <link>https://arxiv.org/abs/2506.22500</link>
   <description>Surgical risk identification is critical for patient safety and reducing preventable medical errors. While multimodal large language models (MLLMs) show promise for automated operating room (OR) risk detection, they often exhibit visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety violations despite understanding textual rules. To address this, we introduce a dataset comprising over 34,000 synthetic images generated by diffusion models, depicting operating room scenes containing entities that violate established safety rules. These images were created to alleviate data scarcity and examine MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated images that serve as a gold-standard reference for validation. This comprehensive dataset, spanning diverse perspectives, stages, and configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC significantly improves MLLMs' detection of trained conflict entities and generalizes well to new viewpoints for these entities, but performance on untrained entity types remains poor, highlighting learning specificity and the need for comprehensive training. The main contributions of this work include: (1) a data generation methodology tailored for rule-violation scenarios; (2) the release of the OR-VSKC dataset and its associated benchmark as open-source resources; and (3) an empirical analysis of violation-sensitive knowledge consistency in representative MLLMs. The dataset and appendix are available at https://github.com/zgg2577/VS-KC.</description>
   <guid>oai:arXiv.org:2506.22500v1</guid>
   <category>cs.CV</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Weiyi Zhao, Xiaoyu Tan, Liang Liu, Sijia Li, Youwei Song, Xihe Qiu</creator>
  </item>
  <item>
   <title>Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation</title>
   <link>https://arxiv.org/abs/2506.23120</link>
   <description>Recent advances in point cloud perception have demonstrated remarkable progress in scene understanding through vision-language alignment leveraging large language models (LLMs). However, existing methods may still encounter challenges in handling complex instructions that require accurate spatial reasoning, even if the 3D point cloud data provides detailed spatial cues such as size and position for identifying the targets. To tackle this issue, we propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based segmentation framework. The framework emulates human cognitive processes by decomposing spatial reasoning into two sequential stages: first identifying relevant elements, then processing instructions guided by their associated visual priors. Furthermore, acknowledging the inadequacy of existing datasets in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based segmentation dataset comprising 25,185 training samples and 3,966 validation samples with precise annotations. Both quantitative and qualitative experiments demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud perception with stronger spatial reasoning capabilities, and we hope that they can serve as a new baseline and benchmark for future work.</description>
   <guid>oai:arXiv.org:2506.23120v1</guid>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Zhenhua Ning, Zhuotao Tian, Shaoshuai Shi, Guangming Lu, Daojing He, Wenjie Pei, Li Jiang</creator>
  </item>
  <item>
   <title>Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving</title>
   <link>https://arxiv.org/abs/2506.23523</link>
   <description>Traditional vision-based autonomous driving systems often face difficulties in navigating complex environments when relying solely on single-image inputs. To overcome this limitation, incorporating temporal data such as past image frames or steering sequences, has proven effective in enhancing robustness and adaptability in challenging scenarios. While previous high-performance methods exist, they often rely on resource-intensive fusion networks, making them impractical for training and unsuitable for federated learning. To address these challenges, we propose lightweight temporal transformer decomposition, a method that processes sequential image frames and temporal steering data by breaking down large attention maps into smaller matrices. This approach reduces model complexity, enabling efficient weight updates for convergence and real-time predictions while leveraging temporal information to enhance autonomous driving performance. Intensive experiments on three datasets demonstrate that our method outperforms recent approaches by a clear margin while achieving real-time performance. Additionally, real robot experiments further confirm the effectiveness of our method.</description>
   <guid>oai:arXiv.org:2506.23523v1</guid>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Tuong Do, Binh X. Nguyen, Quang D. Tran, Erman Tjiputra, Te-Chuan Chiu, Anh Nguyen</creator>
  </item>
  <item>
   <title>FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation</title>
   <link>https://arxiv.org/abs/2506.22580</link>
   <description>Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model's effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \textit{client-adaptive momentum} terms derived from each client's loss reduction during local training, as well as a \textit{personalized dampening factor} to curb overfitting. We further introduce a novel \textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at https://github.com/siomvas/FedCLAM.</description>
   <guid>oai:arXiv.org:2506.22580v1</guid>
   <category>eess.IV</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Vasilis Siomos, Jonathan Passerat-Palmbach, Giacomo Tarroni</creator>
  </item>
  <item>
   <title>Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation</title>
   <link>https://arxiv.org/abs/2506.23334</link>
   <description>Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.</description>
   <guid>oai:arXiv.org:2506.23334v1</guid>
   <category>eess.IV</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Hongyi Pan, Ziliang Hong, Gorkem Durak, Ziyue Xu, Ulas Bagci</creator>
  </item>
  <item>
   <title>FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization</title>
   <link>https://arxiv.org/abs/2506.23516</link>
   <description>Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios.</description>
   <guid>oai:arXiv.org:2506.23516v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee</creator>
  </item>
  <item>
   <title>MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI</title>
   <link>https://arxiv.org/abs/2506.23563</link>
   <description>Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at https://github.com/HJYao00/MMReason.</description>
   <guid>oai:arXiv.org:2506.23563v1</guid>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao</creator>
  </item>
  <item>
   <title>Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos</title>
   <link>https://arxiv.org/abs/2506.23759</link>
   <description>Surgical instrument segmentation under Federated Learning (FL) is a promising direction, which enables multiple surgical sites to collaboratively train the model without centralizing datasets. However, there exist very limited FL works in surgical data science, and FL methods for other modalities do not consider inherent characteristics in surgical domain: i) different scenarios show diverse anatomical backgrounds while highly similar instrument representation; ii) there exist surgical simulators which promote large-scale synthetic data generation with minimal efforts. In this paper, we propose a novel Personalized FL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST), which wisely leverages surgical domain knowledge during both local-site and global-server training to boost segmentation. Concretely, our model embraces a Representation Separation and Cooperation (RSC) mechanism in local-site training, which decouples the query embedding layer to be trained privately, to encode respective backgrounds. Meanwhile, other parameters are optimized globally to capture the consistent representations of instruments, including the temporal layer to capture similar motion patterns. A textual-guided channel selection is further designed to highlight site-specific features, facilitating model adapta tion to each site. Moreover, in global-server training, we propose Synthesis-based Explicit Representation Quantification (SERQ), which defines an explicit representation target based on synthetic data to synchronize the model convergence during fusion for improving model generalization.</description>
   <guid>oai:arXiv.org:2506.23759v1</guid>
   <category>eess.IV</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Zheng Fang, Xiaoming Qi, Chun-Mei Feng, Jialun Pei, Weixin Si, Yueming Jin</creator>
  </item>
  <item>
   <title>FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</title>
   <link>https://arxiv.org/abs/2410.09432</link>
   <description>Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in federated learning environments, where data is distributed across multiple clients, presents unique challenges. Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates. To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pretrained frozen weight matrix. Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA's efficiency. We evaluate the method on various models across arithmetic reasoning, commonsense reasoning, natural language understanding and natural language generation tasks, showing consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. Our method's simplicity, efficiency, and broad applicability position it as a promising solution for accurate and effective federated fine-tuning of foundation models. Our code is publicly available at https://github.com/RaghavSinghal10/fedex-lora.</description>
   <guid>oai:arXiv.org:2410.09432v4</guid>
   <category>cs.DC</category>
   <category>cs.CL</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Raghav Singhal, Kaustubh Ponkshe, Praneeth Vepakomma</creator>
  </item>
  <item>
   <title>Efficiently Serving Large Multimodal Models Using EPD Disaggregation</title>
   <link>https://arxiv.org/abs/2501.05460</link>
   <description>Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.</description>
   <guid>oai:arXiv.org:2501.05460v4</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan</creator>
  </item>
  <item>
   <title>MedSegNet10: A Publicly Accessible Network Repository for Split Federated Medical Image Segmentation</title>
   <link>https://arxiv.org/abs/2503.20830</link>
   <description>Machine Learning (ML) and Deep Learning (DL) have shown significant promise in healthcare, particularly in medical image segmentation, which is crucial for accurate disease diagnosis and treatment planning. Despite their potential, challenges such as data privacy concerns, limited annotated data, and inadequate training data persist. Decentralized learning approaches such as federated learning (FL), split learning (SL), and split federated learning (SplitFed/SFL) address these issues effectively. This paper introduces &quot;MedSegNet10,&quot; a publicly accessible repository designed for medical image segmentation using split-federated learning. MedSegNet10 provides a collection of pre-trained neural network architectures optimized for various medical image types, including microscopic images of human blastocysts, dermatoscopic images of skin lesions, and endoscopic images of lesions, polyps, and ulcers, with applications extending beyond these examples. By leveraging SplitFed's benefits, MedSegNet10 allows collaborative training on privately stored, horizontally split data, ensuring privacy and integrity. This repository supports researchers, practitioners, trainees, and data scientists, aiming to advance medical image segmentation while maintaining patient data privacy. The repository is available at: https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuX (password upon request to the authors).</description>
   <guid>oai:arXiv.org:2503.20830v2</guid>
   <category>eess.IV</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Chamani Shiranthika, Zahra Hafezi Kafshgari, Hadi Hadizadeh, Parvaneh Saeedi</creator>
  </item>
  <item>
   <title>Decoding Federated Learning: The FedNAM+ Conformal Revolution</title>
   <link>https://arxiv.org/abs/2506.17872</link>
   <description>Federated learning has significantly advanced distributed training of machine learning models across decentralized data sources. However, existing frameworks often lack comprehensive solutions that combine uncertainty quantification, interpretability, and robustness. To address this, we propose FedNAM+, a federated learning framework that integrates Neural Additive Models (NAMs) with a novel conformal prediction method to enable interpretable and reliable uncertainty estimation. Our method introduces a dynamic level adjustment technique that utilizes gradient-based sensitivity maps to identify key input features influencing predictions. This facilitates both interpretability and pixel-wise uncertainty estimates. Unlike traditional interpretability methods such as LIME and SHAP, which do not provide confidence intervals, FedNAM+ offers visual insights into prediction reliability. We validate our approach through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with transparent uncertainty measures. Visual analysis highlights variable uncertainty intervals, revealing low-confidence regions where model performance can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+ delivers efficient and global uncertainty estimates with reduced computational overhead, making it particularly suitable for federated learning scenarios. Overall, FedNAM+ provides a robust, interpretable, and computationally efficient framework that enhances trust and transparency in decentralized predictive modeling.</description>
   <guid>oai:arXiv.org:2506.17872v2</guid>
   <category>cs.LG</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Sree Bhargavi Balija, Amitash Nanda, Debashis Sahoo</creator>
  </item>
  <item>
   <title>Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models</title>
   <link>https://arxiv.org/abs/2501.14406</link>
   <description>Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Adaptive Rank Allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.</description>
   <guid>oai:arXiv.org:2501.14406v3</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</creator>
  </item>
  <item>
   <title>XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs</title>
   <link>https://arxiv.org/abs/2506.23325</link>
   <description>Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at https://github.com/gyt1145028706/XY-Tokenizer.</description>
   <guid>oai:arXiv.org:2506.23325v1</guid>
   <category>cs.SD</category>
   <category>cs.AI</category>
   <category>eess.AS</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu</creator>
  </item>
  <item>
   <title>Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC</title>
   <link>https://arxiv.org/abs/2506.24045</link>
   <description>The proliferation of agentic Large Language Models (LLMs) on personal devices introduces a new class of workloads characterized by a dichotomy of objectives. Reactive tasks, initiated by users, demand immediate, low-latency responses, while proactive tasks operate invisibly and prioritize throughput. Existing on-device LLM engines, designed for isolated inferences, fail to efficiently manage these concurrent and conflicting requests on consumer-grade heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces Agent.xpu, an efficient serving system for agentic LLM workloads on memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu first constructs a heterogeneous execution graph, which fuses and chunks model kernels for affinity-guided, elastic accelerator mapping with predictive kernel annotation. At runtime, its online scheduler enables fine-grained, kernel-level preemption to guarantee the responsiveness of reactive tasks. To maximize SoC utilization, it adopts slack-aware kernel backfill to opportunistically append proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves 4.6$\times$ lower latency for reactive tasks and sustains 1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to state-of-the-art inference engines.</description>
   <guid>oai:arXiv.org:2506.24045v1</guid>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Xinming Wei, Jiahao Zhang, Haoran Li, Jiayu Chen, Rui Qu, Maoliang Li, Xiang Chen, Guojie Luo</creator>
  </item>
  <item>
   <title>FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model</title>
   <link>https://arxiv.org/abs/2506.23210</link>
   <description>Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and enables overcoming the catastrophic forgetting issue in each round by utilizing a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and low computing cost.</description>
   <guid>oai:arXiv.org:2506.23210v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.DC</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Taehwan Yoon, Bongjun Choi</creator>
  </item>
  <item>
   <title>Detect \&amp; Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning</title>
   <link>https://arxiv.org/abs/2506.23583</link>
   <description>Federated learning with secure aggregation enables private and collaborative learning from decentralised data without leaking sensitive client information. However, secure aggregation also complicates the detection of malicious client behaviour and the evaluation of individual client contributions to the learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et al.) were proposed for contribution evaluation (CE) and misbehaviour detection (MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance on the random selection of clients in each training round, while FedGT lacks the CE ability. In this work, we combine the strengths of QI and FedGT to achieve both robust MD and accurate CE. Our experiments demonstrate superior performance compared to using either method independently.</description>
   <guid>oai:arXiv.org:2506.23583v1</guid>
   <category>cs.CR</category>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</rights>
   <creator>Marvin Xhemrishi, Alexandre Graell i Amat, Bal\'azs Pej\'o</creator>
  </item>
  <item>
   <title>Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction</title>
   <link>https://arxiv.org/abs/2506.23836</link>
   <description>We consider centralized distributed optimization in the classical federated learning setup, where $n$ workers jointly find an $\varepsilon$-stationary point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access only to unbiased stochastic gradients with variance $\sigma^2$. Each worker requires at most $h$ seconds to compute a stochastic gradient, and the communication times from the server to the workers and from the workers to the server are $\tau_{s}$ and $\tau_{w}$ seconds per coordinate, respectively. One of the main motivations for distributed optimization is to achieve scalability with respect to $n$. For instance, it is well known that the distributed version of SGD has a variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{n \varepsilon^2},$ which improves with the number of workers $n,$ where $\Delta = f(x^0) - f^*,$ and $x^0 \in R^d$ is the starting point. Similarly, using unbiased sparsification compressors, it is possible to reduce both the variance-dependent runtime term and the communication runtime term. However, once we account for the communication from the server to the workers $\tau_{s}$, we prove that it becomes infeasible to design a method using unbiased random sparsification compressors that scales both the server-side communication runtime term $\tau_{s} d \frac{L \Delta}{\varepsilon}$ and the variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{\varepsilon^2},$ better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case, where all workers access the same distribution. To establish this result, we construct a new &quot;worst-case&quot; function and develop a new lower bound framework that reduces the analysis to the concentration of a random sum, for which we prove a concentration bound. These results reveal fundamental limitations in scaling distributed optimization, even under the homogeneous assumption.</description>
   <guid>oai:arXiv.org:2506.23836v1</guid>
   <category>math.OC</category>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Alexander Tyurin</creator>
  </item>
  <item>
   <title>FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</title>
   <link>https://arxiv.org/abs/2410.09432</link>
   <description>Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in federated learning environments, where data is distributed across multiple clients, presents unique challenges. Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates. To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pretrained frozen weight matrix. Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA's efficiency. We evaluate the method on various models across arithmetic reasoning, commonsense reasoning, natural language understanding and natural language generation tasks, showing consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. Our method's simplicity, efficiency, and broad applicability position it as a promising solution for accurate and effective federated fine-tuning of foundation models. Our code is publicly available at https://github.com/RaghavSinghal10/fedex-lora.</description>
   <guid>oai:arXiv.org:2410.09432v4</guid>
   <category>cs.DC</category>
   <category>cs.CL</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Raghav Singhal, Kaustubh Ponkshe, Praneeth Vepakomma</creator>
  </item>
  <item>
   <title>Efficiently Serving Large Multimodal Models Using EPD Disaggregation</title>
   <link>https://arxiv.org/abs/2501.05460</link>
   <description>Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.</description>
   <guid>oai:arXiv.org:2501.05460v4</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan</creator>
  </item>
  <item>
   <title>Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models</title>
   <link>https://arxiv.org/abs/2501.14406</link>
   <description>Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Adaptive Rank Allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.</description>
   <guid>oai:arXiv.org:2501.14406v3</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</creator>
  </item>
  <item>
   <title>Scaling Human Judgment in Community Notes with LLMs</title>
   <link>https://arxiv.org/abs/2506.24118</link>
   <description>This paper argues for a new paradigm for Community Notes in the LLM era: an open ecosystem where both humans and LLMs can write notes, and the decision of which notes are helpful enough to show remains in the hands of humans. This approach can accelerate the delivery of notes, while maintaining trust and legitimacy through Community Notes' foundational principle: A community of diverse human raters collectively serve as the ultimate evaluator and arbiter of what is helpful. Further, the feedback from this diverse community can be used to improve LLMs' ability to produce accurate, unbiased, broadly helpful notes--what we term Reinforcement Learning from Community Feedback (RLCF). This becomes a two-way street: LLMs serve as an asset to humans--helping deliver context quickly and with minimal effort--while human feedback, in turn, enhances the performance of LLMs. This paper describes how such a system can work, its benefits, key new risks and challenges it introduces, and a research agenda to solve those challenges and realize the potential of this approach.</description>
   <guid>oai:arXiv.org:2506.24118v1</guid>
   <category>cs.CY</category>
   <category>cs.SI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Haiwen Li, Soham De, Manon Revel, Andreas Haupt, Brad Miller, Keith Coleman, Jay Baxter, Martin Saveski, Michiel A. Bakker</creator>
  </item>
  <item>
   <title>Hallucination Detection with Small Language Models</title>
   <link>https://arxiv.org/abs/2506.22486</link>
   <description>Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating &quot;Yes&quot; tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.</description>
   <guid>oai:arXiv.org:2506.22486v1</guid>
   <category>cs.CL</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Ming Cheung</creator>
  </item>
  <item>
   <title>MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI</title>
   <link>https://arxiv.org/abs/2506.23563</link>
   <description>Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at https://github.com/HJYao00/MMReason.</description>
   <guid>oai:arXiv.org:2506.23563v1</guid>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao</creator>
  </item>
  <item>
   <title>Interpretable LLM-based Table Question Answering</title>
   <link>https://arxiv.org/abs/2412.12386</link>
   <description>Interpretability in Table Question Answering (Table QA) is critical, especially in high-stakes domains like finance and healthcare. While recent Table QA approaches based on Large Language Models (LLMs) achieve high accuracy, they often produce ambiguous explanations of how answers are derived.
  We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's decision-making process interpretable. POS decomposes a question into a sequence of atomic steps, each directly translated into an executable SQL command on the table, thereby ensuring that every intermediate result is transparent. Through extensive experiments, we show that: First, POS generates the highest-quality explanations among compared methods, which markedly improves the users' ability to simulate and verify the model's decisions. Second, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and FeTaQA), POS achieves QA accuracy that is competitive to existing methods, while also offering greater efficiency-requiring significantly fewer LLM calls and table database queries (up to 25x fewer)-and more robust performance on large-sized tables. Finally, we observe high agreement (up to 90.59% in forward simulation) between LLMs and human users when making decisions based on the same explanations, suggesting that LLMs could serve as an effective proxy for humans in evaluating Table QA explanations.</description>
   <guid>oai:arXiv.org:2412.12386v3</guid>
   <category>cs.CL</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Giang Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, Freddy Lecue</creator>
  </item>
  <item>
   <title>FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</title>
   <link>https://arxiv.org/abs/2410.09432</link>
   <description>Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in federated learning environments, where data is distributed across multiple clients, presents unique challenges. Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates. To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pretrained frozen weight matrix. Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA's efficiency. We evaluate the method on various models across arithmetic reasoning, commonsense reasoning, natural language understanding and natural language generation tasks, showing consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. Our method's simplicity, efficiency, and broad applicability position it as a promising solution for accurate and effective federated fine-tuning of foundation models. Our code is publicly available at https://github.com/RaghavSinghal10/fedex-lora.</description>
   <guid>oai:arXiv.org:2410.09432v4</guid>
   <category>cs.DC</category>
   <category>cs.CL</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Raghav Singhal, Kaustubh Ponkshe, Praneeth Vepakomma</creator>
  </item>
  <item>
   <title>Scaling Human Judgment in Community Notes with LLMs</title>
   <link>https://arxiv.org/abs/2506.24118</link>
   <description>This paper argues for a new paradigm for Community Notes in the LLM era: an open ecosystem where both humans and LLMs can write notes, and the decision of which notes are helpful enough to show remains in the hands of humans. This approach can accelerate the delivery of notes, while maintaining trust and legitimacy through Community Notes' foundational principle: A community of diverse human raters collectively serve as the ultimate evaluator and arbiter of what is helpful. Further, the feedback from this diverse community can be used to improve LLMs' ability to produce accurate, unbiased, broadly helpful notes--what we term Reinforcement Learning from Community Feedback (RLCF). This becomes a two-way street: LLMs serve as an asset to humans--helping deliver context quickly and with minimal effort--while human feedback, in turn, enhances the performance of LLMs. This paper describes how such a system can work, its benefits, key new risks and challenges it introduces, and a research agenda to solve those challenges and realize the potential of this approach.</description>
   <guid>oai:arXiv.org:2506.24118v1</guid>
   <category>cs.CY</category>
   <category>cs.SI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Haiwen Li, Soham De, Manon Revel, Andreas Haupt, Brad Miller, Keith Coleman, Jay Baxter, Martin Saveski, Michiel A. Bakker</creator>
  </item>
  <item>
   <title>Asymptotically Optimal Secure Aggregation for Wireless Federated Learning with Multiple Servers</title>
   <link>https://arxiv.org/abs/2506.23680</link>
   <description>In this paper, we investigate the transmission latency of the secure aggregation problem in a \emph{wireless} federated learning system with multiple curious servers. We propose a privacy-preserving coded aggregation scheme where the servers can not infer any information about the distributed users' local gradients, nor the aggregation value. In our scheme, each user encodes its local gradient into $\sK$ confidential messages intended exclusively for different servers using a multi-secret sharing method, and each server forwards the summation of the received confidential messages, while the users sequentially employ artificial noise alignment techniques to facilitate secure transmission. Through these summations, the user can recover the aggregation of all local gradients. We prove the privacy guarantee in the information-theoretic sense and characterize the uplink and downlink communication latency measured by \emph{normalized delivery time} (NDT), both of which decrease monotonically with the number of servers $\sK$ while increasing over most of the range of the number of users $\sM$. Finally, we establish a lower bound on the NDT of the considered system and theoretically prove that the scheme achieves the optimal uplink and downlink NDT under the conditions $\sK \gg \sM \gg 0$ and $\sK \gg \sM$, respectively. For arbitrary $\sK$ and $\sM$, the proposed scheme achieves the optimal uplink NDT within a multiplicative gap of $4$.</description>
   <guid>oai:arXiv.org:2506.23680v1</guid>
   <category>cs.IT</category>
   <category>math.IT</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Zhenhao Huang, Kai Liang, Yuanming Shi, Songze Li, Youlong Wu</creator>
  </item>
  <item>
   <title>P$^2$U: Progressive Precision Update For Efficient Model Distribution</title>
   <link>https://arxiv.org/abs/2506.22871</link>
   <description>Efficient model distribution is becoming increasingly critical in bandwidth-constrained environments. In this paper, we propose a simple yet effective approach called Progressive Precision Update (P$^2$U) to address this problem. Instead of transmitting the original high-precision model, P$^2$U transmits a lower-bit precision model, coupled with a model update representing the difference between the original high-precision model and the transmitted low precision version. With extensive experiments on various model architectures, ranging from small models ($1 - 6$ million parameters) to a large model (more than $100$ million parameters) and using three different data sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U consistently achieves better tradeoff between accuracy, bandwidth usage and latency. Moreover, we show that when bandwidth or startup time is the priority, aggressive quantization (e.g., 4-bit) can be used without severely compromising performance. These results establish P$^2$U as an effective and practical solution for scalable and efficient model distribution in low-resource settings, including federated learning, edge computing, and IoT deployments. Given that P$^2$U complements existing compression techniques and can be implemented alongside any compression method, e.g., sparsification, quantization, pruning, etc., the potential for improvement is even greater.</description>
   <guid>oai:arXiv.org:2506.22871v1</guid>
   <category>cs.LG</category>
   <category>cs.MM</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</rights>
   <creator>Homayun Afrabandpey, Hamed Rezazadegan Tavakoli</creator>
  </item>
  <item>
   <title>MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI</title>
   <link>https://arxiv.org/abs/2506.23563</link>
   <description>Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at https://github.com/HJYao00/MMReason.</description>
   <guid>oai:arXiv.org:2506.23563v1</guid>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao</creator>
  </item>
  <item>
   <title>Hallucination Detection with Small Language Models</title>
   <link>https://arxiv.org/abs/2506.22486</link>
   <description>Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating &quot;Yes&quot; tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.</description>
   <guid>oai:arXiv.org:2506.22486v1</guid>
   <category>cs.CL</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Ming Cheung</creator>
  </item>
  <item>
   <title>Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models</title>
   <link>https://arxiv.org/abs/2506.22500</link>
   <description>Surgical risk identification is critical for patient safety and reducing preventable medical errors. While multimodal large language models (MLLMs) show promise for automated operating room (OR) risk detection, they often exhibit visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety violations despite understanding textual rules. To address this, we introduce a dataset comprising over 34,000 synthetic images generated by diffusion models, depicting operating room scenes containing entities that violate established safety rules. These images were created to alleviate data scarcity and examine MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated images that serve as a gold-standard reference for validation. This comprehensive dataset, spanning diverse perspectives, stages, and configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC significantly improves MLLMs' detection of trained conflict entities and generalizes well to new viewpoints for these entities, but performance on untrained entity types remains poor, highlighting learning specificity and the need for comprehensive training. The main contributions of this work include: (1) a data generation methodology tailored for rule-violation scenarios; (2) the release of the OR-VSKC dataset and its associated benchmark as open-source resources; and (3) an empirical analysis of violation-sensitive knowledge consistency in representative MLLMs. The dataset and appendix are available at https://github.com/zgg2577/VS-KC.</description>
   <guid>oai:arXiv.org:2506.22500v1</guid>
   <category>cs.CV</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Weiyi Zhao, Xiaoyu Tan, Liang Liu, Sijia Li, Youwei Song, Xihe Qiu</creator>
  </item>
  <item>
   <title>FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation</title>
   <link>https://arxiv.org/abs/2506.22580</link>
   <description>Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model's effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \textit{client-adaptive momentum} terms derived from each client's loss reduction during local training, as well as a \textit{personalized dampening factor} to curb overfitting. We further introduce a novel \textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at https://github.com/siomvas/FedCLAM.</description>
   <guid>oai:arXiv.org:2506.22580v1</guid>
   <category>eess.IV</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Vasilis Siomos, Jonathan Passerat-Palmbach, Giacomo Tarroni</creator>
  </item>
  <item>
   <title>FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model</title>
   <link>https://arxiv.org/abs/2506.23210</link>
   <description>Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and enables overcoming the catastrophic forgetting issue in each round by utilizing a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and low computing cost.</description>
   <guid>oai:arXiv.org:2506.23210v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.DC</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Taehwan Yoon, Bongjun Choi</creator>
  </item>
  <item>
   <title>XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs</title>
   <link>https://arxiv.org/abs/2506.23325</link>
   <description>Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at https://github.com/gyt1145028706/XY-Tokenizer.</description>
   <guid>oai:arXiv.org:2506.23325v1</guid>
   <category>cs.SD</category>
   <category>cs.AI</category>
   <category>eess.AS</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu</creator>
  </item>
  <item>
   <title>Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation</title>
   <link>https://arxiv.org/abs/2506.23334</link>
   <description>Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.</description>
   <guid>oai:arXiv.org:2506.23334v1</guid>
   <category>eess.IV</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Hongyi Pan, Ziliang Hong, Gorkem Durak, Ziyue Xu, Ulas Bagci</creator>
  </item>
  <item>
   <title>FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization</title>
   <link>https://arxiv.org/abs/2506.23516</link>
   <description>Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios.</description>
   <guid>oai:arXiv.org:2506.23516v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee</creator>
  </item>
  <item>
   <title>Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning</title>
   <link>https://arxiv.org/abs/2310.11594</link>
   <description>The delicate equilibrium between user privacy and the ability to unleash the potential of distributed data is an important concern. Federated learning, which enables the training of collaborative models without sharing of data, has emerged as a privacy-centric solution. This approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data into the training process, as well as evasion attacks that aim to induce misclassifications at test time. Our research investigates the intersection of adversarial training, a common defense method against evasion attacks, and backdoor attacks within federated learning. We introduce Adversarial Robustness Unhardening (ARU), which is employed by a subset of adversarial clients to intentionally undermine model robustness during federated training, rendering models susceptible to a broader range of evasion attacks. We present extensive experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our results show that ARU can substantially undermine adversarial training's ability to harden models against test-time evasion attacks, and that adversaries employing ARU can even evade robust aggregation defenses that often neutralize poisoning or backdoor attacks.</description>
   <guid>oai:arXiv.org:2310.11594v3</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong</creator>
  </item>
  <item>
   <title>Efficiently Serving Large Multimodal Models Using EPD Disaggregation</title>
   <link>https://arxiv.org/abs/2501.05460</link>
   <description>Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.</description>
   <guid>oai:arXiv.org:2501.05460v4</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan</creator>
  </item>
  <item>
   <title>Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models</title>
   <link>https://arxiv.org/abs/2501.14406</link>
   <description>Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Adaptive Rank Allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.</description>
   <guid>oai:arXiv.org:2501.14406v3</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</creator>
  </item>
  <item>
   <title>FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments</title>
   <link>https://arxiv.org/abs/2503.19564</link>
   <description>As artificial intelligence systems increasingly operate in Real-world environments, the integration of multi-modal data sources such as vision, language, and audio presents both unprecedented opportunities and critical challenges for achieving trustworthy intelligence. In this paper, we propose a novel framework that unifies federated learning with explainable multi-modal reasoning to ensure trustworthiness in decentralized, dynamic settings. Our approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence), leverages cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration to address challenges posed by data heterogeneity, modality imbalance, and out-of-distribution generalization. Through rigorous evaluation across federated multi-modal benchmarks involving vision-language tasks, we demonstrate improved performance in both accuracy and interpretability while reducing vulnerabilities to adversarial and spurious correlations. Further, we introduce a novel trust score aggregation method to quantify global model reliability under dynamic client participation. Our findings pave the way toward developing robust, interpretable, and socially responsible AI systems in Real-world environments.</description>
   <guid>oai:arXiv.org:2503.19564v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Sree Bhargavi Balija</creator>
  </item>
  <item>
   <title>VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption</title>
   <link>https://arxiv.org/abs/2506.12846</link>
   <description>Federated learning is a promising distributed learning paradigm that enables collaborative model training without exposing local client data, thereby protect data privacy. However, it also brings new threats and challenges. The advancement of model inversion attacks has rendered the plaintext transmission of local models insecure, while the distributed nature of federated learning makes it particularly vulnerable to attacks raised by malicious clients. To protect data privacy and prevent malicious client attacks, this paper proposes a privacy-preserving federated learning framework based on verifiable functional encryption, without a non-colluding dual-server setup or additional trusted third-party. Specifically, we propose a novel decentralized verifiable functional encryption (DVFE) scheme that enables the verification of specific relationships over multi-dimensional ciphertexts. This scheme is formally treated, in terms of definition, security model and security proof. Furthermore, based on the proposed DVFE scheme, we design a privacy-preserving federated learning framework VFEFL that incorporates a novel robust aggregation rule to detect malicious clients, enabling the effective training of high-accuracy models under adversarial settings. Finally, we provide formal analysis and empirical evaluation of the proposed schemes. The results demonstrate that our approach achieves the desired privacy protection, robustness, verifiability and fidelity, while eliminating the reliance on non-colluding dual-server settings or trusted third parties required by existing methods.</description>
   <guid>oai:arXiv.org:2506.12846v2</guid>
   <category>cs.CR</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Nina Cai, Jinguang Han, Weizhi Meng</creator>
  </item>
  <item>
   <title>Privacy-Preserving Methods for Bug Severity Prediction</title>
   <link>https://arxiv.org/abs/2506.22752</link>
   <description>Bug severity prediction is a critical task in software engineering as it enables more efficient resource allocation and prioritization in software maintenance. While AI-based analyses and models significantly require access to extensive datasets, industrial applications face challenges due to data-sharing constraints and the limited availability of labeled data. In this study, we investigate method-level bug severity prediction using source code metrics and Large Language Models (LLMs) with two widely used datasets. We compare the performance of models trained using centralized learning, federated learning, and synthetic data generation. Our experimental results, obtained using two widely recognized software defect datasets, indicate that models trained with federated learning and synthetic data achieve comparable results to centrally trained models without data sharing. Our finding highlights the potential of privacy-preserving approaches such as federated learning and synthetic data generation to enable effective bug severity prediction in industrial context where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository: https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.</description>
   <guid>oai:arXiv.org:2506.22752v1</guid>
   <category>cs.SE</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Havvanur Dervi\c{s}o\u{g}lu, Ru\c{s}en Halepmollas{\i}, Elif Eyvaz</creator>
  </item>
  <item>
   <title>P$^2$U: Progressive Precision Update For Efficient Model Distribution</title>
   <link>https://arxiv.org/abs/2506.22871</link>
   <description>Efficient model distribution is becoming increasingly critical in bandwidth-constrained environments. In this paper, we propose a simple yet effective approach called Progressive Precision Update (P$^2$U) to address this problem. Instead of transmitting the original high-precision model, P$^2$U transmits a lower-bit precision model, coupled with a model update representing the difference between the original high-precision model and the transmitted low precision version. With extensive experiments on various model architectures, ranging from small models ($1 - 6$ million parameters) to a large model (more than $100$ million parameters) and using three different data sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U consistently achieves better tradeoff between accuracy, bandwidth usage and latency. Moreover, we show that when bandwidth or startup time is the priority, aggressive quantization (e.g., 4-bit) can be used without severely compromising performance. These results establish P$^2$U as an effective and practical solution for scalable and efficient model distribution in low-resource settings, including federated learning, edge computing, and IoT deployments. Given that P$^2$U complements existing compression techniques and can be implemented alongside any compression method, e.g., sparsification, quantization, pruning, etc., the potential for improvement is even greater.</description>
   <guid>oai:arXiv.org:2506.22871v1</guid>
   <category>cs.LG</category>
   <category>cs.MM</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</rights>
   <creator>Homayun Afrabandpey, Hamed Rezazadegan Tavakoli</creator>
  </item>
  <item>
   <title>FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model</title>
   <link>https://arxiv.org/abs/2506.23210</link>
   <description>Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and enables overcoming the catastrophic forgetting issue in each round by utilizing a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and low computing cost.</description>
   <guid>oai:arXiv.org:2506.23210v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.DC</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Taehwan Yoon, Bongjun Choi</creator>
  </item>
  <item>
   <title>FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization</title>
   <link>https://arxiv.org/abs/2506.23516</link>
   <description>Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios.</description>
   <guid>oai:arXiv.org:2506.23516v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee</creator>
  </item>
  <item>
   <title>A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization</title>
   <link>https://arxiv.org/abs/2506.22606</link>
   <description>In the current paradigm of digital personalized services, the centralized management of personal data raises significant privacy concerns, security vulnerabilities, and diminished individual autonomy over sensitive information. Despite their efficiency, traditional centralized architectures frequently fail to satisfy rigorous privacy requirements and expose users to data breaches and unauthorized access risks. This pressing challenge calls for a fundamental paradigm shift in methodologies for collecting, storing, and utilizing personal data across diverse sectors, including education, healthcare, and finance.
  This paper introduces a novel decentralized, privacy-preserving architecture that handles heterogeneous personal information, ranging from educational credentials to health records and financial data. Unlike traditional models, our system grants users complete data ownership and control, allowing them to selectively share information without compromising privacy. The architecture's foundation comprises advanced privacy-enhancing technologies, including secure enclaves and federated learning, enabling secure computation, verification, and data sharing. The system supports diverse functionalities, including local computation, model training, and privacy-preserving data sharing, while ensuring data credibility and robust user privacy.</description>
   <guid>oai:arXiv.org:2506.22606v1</guid>
   <category>cs.CR</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Osama Zafar, Mina Namazi, Yuqiao Xu, Youngjin Yoo, Erman Ayday</creator>
  </item>
  <item>
   <title>Detect \&amp; Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning</title>
   <link>https://arxiv.org/abs/2506.23583</link>
   <description>Federated learning with secure aggregation enables private and collaborative learning from decentralised data without leaking sensitive client information. However, secure aggregation also complicates the detection of malicious client behaviour and the evaluation of individual client contributions to the learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et al.) were proposed for contribution evaluation (CE) and misbehaviour detection (MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance on the random selection of clients in each training round, while FedGT lacks the CE ability. In this work, we combine the strengths of QI and FedGT to achieve both robust MD and accurate CE. Our experiments demonstrate superior performance compared to using either method independently.</description>
   <guid>oai:arXiv.org:2506.23583v1</guid>
   <category>cs.CR</category>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</rights>
   <creator>Marvin Xhemrishi, Alexandre Graell i Amat, Bal\'azs Pej\'o</creator>
  </item>
  <item>
   <title>Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction</title>
   <link>https://arxiv.org/abs/2506.23836</link>
   <description>We consider centralized distributed optimization in the classical federated learning setup, where $n$ workers jointly find an $\varepsilon$-stationary point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access only to unbiased stochastic gradients with variance $\sigma^2$. Each worker requires at most $h$ seconds to compute a stochastic gradient, and the communication times from the server to the workers and from the workers to the server are $\tau_{s}$ and $\tau_{w}$ seconds per coordinate, respectively. One of the main motivations for distributed optimization is to achieve scalability with respect to $n$. For instance, it is well known that the distributed version of SGD has a variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{n \varepsilon^2},$ which improves with the number of workers $n,$ where $\Delta = f(x^0) - f^*,$ and $x^0 \in R^d$ is the starting point. Similarly, using unbiased sparsification compressors, it is possible to reduce both the variance-dependent runtime term and the communication runtime term. However, once we account for the communication from the server to the workers $\tau_{s}$, we prove that it becomes infeasible to design a method using unbiased random sparsification compressors that scales both the server-side communication runtime term $\tau_{s} d \frac{L \Delta}{\varepsilon}$ and the variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{\varepsilon^2},$ better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case, where all workers access the same distribution. To establish this result, we construct a new &quot;worst-case&quot; function and develop a new lower bound framework that reduces the analysis to the concentration of a random sum, for which we prove a concentration bound. These results reveal fundamental limitations in scaling distributed optimization, even under the homogeneous assumption.</description>
   <guid>oai:arXiv.org:2506.23836v1</guid>
   <category>math.OC</category>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Alexander Tyurin</creator>
  </item>
  <item>
   <title>Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC</title>
   <link>https://arxiv.org/abs/2506.24045</link>
   <description>The proliferation of agentic Large Language Models (LLMs) on personal devices introduces a new class of workloads characterized by a dichotomy of objectives. Reactive tasks, initiated by users, demand immediate, low-latency responses, while proactive tasks operate invisibly and prioritize throughput. Existing on-device LLM engines, designed for isolated inferences, fail to efficiently manage these concurrent and conflicting requests on consumer-grade heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces Agent.xpu, an efficient serving system for agentic LLM workloads on memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu first constructs a heterogeneous execution graph, which fuses and chunks model kernels for affinity-guided, elastic accelerator mapping with predictive kernel annotation. At runtime, its online scheduler enables fine-grained, kernel-level preemption to guarantee the responsiveness of reactive tasks. To maximize SoC utilization, it adopts slack-aware kernel backfill to opportunistically append proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves 4.6$\times$ lower latency for reactive tasks and sustains 1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to state-of-the-art inference engines.</description>
   <guid>oai:arXiv.org:2506.24045v1</guid>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Xinming Wei, Jiahao Zhang, Haoran Li, Jiayu Chen, Rui Qu, Maoliang Li, Xiang Chen, Guojie Luo</creator>
  </item>
  <item>
   <title>FedDTG:Federated Data-Free Knowledge Distillation via Three-Player Generative Adversarial Networks</title>
   <link>https://arxiv.org/abs/2201.03169</link>
   <description>While existing federated learning approaches primarily focus on aggregating local models to construct a global model, in realistic settings, some clients may be reluctant to share their private models due to the inclusion of privacy-sensitive information. Knowledge distillation, which can extract model knowledge without accessing model parameters, is well-suited for this federated scenario. However, most distillation methods in federated learning (federated distillation) require a proxy dataset, which is difficult to obtain in the real world. Therefore, in this paper, we introduce a distributed three-player Generative Adversarial Network (GAN) to implement data-free mutual distillation and propose an effective method called FedDTG. We confirmed that the fake samples generated by GAN can make federated distillation more efficient and robust. Additionally, the distillation process between clients can deliver good individual client performance while simultaneously acquiring global knowledge and protecting data privacy. Our extensive experiments on benchmark vision datasets demonstrate that our method outperforms other federated distillation algorithms in terms of generalization.</description>
   <guid>oai:arXiv.org:2201.03169v5</guid>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Lingzhi Gao, Zhenyuan Zhang, Chao Wu</creator>
  </item>
  <item>
   <title>Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning</title>
   <link>https://arxiv.org/abs/2310.11594</link>
   <description>The delicate equilibrium between user privacy and the ability to unleash the potential of distributed data is an important concern. Federated learning, which enables the training of collaborative models without sharing of data, has emerged as a privacy-centric solution. This approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data into the training process, as well as evasion attacks that aim to induce misclassifications at test time. Our research investigates the intersection of adversarial training, a common defense method against evasion attacks, and backdoor attacks within federated learning. We introduce Adversarial Robustness Unhardening (ARU), which is employed by a subset of adversarial clients to intentionally undermine model robustness during federated training, rendering models susceptible to a broader range of evasion attacks. We present extensive experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our results show that ARU can substantially undermine adversarial training's ability to harden models against test-time evasion attacks, and that adversaries employing ARU can even evade robust aggregation defenses that often neutralize poisoning or backdoor attacks.</description>
   <guid>oai:arXiv.org:2310.11594v3</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong</creator>
  </item>
  <item>
   <title>MedLeak: Multimodal Medical Data Leakage in Secure Federated Learning with Crafted Models</title>
   <link>https://arxiv.org/abs/2407.09972</link>
   <description>Federated learning (FL) allows participants to collaboratively train machine learning models while keeping their data local, making it ideal for collaborations among healthcare institutions on sensitive data. However, in this paper, we propose a novel privacy attack called MedLeak, which allows a malicious FL server to recover high-quality site-specific private medical data from the client model updates. MedLeak works by introducing an adversarially crafted model during the FL training process. Honest clients, unaware of the insidious changes in the published models, continue to send back their updates as per the standard FL protocol. Leveraging a novel analytical method, MedLeak can efficiently recover private client data from the aggregated parameter updates, eliminating costly optimization. In addition, the scheme relies solely on the aggregated updates, thus rendering secure aggregation protocols ineffective, as they depend on the randomization of intermediate results for security while leaving the final aggregated results unaltered.
  We implement MedLeak on medical image datasets (MedMNIST, COVIDx CXR-4, and Kaggle Brain Tumor MRI), as well as a medical text dataset (MedAbstract). The results demonstrate that our attack achieves high recovery rates and strong quantitative scores on both image and text datasets. We also thoroughly evaluate MedLeak across different attack parameters, providing insights into key factors that influence attack performance and potential defenses. Furthermore, we demonstrate that the recovered data can support downstream tasks such as disease classification with minimal performance loss. Our findings validate the need for enhanced privacy measures in FL systems, particularly for safeguarding sensitive medical data against powerful model inversion attacks.</description>
   <guid>oai:arXiv.org:2407.09972v2</guid>
   <category>cs.LG</category>
   <category>cs.CR</category>
   <category>eess.IV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Shanghao Shi, Md Shahedul Haque, Abhijeet Parida, Chaoyu Zhang, Marius George Linguraru, Y. Thomas Hou, Syed Muhammad Anwar, Wenjing Lou</creator>
  </item>
  <item>
   <title>FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments</title>
   <link>https://arxiv.org/abs/2503.19564</link>
   <description>As artificial intelligence systems increasingly operate in Real-world environments, the integration of multi-modal data sources such as vision, language, and audio presents both unprecedented opportunities and critical challenges for achieving trustworthy intelligence. In this paper, we propose a novel framework that unifies federated learning with explainable multi-modal reasoning to ensure trustworthiness in decentralized, dynamic settings. Our approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence), leverages cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration to address challenges posed by data heterogeneity, modality imbalance, and out-of-distribution generalization. Through rigorous evaluation across federated multi-modal benchmarks involving vision-language tasks, we demonstrate improved performance in both accuracy and interpretability while reducing vulnerabilities to adversarial and spurious correlations. Further, we introduce a novel trust score aggregation method to quantify global model reliability under dynamic client participation. Our findings pave the way toward developing robust, interpretable, and socially responsible AI systems in Real-world environments.</description>
   <guid>oai:arXiv.org:2503.19564v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Sree Bhargavi Balija</creator>
  </item>
  <item>
   <title>Decoding Federated Learning: The FedNAM+ Conformal Revolution</title>
   <link>https://arxiv.org/abs/2506.17872</link>
   <description>Federated learning has significantly advanced distributed training of machine learning models across decentralized data sources. However, existing frameworks often lack comprehensive solutions that combine uncertainty quantification, interpretability, and robustness. To address this, we propose FedNAM+, a federated learning framework that integrates Neural Additive Models (NAMs) with a novel conformal prediction method to enable interpretable and reliable uncertainty estimation. Our method introduces a dynamic level adjustment technique that utilizes gradient-based sensitivity maps to identify key input features influencing predictions. This facilitates both interpretability and pixel-wise uncertainty estimates. Unlike traditional interpretability methods such as LIME and SHAP, which do not provide confidence intervals, FedNAM+ offers visual insights into prediction reliability. We validate our approach through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with transparent uncertainty measures. Visual analysis highlights variable uncertainty intervals, revealing low-confidence regions where model performance can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+ delivers efficient and global uncertainty estimates with reduced computational overhead, making it particularly suitable for federated learning scenarios. Overall, FedNAM+ provides a robust, interpretable, and computationally efficient framework that enhances trust and transparency in decentralized predictive modeling.</description>
   <guid>oai:arXiv.org:2506.17872v2</guid>
   <category>cs.LG</category>
   <category>cs.CV</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Sree Bhargavi Balija, Amitash Nanda, Debashis Sahoo</creator>
  </item>
  <item>
   <title>Interpretable LLM-based Table Question Answering</title>
   <link>https://arxiv.org/abs/2412.12386</link>
   <description>Interpretability in Table Question Answering (Table QA) is critical, especially in high-stakes domains like finance and healthcare. While recent Table QA approaches based on Large Language Models (LLMs) achieve high accuracy, they often produce ambiguous explanations of how answers are derived.
  We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's decision-making process interpretable. POS decomposes a question into a sequence of atomic steps, each directly translated into an executable SQL command on the table, thereby ensuring that every intermediate result is transparent. Through extensive experiments, we show that: First, POS generates the highest-quality explanations among compared methods, which markedly improves the users' ability to simulate and verify the model's decisions. Second, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and FeTaQA), POS achieves QA accuracy that is competitive to existing methods, while also offering greater efficiency-requiring significantly fewer LLM calls and table database queries (up to 25x fewer)-and more robust performance on large-sized tables. Finally, we observe high agreement (up to 90.59% in forward simulation) between LLMs and human users when making decisions based on the same explanations, suggesting that LLMs could serve as an effective proxy for humans in evaluating Table QA explanations.</description>
   <guid>oai:arXiv.org:2412.12386v3</guid>
   <category>cs.CL</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Giang Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, Freddy Lecue</creator>
  </item>
  <item>
   <title>Efficiently Serving Large Multimodal Models Using EPD Disaggregation</title>
   <link>https://arxiv.org/abs/2501.05460</link>
   <description>Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.</description>
   <guid>oai:arXiv.org:2501.05460v4</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.CV</category>
   <category>cs.LG</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan</creator>
  </item>
  <item>
   <title>Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models</title>
   <link>https://arxiv.org/abs/2501.14406</link>
   <description>Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Adaptive Rank Allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.</description>
   <guid>oai:arXiv.org:2501.14406v3</guid>
   <category>cs.DC</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NI</category>
   <pubdate>Tue, 01 Jul 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</creator>
  </item>
 </channel>
</rss>
