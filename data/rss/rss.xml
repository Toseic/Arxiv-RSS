<?xml version="1.0" ?>
<rss>
 <channel>
  <title>arxiv-rss</title>
  <link></link>
  <description></description>
  <docs></docs>
  <language>en-us</language>
  <lastBuildDate>Tue, 13 May 2025 03:07:40 </lastBuildDate>
  <managingEditor></managingEditor>
  <pubDate>Tue, 13 May 2025 03:07:40 </pubDate>
  <item>
   <title>FedAvgen: Metadata for Model Aggregation In Communication Systems</title>
   <link>https://arxiv.org/abs/2505.05486</link>
   <description>To improve business efficiency and minimize costs, Artificial Intelligence (AI) practitioners have adopted a shift from formulating models from scratch towards sharing pretrained models. The pretrained models are then aggregated into a global model with higher generalization capabilities, which is afterwards distributed to the client devices. This approach is known as federated learning and inherently utilizes different techniques to select the candidate client models averaged to obtain the global model. This approach, in the case of communication systems, faces challenges arising from the existential diversity in device profiles. The multiplicity in profiles motivates our conceptual assessment of a metaheuristic algorithm (FedAvgen), which relates each pretrained model with its weight space as metadata, to a phenotype and genotype, respectively. This parent-child genetic evolution characterizes the global averaging step in federated learning. We then compare the results of our approach to two widely adopted baseline federated learning algorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient Descent (FedSGD).</description>
   <guid>oai:arXiv.org:2505.05486v1</guid>
   <category>cs.NE</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Anthony Kiggundu, Dennis Krummacker, Hans D. Schotten</creator>
  </item>
  <item>
   <title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
   <link>https://arxiv.org/abs/2406.09831</link>
   <description>Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.</description>
   <guid>oai:arXiv.org:2406.09831v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.NE</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Youyang Qu, Ming Liu, Tianqing Zhu, Longxiang Gao, Shui Yu, Wanlei Zhou</creator>
  </item>
  <item>
   <title>From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning</title>
   <link>https://arxiv.org/abs/2501.03119</link>
   <description>Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange. Nevertheless, model training leaves exploitable traces that can be used to infer sensitive information. In Decentralized FL (DFL), the topology, defining how participants are connected, plays a crucial role in shaping the model's privacy, robustness, and convergence. However, the topology introduces an unexplored vulnerability: attackers can exploit it to infer participant relationships and launch targeted attacks. This work uncovers the hidden risks of DFL topologies by proposing a novel Topology Inference Attack that infers the topology solely from model behavior. A taxonomy of topology inference attacks is introduced, categorizing them by the attacker's capabilities and knowledge. Practical attack strategies are designed for various scenarios, and experiments are conducted to identify key factors influencing attack success. The results demonstrate that analyzing only the model of each node can accurately infer the DFL topology, highlighting a critical privacy risk in DFL systems. These findings offer valuable insights for improving privacy preservation in DFL environments.</description>
   <guid>oai:arXiv.org:2501.03119v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</rights>
   <creator>Chao Feng, Yuanzhe Gao, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller</creator>
  </item>
  <item>
   <title>Privacy-Preserved Automated Scoring using Federated Learning for Educational Research</title>
   <link>https://arxiv.org/abs/2503.11711</link>
   <description>Data privacy remains a critical concern in educational research, requiring strict adherence to ethical standards and regulatory protocols. While traditional approaches rely on anonymization and centralized data collection, they often expose raw student data to security vulnerabilities and impose substantial logistical overhead. In this study, we propose a federated learning (FL) framework for automated scoring of educational assessments that eliminates the need to share sensitive data across institutions. Our approach leverages parameter-efficient fine-tuning of large language models (LLMs) with Low-Rank Adaptation (LoRA), enabling each client (school) to train locally while sharing only optimized model updates. To address data heterogeneity, we implement an adaptive weighted aggregation strategy that considers both client performance and data volume. We benchmark our model against two state-of-the-art FL methods and a centralized learning baseline using NGSS-aligned multi-label science assessment data from nine middle schools. Results show that our model achieves the highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0 percentage points of the centralized model on these metrics. Additionally, it achieves comparable rubric-level scoring accuracy, with only a 1.3% difference in rubric match and a lower score deviation (MAE), highlighting its effectiveness in preserving both prediction quality and interpretability.</description>
   <guid>oai:arXiv.org:2503.11711v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Ehsan Latif, Xiaoming Zhai</creator>
  </item>
  <item>
   <title>Safe-EF: Error Feedback for Nonsmooth Constrained Optimization</title>
   <link>https://arxiv.org/abs/2505.06053</link>
   <description>Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-K) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity.</description>
   <guid>oai:arXiv.org:2505.06053v1</guid>
   <category>cs.LG</category>
   <category>math.OC</category>
   <category>stat.ML</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Rustem Islamov, Yarden As, Ilyas Fatkhullin</creator>
  </item>
  <item>
   <title>FedAvgen: Metadata for Model Aggregation In Communication Systems</title>
   <link>https://arxiv.org/abs/2505.05486</link>
   <description>To improve business efficiency and minimize costs, Artificial Intelligence (AI) practitioners have adopted a shift from formulating models from scratch towards sharing pretrained models. The pretrained models are then aggregated into a global model with higher generalization capabilities, which is afterwards distributed to the client devices. This approach is known as federated learning and inherently utilizes different techniques to select the candidate client models averaged to obtain the global model. This approach, in the case of communication systems, faces challenges arising from the existential diversity in device profiles. The multiplicity in profiles motivates our conceptual assessment of a metaheuristic algorithm (FedAvgen), which relates each pretrained model with its weight space as metadata, to a phenotype and genotype, respectively. This parent-child genetic evolution characterizes the global averaging step in federated learning. We then compare the results of our approach to two widely adopted baseline federated learning algorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient Descent (FedSGD).</description>
   <guid>oai:arXiv.org:2505.05486v1</guid>
   <category>cs.NE</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Anthony Kiggundu, Dennis Krummacker, Hans D. Schotten</creator>
  </item>
  <item>
   <title>DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning under Two-sided Incomplete Information</title>
   <link>https://arxiv.org/abs/2505.05842</link>
   <description>Online Federated Learning (OFL) is a real-time learning paradigm that sequentially executes parameter aggregation immediately for each random arriving client. To motivate clients to participate in OFL, it is crucial to offer appropriate incentives to offset the training resource consumption. However, the design of incentive mechanisms in OFL is constrained by the dynamic variability of Two-sided Incomplete Information (TII) concerning resources, where the server is unaware of the clients' dynamically changing computational resources, while clients lack knowledge of the real-time communication resources allocated by the server. To incentivize clients to participate in training by offering dynamic rewards to each arriving client, we design a novel Dynamic Bayesian persuasion pricing for online Federated learning (DaringFed) under TII. Specifically, we begin by formulating the interaction between the server and clients as a dynamic signaling and pricing allocation problem within a Bayesian persuasion game, and then demonstrate the existence of a unique Bayesian persuasion Nash equilibrium. By deriving the optimal design of DaringFed under one-sided incomplete information, we further analyze the approximate optimal design of DaringFed with a specific bound under TII. Finally, extensive evaluation conducted on real datasets demonstrate that DaringFed optimizes accuracy and converges speed by 16.99%, while experiments with synthetic datasets validate the convergence of estimate unknown values and the effectiveness of DaringFed in improving the server's utility by up to 12.6%.</description>
   <guid>oai:arXiv.org:2505.05842v1</guid>
   <category>cs.GT</category>
   <category>cs.LG</category>
   <category>stat.ML</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Yun Xin, Jianfeng Lu, Shuqin Cao, Gang Li, Haozhao Wang, Guanghui Wen</creator>
  </item>
  <item>
   <title>Adaptive Gradient Clipping for Robust Federated Learning</title>
   <link>https://arxiv.org/abs/2405.14432</link>
   <description>Robust federated learning aims to maintain reliable performance despite the presence of adversarial or misbehaving workers. While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods were proven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping. However, existing static clipping strategies yield inconsistent results: enhancing robustness against some attacks while being ineffective or even detrimental against others. To address this limitation, we propose a principled adaptive clipping strategy, Adaptive Robust Clipping (ARC), which dynamically adjusts clipping thresholds based on the input gradients. We prove that ARC not only preserves the theoretical robustness guarantees of SOTA Robust-DGD methods but also provably improves asymptotic convergence when the model is well-initialized. Extensive experiments on benchmark image classification tasks confirm these theoretical insights, demonstrating that ARC significantly enhances robustness, particularly in highly heterogeneous and adversarial settings.</description>
   <guid>oai:arXiv.org:2405.14432v5</guid>
   <category>cs.LG</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Ahmed Jellouli, Geovani Rizk, John Stephan</creator>
  </item>
  <item>
   <title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
   <link>https://arxiv.org/abs/2406.09831</link>
   <description>Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.</description>
   <guid>oai:arXiv.org:2406.09831v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.NE</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Youyang Qu, Ming Liu, Tianqing Zhu, Longxiang Gao, Shui Yu, Wanlei Zhou</creator>
  </item>
  <item>
   <title>Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations</title>
   <link>https://arxiv.org/abs/2409.17264</link>
   <description>As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.
  We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).
  Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.</description>
   <guid>oai:arXiv.org:2409.17264v3</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Amey Agrawal, Haoran Qiu, Junda Chen, \'I\~nigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse</creator>
  </item>
  <item>
   <title>From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning</title>
   <link>https://arxiv.org/abs/2501.03119</link>
   <description>Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange. Nevertheless, model training leaves exploitable traces that can be used to infer sensitive information. In Decentralized FL (DFL), the topology, defining how participants are connected, plays a crucial role in shaping the model's privacy, robustness, and convergence. However, the topology introduces an unexplored vulnerability: attackers can exploit it to infer participant relationships and launch targeted attacks. This work uncovers the hidden risks of DFL topologies by proposing a novel Topology Inference Attack that infers the topology solely from model behavior. A taxonomy of topology inference attacks is introduced, categorizing them by the attacker's capabilities and knowledge. Practical attack strategies are designed for various scenarios, and experiments are conducted to identify key factors influencing attack success. The results demonstrate that analyzing only the model of each node can accurately infer the DFL topology, highlighting a critical privacy risk in DFL systems. These findings offer valuable insights for improving privacy preservation in DFL environments.</description>
   <guid>oai:arXiv.org:2501.03119v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</rights>
   <creator>Chao Feng, Yuanzhe Gao, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller</creator>
  </item>
  <item>
   <title>Privacy-Preserved Automated Scoring using Federated Learning for Educational Research</title>
   <link>https://arxiv.org/abs/2503.11711</link>
   <description>Data privacy remains a critical concern in educational research, requiring strict adherence to ethical standards and regulatory protocols. While traditional approaches rely on anonymization and centralized data collection, they often expose raw student data to security vulnerabilities and impose substantial logistical overhead. In this study, we propose a federated learning (FL) framework for automated scoring of educational assessments that eliminates the need to share sensitive data across institutions. Our approach leverages parameter-efficient fine-tuning of large language models (LLMs) with Low-Rank Adaptation (LoRA), enabling each client (school) to train locally while sharing only optimized model updates. To address data heterogeneity, we implement an adaptive weighted aggregation strategy that considers both client performance and data volume. We benchmark our model against two state-of-the-art FL methods and a centralized learning baseline using NGSS-aligned multi-label science assessment data from nine middle schools. Results show that our model achieves the highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0 percentage points of the centralized model on these metrics. Additionally, it achieves comparable rubric-level scoring accuracy, with only a 1.3% difference in rubric match and a lower score deviation (MAE), highlighting its effectiveness in preserving both prediction quality and interpretability.</description>
   <guid>oai:arXiv.org:2503.11711v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Ehsan Latif, Xiaoming Zhai</creator>
  </item>
  <item>
   <title>Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning</title>
   <link>https://arxiv.org/abs/2505.01454</link>
   <description>Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet it faces significant challenges in communication efficiency and vulnerability to poisoning attacks. While sparsification techniques mitigate communication overhead by transmitting only critical model parameters, they inadvertently amplify security risks: adversarial clients can exploit sparse updates to evade detection and degrade model performance. Existing defense mechanisms, designed for standard FL communication scenarios, are ineffective in addressing these vulnerabilities within sparsified FL. To bridge this gap, we propose FLARE, a novel federated learning framework that integrates sparse index mask inspection and model update sign similarity analysis to detect and mitigate poisoning attacks in sparsified FL. Extensive experiments across multiple datasets and adversarial scenarios demonstrate that FLARE significantly outperforms existing defense strategies, effectively securing sparsified FL against poisoning attacks while maintaining communication efficiency.</description>
   <guid>oai:arXiv.org:2505.01454v2</guid>
   <category>cs.CR</category>
   <category>cs.LG</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</rights>
   <creator>Zhiyong Jin, Runhua Xu, Chao Li, Yizhong Liu, Jianxin Li</creator>
  </item>
  <item>
   <title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
   <link>https://arxiv.org/abs/2505.06184</link>
   <description>Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.</description>
   <guid>oai:arXiv.org:2505.06184v1</guid>
   <category>cs.SI</category>
   <category>cs.CL</category>
   <category>cs.IR</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by-sa/4.0/</rights>
   <creator>Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour</creator>
  </item>
  <item>
   <title>UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</title>
   <link>https://arxiv.org/abs/2503.20377</link>
   <description>As the Large-scale Language Models (LLMs) continue to scale, the requisite computational power and bandwidth escalate. To address this, we introduce UB-Mesh, a novel AI datacenter network architecture designed to enhance scalability, performance, cost-efficiency and availability. Unlike traditional datacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a hierarchically localized nD-FullMesh network topology. This design fully leverages the data locality of LLM training, prioritizing short-range, direct interconnects to minimize data movement distance and reduce switch usage.
  Although UB-Mesh's nD-FullMesh topology offers several theoretical advantages, its concrete architecture design, physical implementation and networking system optimization present new challenges. For the actual construction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is based on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of hardware components that serve as the foundational building blocks, including specifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch (HRS), NICs and others. These components are interconnected via a novel Unified Bus (UB) technique, which enables flexible IO bandwidth allocation and hardware resource pooling. For networking system optimization, we propose advanced routing mechanism named All-Path-Routing (APR) to efficiently manage data traffic. These optimizations, combined with topology-aware performance enhancements and robust reliability measures like 64+1 backup design, result in 2.04x higher cost-efficiency, 7.2% higher network availability compared to traditional Clos architecture and 95%+ linearity in various LLM training tasks.</description>
   <guid>oai:arXiv.org:2503.20377v2</guid>
   <category>cs.AR</category>
   <category>cs.NI</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Heng Liao, Bingyang Liu, Xianping Chen, Zhigang Guo, Chuanning Cheng, Jianbing Wang, Xiangyu Chen, Peng Dong, Rui Meng, Wenjie Liu, Zhe Zhou, Ziyang Zhang, Yuhang Gai, Cunle Qian, Yi Xiong, Zhongwu Cheng, Jing Xia, Yuli Ma, Xi Chen, Wenhua Du, Shizhong Xiao, Chungang Li, Yong Qin, Liudong Xiong, Zhou Yu, Lv Chen, Lei Chen, Buyun Wang, Pei Wu, Junen Gao, Xiaochu Li, Jian He, Shizhuan Yan, Bill McColl</creator>
  </item>
  <item>
   <title>UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</title>
   <link>https://arxiv.org/abs/2503.20377</link>
   <description>As the Large-scale Language Models (LLMs) continue to scale, the requisite computational power and bandwidth escalate. To address this, we introduce UB-Mesh, a novel AI datacenter network architecture designed to enhance scalability, performance, cost-efficiency and availability. Unlike traditional datacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a hierarchically localized nD-FullMesh network topology. This design fully leverages the data locality of LLM training, prioritizing short-range, direct interconnects to minimize data movement distance and reduce switch usage.
  Although UB-Mesh's nD-FullMesh topology offers several theoretical advantages, its concrete architecture design, physical implementation and networking system optimization present new challenges. For the actual construction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is based on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of hardware components that serve as the foundational building blocks, including specifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch (HRS), NICs and others. These components are interconnected via a novel Unified Bus (UB) technique, which enables flexible IO bandwidth allocation and hardware resource pooling. For networking system optimization, we propose advanced routing mechanism named All-Path-Routing (APR) to efficiently manage data traffic. These optimizations, combined with topology-aware performance enhancements and robust reliability measures like 64+1 backup design, result in 2.04x higher cost-efficiency, 7.2% higher network availability compared to traditional Clos architecture and 95%+ linearity in various LLM training tasks.</description>
   <guid>oai:arXiv.org:2503.20377v2</guid>
   <category>cs.AR</category>
   <category>cs.NI</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Heng Liao, Bingyang Liu, Xianping Chen, Zhigang Guo, Chuanning Cheng, Jianbing Wang, Xiangyu Chen, Peng Dong, Rui Meng, Wenjie Liu, Zhe Zhou, Ziyang Zhang, Yuhang Gai, Cunle Qian, Yi Xiong, Zhongwu Cheng, Jing Xia, Yuli Ma, Xi Chen, Wenhua Du, Shizhong Xiao, Chungang Li, Yong Qin, Liudong Xiong, Zhou Yu, Lv Chen, Lei Chen, Buyun Wang, Pei Wu, Junen Gao, Xiaochu Li, Jian He, Shizhuan Yan, Bill McColl</creator>
  </item>
  <item>
   <title>FedAvgen: Metadata for Model Aggregation In Communication Systems</title>
   <link>https://arxiv.org/abs/2505.05486</link>
   <description>To improve business efficiency and minimize costs, Artificial Intelligence (AI) practitioners have adopted a shift from formulating models from scratch towards sharing pretrained models. The pretrained models are then aggregated into a global model with higher generalization capabilities, which is afterwards distributed to the client devices. This approach is known as federated learning and inherently utilizes different techniques to select the candidate client models averaged to obtain the global model. This approach, in the case of communication systems, faces challenges arising from the existential diversity in device profiles. The multiplicity in profiles motivates our conceptual assessment of a metaheuristic algorithm (FedAvgen), which relates each pretrained model with its weight space as metadata, to a phenotype and genotype, respectively. This parent-child genetic evolution characterizes the global averaging step in federated learning. We then compare the results of our approach to two widely adopted baseline federated learning algorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient Descent (FedSGD).</description>
   <guid>oai:arXiv.org:2505.05486v1</guid>
   <category>cs.NE</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Anthony Kiggundu, Dennis Krummacker, Hans D. Schotten</creator>
  </item>
  <item>
   <title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
   <link>https://arxiv.org/abs/2406.09831</link>
   <description>Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.</description>
   <guid>oai:arXiv.org:2406.09831v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.NE</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Youyang Qu, Ming Liu, Tianqing Zhu, Longxiang Gao, Shui Yu, Wanlei Zhou</creator>
  </item>
  <item>
   <title>Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations</title>
   <link>https://arxiv.org/abs/2409.17264</link>
   <description>As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.
  We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).
  Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.</description>
   <guid>oai:arXiv.org:2409.17264v3</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Amey Agrawal, Haoran Qiu, Junda Chen, \'I\~nigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse</creator>
  </item>
  <item>
   <title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
   <link>https://arxiv.org/abs/2505.06184</link>
   <description>Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.</description>
   <guid>oai:arXiv.org:2505.06184v1</guid>
   <category>cs.SI</category>
   <category>cs.CL</category>
   <category>cs.IR</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-sa/4.0/</rights>
   <creator>Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour</creator>
  </item>
  <item>
   <title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
   <link>https://arxiv.org/abs/2505.06184</link>
   <description>Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.</description>
   <guid>oai:arXiv.org:2505.06184v1</guid>
   <category>cs.SI</category>
   <category>cs.CL</category>
   <category>cs.IR</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-sa/4.0/</rights>
   <creator>Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour</creator>
  </item>
  <item>
   <title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
   <link>https://arxiv.org/abs/2406.09831</link>
   <description>Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.</description>
   <guid>oai:arXiv.org:2406.09831v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <category>cs.CL</category>
   <category>cs.NE</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Youyang Qu, Ming Liu, Tianqing Zhu, Longxiang Gao, Shui Yu, Wanlei Zhou</creator>
  </item>
  <item>
   <title>Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction</title>
   <link>https://arxiv.org/abs/2502.04521</link>
   <description>Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL aggregates locally trained model weights into a global model, inherently constraining all sites to use a homogeneous model architecture. This rigidity forces sites to compromise on architectures tailored to their compute resources and application-specific needs, making conventional FL unsuitable for model-heterogeneous settings where each site may prefer a distinct architecture. To overcome this limitation, we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that learns the distribution of multi-site MR images. For high-fidelity synthesis, we propose a novel site-prompted GAT prior that controllably synthesizes realistic MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its own reconstruction model -- using an architecture of its choice -- on a hybrid dataset augmenting its local MRI dataset with GAT-generated synthetic MR images emulating datasets from other sites. This hybrid training strategy enables site-specific reconstruction models to generalize more effectively across diverse data distributions while preserving data privacy. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT enables flexible, model-heterogeneous collaborations and achieves superior within-site and cross-site reconstruction performance compared to state-of-the-art FL baselines.</description>
   <guid>oai:arXiv.org:2502.04521v2</guid>
   <category>eess.IV</category>
   <category>cs.CV</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, Tolga \c{C}ukur</creator>
  </item>
  <item>
   <title>DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning under Two-sided Incomplete Information</title>
   <link>https://arxiv.org/abs/2505.05842</link>
   <description>Online Federated Learning (OFL) is a real-time learning paradigm that sequentially executes parameter aggregation immediately for each random arriving client. To motivate clients to participate in OFL, it is crucial to offer appropriate incentives to offset the training resource consumption. However, the design of incentive mechanisms in OFL is constrained by the dynamic variability of Two-sided Incomplete Information (TII) concerning resources, where the server is unaware of the clients' dynamically changing computational resources, while clients lack knowledge of the real-time communication resources allocated by the server. To incentivize clients to participate in training by offering dynamic rewards to each arriving client, we design a novel Dynamic Bayesian persuasion pricing for online Federated learning (DaringFed) under TII. Specifically, we begin by formulating the interaction between the server and clients as a dynamic signaling and pricing allocation problem within a Bayesian persuasion game, and then demonstrate the existence of a unique Bayesian persuasion Nash equilibrium. By deriving the optimal design of DaringFed under one-sided incomplete information, we further analyze the approximate optimal design of DaringFed with a specific bound under TII. Finally, extensive evaluation conducted on real datasets demonstrate that DaringFed optimizes accuracy and converges speed by 16.99%, while experiments with synthetic datasets validate the convergence of estimate unknown values and the effectiveness of DaringFed in improving the server's utility by up to 12.6%.</description>
   <guid>oai:arXiv.org:2505.05842v1</guid>
   <category>cs.GT</category>
   <category>cs.LG</category>
   <category>stat.ML</category>
   <pubdate>Mon, 12 May 2025 00:00:00 -0400</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Yun Xin, Jianfeng Lu, Shuqin Cao, Gang Li, Haozhao Wang, Guanghui Wen</creator>
  </item>
 </channel>
</rss>
