<?xml version="1.0" ?>
<rss>
 <channel>
  <title>arxiv-rss</title>
  <link></link>
  <description></description>
  <docs></docs>
  <language>en-us</language>
  <lastBuildDate>Sat, 01 Feb 2025 02:42:54 </lastBuildDate>
  <managingEditor></managingEditor>
  <pubDate>Sat, 01 Feb 2025 02:42:54 </pubDate>
  <item>
   <title>Advancing Personalized Federated Learning: Integrative Approaches with AI for Enhanced Privacy and Customization</title>
   <link>https://arxiv.org/abs/2501.18174</link>
   <description>In the age of data-driven decision making, preserving privacy while providing personalized experiences has become paramount. Personalized Federated Learning (PFL) offers a promising framework by decentralizing the learning process, thus ensuring data privacy and reducing reliance on centralized data repositories. However, the integration of advanced Artificial Intelligence (AI) techniques within PFL remains underexplored. This paper proposes a novel approach that enhances PFL with cutting-edge AI methodologies including adaptive optimization, transfer learning, and differential privacy. We present a model that not only boosts the performance of individual client models but also ensures robust privacy-preserving mechanisms and efficient resource utilization across heterogeneous networks. Empirical results demonstrate significant improvements in model accuracy and personalization, along with stringent privacy adherence, as compared to conventional federated learning models. This work paves the way for a new era of truly personalized and privacy-conscious AI systems, offering significant implications for industries requiring compliance with stringent data protection regulations.</description>
   <guid>oai:arXiv.org:2501.18174v1</guid>
   <category>cs.LG</category>
   <category>eess.SP</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/publicdomain/zero/1.0/</rights>
   <creator>Kevin Cooper, Michael Geller</creator>
  </item>
  <item>
   <title>Update Estimation and Scheduling for Over-the-Air Federated Learning with Energy Harvesting Devices</title>
   <link>https://arxiv.org/abs/2501.18298</link>
   <description>We study over-the-air (OTA) federated learning (FL) for energy harvesting devices with heterogeneous data distribution over wireless fading multiple access channel (MAC). To address the impact of low energy arrivals and data heterogeneity on global learning, we propose user scheduling strategies. Specifically, we develop two approaches: 1) entropy-based scheduling for known data distributions and 2) least-squares-based user representation estimation for scheduling with unknown data distributions at the parameter server. Both methods aim to select diverse users, mitigating bias and enhancing convergence. Numerical and analytical results demonstrate improved learning performance by reducing redundancy and conserving energy.</description>
   <guid>oai:arXiv.org:2501.18298v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Furkan Bagci, Busra Tegin, Mohammad Kazemi, Tolga M. Duman</creator>
  </item>
  <item>
   <title>Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation</title>
   <link>https://arxiv.org/abs/2501.18416</link>
   <description>Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four potential vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these potential risks, we propose a human-AI collaborative framework that introduces both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols. Our findings will guide future research and emphasize proactive strategies for emerging military contexts.</description>
   <guid>oai:arXiv.org:2501.18416v1</guid>
   <category>cs.LG</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, Joonhyuk Kang</creator>
  </item>
  <item>
   <title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
   <link>https://arxiv.org/abs/2501.18280</link>
   <description>The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.</description>
   <guid>oai:arXiv.org:2501.18280v1</guid>
   <category>cs.CL</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NE</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang</creator>
  </item>
  <item>
   <title>Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback</title>
   <link>https://arxiv.org/abs/2112.14332</link>
   <description>Due to the high cost of communication, federated learning (FL) systems need to sample a subset of clients that are involved in each round of training. As a result, client sampling plays an important role in FL systems as it affects the convergence rate of optimization algorithms used to train machine learning models. Despite its importance, there is limited work on how to sample clients effectively. In this paper, we cast client sampling as an online learning task with bandit feedback, which we solve with an online stochastic mirror descent (OSMD) algorithm designed to minimize the sampling variance. We then theoretically show how our sampling method can improve the convergence speed of federated optimization algorithms over the widely used uniform sampling. Through both simulated and real data experiments, we empirically illustrate the advantages of the proposed client sampling algorithm over uniform sampling and existing online learning-based sampling strategies. The proposed adaptive sampling procedure is applicable beyond the FL problem studied here and can be used to improve the performance of stochastic optimization procedures such as stochastic gradient descent and stochastic coordinate descent.</description>
   <guid>oai:arXiv.org:2112.14332v5</guid>
   <category>cs.LG</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Boxin Zhao, Lingxiao Wang, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Chaochao Chen, Mladen Kolar</creator>
  </item>
  <item>
   <title>Adaptive Guidance for Local Training in Heterogeneous Federated Learning</title>
   <link>https://arxiv.org/abs/2410.06490</link>
   <description>Model heterogeneity poses a significant challenge in Heterogeneous Federated Learning (HtFL). In scenarios with diverse model architectures, directly aggregating model parameters is impractical, leading HtFL methods to incorporate an extra objective alongside the original local objective on each client to facilitate collaboration. However, this often results in a mismatch between the extra and local objectives. To resolve this, we propose Federated Learning-to-Guide (FedL2G), a method that adaptively learns to guide local training in a federated manner, ensuring the added objective aligns with each client's original goal. With theoretical guarantees, FedL2G utilizes only first-order derivatives w.r.t. model parameters, achieving a non-convex convergence rate of O(1/T). We conduct extensive experiments across two data heterogeneity and six model heterogeneity settings, using 14 heterogeneous model architectures (e.g., CNNs and ViTs). The results show that FedL2G significantly outperforms seven state-of-the-art methods.</description>
   <guid>oai:arXiv.org:2410.06490v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</rights>
   <creator>Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao, Qiang Yang</creator>
  </item>
  <item>
   <title>Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis</title>
   <link>https://arxiv.org/abs/2501.04443</link>
   <description>LocalSGD and SCAFFOLD are widely used methods in distributed stochastic optimization, with numerous applications in machine learning, large-scale data processing, and federated learning. However, rigorously establishing their theoretical advantages over simpler methods, such as minibatch SGD (MbSGD), has proven challenging, as existing analyses often rely on strong assumptions, unrealistic premises, or overly restrictive scenarios.
  In this work, we revisit the convergence properties of LocalSGD and SCAFFOLD under a variety of existing or weaker conditions, including gradient similarity, Hessian similarity, weak convexity, and Lipschitz continuity of the Hessian. Our analysis shows that (i) LocalSGD achieves faster convergence compared to MbSGD for weakly convex functions without requiring stronger gradient similarity assumptions; (ii) LocalSGD benefits significantly from higher-order similarity and smoothness; and (iii) SCAFFOLD demonstrates faster convergence than MbSGD for a broader class of non-quadratic functions. These theoretical insights provide a clearer understanding of the conditions under which LocalSGD and SCAFFOLD outperform MbSGD.</description>
   <guid>oai:arXiv.org:2501.04443v2</guid>
   <category>math.OC</category>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</rights>
   <creator>Ruichen Luo, Sebastian U Stich, Samuel Horv\'ath, Martin Tak\'a\v{c}</creator>
  </item>
  <item>
   <title>Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics</title>
   <link>https://arxiv.org/abs/2501.14883</link>
   <description>Improvements in large language models have led to increasing optimism that they can serve as reliable evaluators of natural language generation outputs. In this paper, we challenge this optimism by thoroughly re-evaluating five state-of-the-art factuality metrics on a collection of 11 datasets for summarization, retrieval-augmented generation, and question answering. We find that these evaluators are inconsistent with each other and often misestimate system-level performance, both of which can lead to a variety of pitfalls. We further show that these metrics exhibit biases against highly paraphrased outputs and outputs that draw upon faraway parts of the source documents. We urge users of these factuality metrics to proceed with caution and manually validate the reliability of these metrics in their domain of interest before proceeding.</description>
   <guid>oai:arXiv.org:2501.14883v2</guid>
   <category>cs.CL</category>
   <category>cs.LG</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Ameya Godbole, Robin Jia</creator>
  </item>
  <item>
   <title>Update Estimation and Scheduling for Over-the-Air Federated Learning with Energy Harvesting Devices</title>
   <link>https://arxiv.org/abs/2501.18298</link>
   <description>We study over-the-air (OTA) federated learning (FL) for energy harvesting devices with heterogeneous data distribution over wireless fading multiple access channel (MAC). To address the impact of low energy arrivals and data heterogeneity on global learning, we propose user scheduling strategies. Specifically, we develop two approaches: 1) entropy-based scheduling for known data distributions and 2) least-squares-based user representation estimation for scheduling with unknown data distributions at the parameter server. Both methods aim to select diverse users, mitigating bias and enhancing convergence. Numerical and analytical results demonstrate improved learning performance by reducing redundancy and conserving energy.</description>
   <guid>oai:arXiv.org:2501.18298v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Furkan Bagci, Busra Tegin, Mohammad Kazemi, Tolga M. Duman</creator>
  </item>
  <item>
   <title>Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis</title>
   <link>https://arxiv.org/abs/2501.04443</link>
   <description>LocalSGD and SCAFFOLD are widely used methods in distributed stochastic optimization, with numerous applications in machine learning, large-scale data processing, and federated learning. However, rigorously establishing their theoretical advantages over simpler methods, such as minibatch SGD (MbSGD), has proven challenging, as existing analyses often rely on strong assumptions, unrealistic premises, or overly restrictive scenarios.
  In this work, we revisit the convergence properties of LocalSGD and SCAFFOLD under a variety of existing or weaker conditions, including gradient similarity, Hessian similarity, weak convexity, and Lipschitz continuity of the Hessian. Our analysis shows that (i) LocalSGD achieves faster convergence compared to MbSGD for weakly convex functions without requiring stronger gradient similarity assumptions; (ii) LocalSGD benefits significantly from higher-order similarity and smoothness; and (iii) SCAFFOLD demonstrates faster convergence than MbSGD for a broader class of non-quadratic functions. These theoretical insights provide a clearer understanding of the conditions under which LocalSGD and SCAFFOLD outperform MbSGD.</description>
   <guid>oai:arXiv.org:2501.04443v2</guid>
   <category>math.OC</category>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</rights>
   <creator>Ruichen Luo, Sebastian U Stich, Samuel Horv\'ath, Martin Tak\'a\v{c}</creator>
  </item>
  <item>
   <title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
   <link>https://arxiv.org/abs/2501.18280</link>
   <description>The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.</description>
   <guid>oai:arXiv.org:2501.18280v1</guid>
   <category>cs.CL</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NE</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang</creator>
  </item>
  <item>
   <title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
   <link>https://arxiv.org/abs/2501.18280</link>
   <description>The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.</description>
   <guid>oai:arXiv.org:2501.18280v1</guid>
   <category>cs.CL</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NE</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang</creator>
  </item>
  <item>
   <title>Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics</title>
   <link>https://arxiv.org/abs/2501.14883</link>
   <description>Improvements in large language models have led to increasing optimism that they can serve as reliable evaluators of natural language generation outputs. In this paper, we challenge this optimism by thoroughly re-evaluating five state-of-the-art factuality metrics on a collection of 11 datasets for summarization, retrieval-augmented generation, and question answering. We find that these evaluators are inconsistent with each other and often misestimate system-level performance, both of which can lead to a variety of pitfalls. We further show that these metrics exhibit biases against highly paraphrased outputs and outputs that draw upon faraway parts of the source documents. We urge users of these factuality metrics to proceed with caution and manually validate the reliability of these metrics in their domain of interest before proceeding.</description>
   <guid>oai:arXiv.org:2501.14883v2</guid>
   <category>cs.CL</category>
   <category>cs.LG</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Ameya Godbole, Robin Jia</creator>
  </item>
  <item>
   <title>Power-Efficient Over-the-Air Aggregation with Receive Beamforming for Federated Learning</title>
   <link>https://arxiv.org/abs/2501.18058</link>
   <description>This paper studies power-efficient uplink transmission design for federated learning (FL) that employs over-the-air analog aggregation and multi-antenna beamforming at the server. We jointly optimize device transmit weights and receive beamforming at each FL communication round to minimize the total device transmit power while ensuring convergence in FL training. Through our convergence analysis, we establish sufficient conditions on the aggregation error to guarantee FL training convergence. Utilizing these conditions, we reformulate the power minimization problem into a unique bi-convex structure that contains a transmit beamforming optimization subproblem and a receive beamforming feasibility subproblem. Despite this unconventional structure, we propose a novel alternating optimization approach that guarantees monotonic decrease of the objective value, to allow convergence to a partial optimum. We further consider imperfect channel state information (CSI), which requires accounting for the channel estimation errors in the power minimization problem and FL convergence analysis. We propose a CSI-error-aware joint beamforming algorithm, which can substantially outperform one that does not account for channel estimation errors. Simulation with canonical classification datasets demonstrates that our proposed methods achieve significant power reduction compared to existing benchmarks across a wide range of parameter settings, while attaining the same target accuracy under the same convergence rate.</description>
   <guid>oai:arXiv.org:2501.18058v1</guid>
   <category>cs.IT</category>
   <category>eess.SP</category>
   <category>math.IT</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Faeze Moradi Kalarde, Min Dong, Ben Liang, Yahia A. Eldemerdash Ahmed, Ho Ting Cheng</creator>
  </item>
  <item>
   <title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
   <link>https://arxiv.org/abs/2501.18280</link>
   <description>The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.</description>
   <guid>oai:arXiv.org:2501.18280v1</guid>
   <category>cs.CL</category>
   <category>cs.AI</category>
   <category>cs.LG</category>
   <category>cs.NE</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang</creator>
  </item>
  <item>
   <title>Adaptive Guidance for Local Training in Heterogeneous Federated Learning</title>
   <link>https://arxiv.org/abs/2410.06490</link>
   <description>Model heterogeneity poses a significant challenge in Heterogeneous Federated Learning (HtFL). In scenarios with diverse model architectures, directly aggregating model parameters is impractical, leading HtFL methods to incorporate an extra objective alongside the original local objective on each client to facilitate collaboration. However, this often results in a mismatch between the extra and local objectives. To resolve this, we propose Federated Learning-to-Guide (FedL2G), a method that adaptively learns to guide local training in a federated manner, ensuring the added objective aligns with each client's original goal. With theoretical guarantees, FedL2G utilizes only first-order derivatives w.r.t. model parameters, achieving a non-convex convergence rate of O(1/T). We conduct extensive experiments across two data heterogeneity and six model heterogeneity settings, using 14 heterogeneous model architectures (e.g., CNNs and ViTs). The results show that FedL2G significantly outperforms seven state-of-the-art methods.</description>
   <guid>oai:arXiv.org:2410.06490v2</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Fri, 31 Jan 2025 00:00:00 -0500</pubdate>
   <announce_type>replace-cross</announce_type>
   <rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</rights>
   <creator>Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao, Qiang Yang</creator>
  </item>
 </channel>
</rss>
