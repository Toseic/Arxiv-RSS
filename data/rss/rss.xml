<?xml version="1.0" ?>
<rss>
 <channel>
  <title>arxiv-rss</title>
  <link></link>
  <description></description>
  <docs></docs>
  <language>en-us</language>
  <lastBuildDate>Tue, 04 Feb 2025 04:13:46 </lastBuildDate>
  <managingEditor></managingEditor>
  <pubDate>Tue, 04 Feb 2025 04:13:46 </pubDate>
  <item>
   <title>SAFL: Structure-Aware Personalized Federated Learning via Client-Specific Clustering and SCSI-Guided Model Pruning</title>
   <link>https://arxiv.org/abs/2501.18659</link>
   <description>Federated Learning (FL) enables clients to collaboratively train machine learning models without sharing local data, preserving privacy in diverse environments. While traditional FL approaches preserve privacy, they often struggle with high computational and communication overhead. To address these issues, model pruning is introduced as a strategy to streamline computations. However, existing pruning methods, when applied solely based on local data, often produce sub-models that inadequately reflect clients' specific tasks due to data insufficiency. To overcome these challenges, this paper introduces SAFL (Structure-Aware Federated Learning), a novel framework that enhances personalized federated learning through client-specific clustering and Similar Client Structure Information (SCSI)-guided model pruning. SAFL employs a two-stage process: initially, it groups clients based on data similarities and uses aggregated pruning criteria to guide the pruning process, facilitating the identification of optimal sub-models. Subsequently, clients train these pruned models and engage in server-based aggregation, ensuring tailored and efficient models for each client. This method significantly reduces computational overhead while improving inference accuracy. Extensive experiments demonstrate that SAFL markedly diminishes model size and improves performance, making it highly effective in federated environments characterized by heterogeneous data.</description>
   <guid>oai:arXiv.org:2501.18659v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Nan Li, Xiaolu Wang, Xiao Du, Puyu Cai, Ting Wang</creator>
  </item>
  <item>
   <title>Continuous-Time Analysis of Federated Averaging</title>
   <link>https://arxiv.org/abs/2501.18870</link>
   <description>Federated averaging (FedAvg) is a popular algorithm for horizontal federated learning (FL), where samples are gathered across different clients and are not shared with each other or a central server. Extensive convergence analysis of FedAvg exists for the discrete iteration setting, guaranteeing convergence for a range of loss functions and varying levels of data heterogeneity. We extend this analysis to the continuous-time setting where the global weights evolve according to a multivariate stochastic differential equation (SDE), which is the first time FedAvg has been studied from the continuous-time perspective. We use techniques from stochastic processes to establish convergence guarantees under different loss functions, some of which are more general than existing work in the discrete setting. We also provide conditions for which FedAvg updates to the server weights can be approximated as normal random variables. Finally, we use the continuous-time formulation to reveal generalization properties of FedAvg.</description>
   <guid>oai:arXiv.org:2501.18870v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <category>math.OC</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Tom Overman, Diego Klabjan</creator>
  </item>
  <item>
   <title>FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling</title>
   <link>https://arxiv.org/abs/2501.19122</link>
   <description>Federated Learning (FL) enables collaborative model training across distributed clients without data sharing, but its high computational and communication demands strain resource-constrained devices. While existing methods use dynamic pruning to improve efficiency by periodically adjusting sparse model topologies while maintaining sparsity, these approaches suffer from issues such as greedy adjustments, unstable topologies, and communication inefficiency, resulting in less robust models and suboptimal performance under data heterogeneity and partial client availability. To address these challenges, we propose Federated Robust pruning via combinatorial Thompson Sampling (FedRTS), a novel framework designed to develop robust sparse models. FedRTS enhances robustness and performance through its Thompson Sampling-based Adjustment (TSAdj) mechanism, which uses probabilistic decisions informed by stable, farsighted information instead of deterministic decisions reliant on unstable and myopic information in previous methods. Extensive experiments demonstrate that FedRTS achieves state-of-the-art performance in computer vision and natural language processing tasks while reducing communication costs, particularly excelling in scenarios with heterogeneous data distributions and partial client participation. Our codes are available at: https://github.com/Little0o0/FedRTS</description>
   <guid>oai:arXiv.org:2501.19122v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Hong Huang, Hai Yang, Yuan Chen, Jiaxun Ye, Dapeng Wu</creator>
  </item>
  <item>
   <title>S-VOTE: Similarity-based Voting for Client Selection in Decentralized Federated Learning</title>
   <link>https://arxiv.org/abs/2501.19279</link>
   <description>Decentralized Federated Learning (DFL) enables collaborative, privacy-preserving model training without relying on a central server. This decentralized approach reduces bottlenecks and eliminates single points of failure, enhancing scalability and resilience. However, DFL also introduces challenges such as suboptimal models with non-IID data distributions, increased communication overhead, and resource usage. Thus, this work proposes S-VOTE, a voting-based client selection mechanism that optimizes resource usage and enhances model performance in federations with non-IID data conditions. S-VOTE considers an adaptive strategy for spontaneous local training that addresses participation imbalance, allowing underutilized clients to contribute without significantly increasing resource costs. Extensive experiments on benchmark datasets demonstrate the S-VOTE effectiveness. More in detail, it achieves lower communication costs by up to 21%, 4-6% faster convergence, and improves local performance by 9-17% compared to baseline methods in some configurations, all while achieving a 14-24% energy consumption reduction. These results highlight the potential of S-VOTE to address DFL challenges in heterogeneous environments.</description>
   <guid>oai:arXiv.org:2501.19279v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Pedro Miguel S\'anchez S\'anchez, Enrique Tom\'as Mart\'inez Beltr\'an, Chao Feng, G\'er\^ome Bovet, Gregorio Mart\'inez P\'erez, Alberto Huertas Celdr\'an</creator>
  </item>
  <item>
   <title>FL-APU: A Software Architecture to Ease Practical Implementation of Cross-Silo Federated Learning</title>
   <link>https://arxiv.org/abs/2501.19091</link>
   <description>Federated Learning (FL) is an upcoming technology that is increasingly applied in real-world applications. Early applications focused on cross-device scenarios, where many participants with limited resources train machine learning (ML) models together, e.g., in the case of Google's GBoard. Contrarily, cross-silo scenarios have only few participants but with many resources, e.g., in the healthcare domain. Despite such early efforts, FL is still rarely used in practice and best practices are, hence, missing. For new applications, in our case inter-organizational cross-silo applications, overcoming this lack of role models is a significant challenge.
  In order to ease the use of FL in real-world cross-silo applications, we here propose a scenario-based architecture for the practical use of FL in the context of multiple companies collaborating to improve the quality of their ML models. The architecture emphasizes the collaboration between the participants and the FL server and extends basic interactions with domain-specific features. First, it combines governance with authentication, creating an environment where only trusted participants can join. Second, it offers traceability of governance decisions and tracking of training processes, which are also crucial in a production environment. Beyond presenting the architectural design, we analyze requirements for the real-world use of FL and evaluate the architecture with a scenario-based analysis method.</description>
   <guid>oai:arXiv.org:2501.19091v1</guid>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>F. Stricker, J. A. Peregrina, D. Bermbach, C. Zirpins</creator>
  </item>
  <item>
   <title>FL-APU: A Software Architecture to Ease Practical Implementation of Cross-Silo Federated Learning</title>
   <link>https://arxiv.org/abs/2501.19091</link>
   <description>Federated Learning (FL) is an upcoming technology that is increasingly applied in real-world applications. Early applications focused on cross-device scenarios, where many participants with limited resources train machine learning (ML) models together, e.g., in the case of Google's GBoard. Contrarily, cross-silo scenarios have only few participants but with many resources, e.g., in the healthcare domain. Despite such early efforts, FL is still rarely used in practice and best practices are, hence, missing. For new applications, in our case inter-organizational cross-silo applications, overcoming this lack of role models is a significant challenge.
  In order to ease the use of FL in real-world cross-silo applications, we here propose a scenario-based architecture for the practical use of FL in the context of multiple companies collaborating to improve the quality of their ML models. The architecture emphasizes the collaboration between the participants and the FL server and extends basic interactions with domain-specific features. First, it combines governance with authentication, creating an environment where only trusted participants can join. Second, it offers traceability of governance decisions and tracking of training processes, which are also crucial in a production environment. Beyond presenting the architectural design, we analyze requirements for the real-world use of FL and evaluate the architecture with a scenario-based analysis method.</description>
   <guid>oai:arXiv.org:2501.19091v1</guid>
   <category>cs.DC</category>
   <category>cs.LG</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>new</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>F. Stricker, J. A. Peregrina, D. Bermbach, C. Zirpins</creator>
  </item>
  <item>
   <title>Continuous-Time Analysis of Federated Averaging</title>
   <link>https://arxiv.org/abs/2501.18870</link>
   <description>Federated averaging (FedAvg) is a popular algorithm for horizontal federated learning (FL), where samples are gathered across different clients and are not shared with each other or a central server. Extensive convergence analysis of FedAvg exists for the discrete iteration setting, guaranteeing convergence for a range of loss functions and varying levels of data heterogeneity. We extend this analysis to the continuous-time setting where the global weights evolve according to a multivariate stochastic differential equation (SDE), which is the first time FedAvg has been studied from the continuous-time perspective. We use techniques from stochastic processes to establish convergence guarantees under different loss functions, some of which are more general than existing work in the discrete setting. We also provide conditions for which FedAvg updates to the server weights can be approximated as normal random variables. Finally, we use the continuous-time formulation to reveal generalization properties of FedAvg.</description>
   <guid>oai:arXiv.org:2501.18870v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <category>math.OC</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Tom Overman, Diego Klabjan</creator>
  </item>
  <item>
   <title>S-VOTE: Similarity-based Voting for Client Selection in Decentralized Federated Learning</title>
   <link>https://arxiv.org/abs/2501.19279</link>
   <description>Decentralized Federated Learning (DFL) enables collaborative, privacy-preserving model training without relying on a central server. This decentralized approach reduces bottlenecks and eliminates single points of failure, enhancing scalability and resilience. However, DFL also introduces challenges such as suboptimal models with non-IID data distributions, increased communication overhead, and resource usage. Thus, this work proposes S-VOTE, a voting-based client selection mechanism that optimizes resource usage and enhances model performance in federations with non-IID data conditions. S-VOTE considers an adaptive strategy for spontaneous local training that addresses participation imbalance, allowing underutilized clients to contribute without significantly increasing resource costs. Extensive experiments on benchmark datasets demonstrate the S-VOTE effectiveness. More in detail, it achieves lower communication costs by up to 21%, 4-6% faster convergence, and improves local performance by 9-17% compared to baseline methods in some configurations, all while achieving a 14-24% energy consumption reduction. These results highlight the potential of S-VOTE to address DFL challenges in heterogeneous environments.</description>
   <guid>oai:arXiv.org:2501.19279v1</guid>
   <category>cs.LG</category>
   <category>cs.DC</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Pedro Miguel S\'anchez S\'anchez, Enrique Tom\'as Mart\'inez Beltr\'an, Chao Feng, G\'er\^ome Bovet, Gregorio Mart\'inez P\'erez, Alberto Huertas Celdr\'an</creator>
  </item>
  <item>
   <title>Personalized Federated Learning on Data with Dynamic Heterogeneity under Limited Storage</title>
   <link>https://arxiv.org/abs/2410.01502</link>
   <description>Recently, a large number of data sources opened up by informatization intensify the data heterogeneity, the faster speed of data generation and the gradual implementation of data regulations limit the storage time of data. In personalized Federated Learning (pFL), clients train customized models to meet their personal objectives. However, due to the time-varying local data heterogeneity and the inaccessibility of previous data, existing pFL methods not only fail to solve the catastrophic forgetting of local models, but also difficult to estimate the degree of collaboration between clients. To address this issue, our core idea is a low consumption and high-quality generative replay architecture. Specifically, we decouple the generator by category to reduce the generation error of each category while mitigating catastrophic forgetting, use local model to improving the quality of generated data and reducing the update frequency of generator, and propose a local data reconstruction scheme to reduce data generation while adjusting the proportion of data categories. Based on above, we propose our pFL framework, pFedGRP, to achieve personalized aggregation and local knowledge transfer. Comprehensive experiments on five datasets with multiple settings show the superiority of pFedGRP over eight baseline methods.</description>
   <guid>oai:arXiv.org:2410.01502v2</guid>
   <category>cs.DC</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>replace</announce_type>
   <rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</rights>
   <creator>Sixing Tan, Xianmin Liu</creator>
  </item>
  <item>
   <title>FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling</title>
   <link>https://arxiv.org/abs/2501.19122</link>
   <description>Federated Learning (FL) enables collaborative model training across distributed clients without data sharing, but its high computational and communication demands strain resource-constrained devices. While existing methods use dynamic pruning to improve efficiency by periodically adjusting sparse model topologies while maintaining sparsity, these approaches suffer from issues such as greedy adjustments, unstable topologies, and communication inefficiency, resulting in less robust models and suboptimal performance under data heterogeneity and partial client availability. To address these challenges, we propose Federated Robust pruning via combinatorial Thompson Sampling (FedRTS), a novel framework designed to develop robust sparse models. FedRTS enhances robustness and performance through its Thompson Sampling-based Adjustment (TSAdj) mechanism, which uses probabilistic decisions informed by stable, farsighted information instead of deterministic decisions reliant on unstable and myopic information in previous methods. Extensive experiments demonstrate that FedRTS achieves state-of-the-art performance in computer vision and natural language processing tasks while reducing communication costs, particularly excelling in scenarios with heterogeneous data distributions and partial client participation. Our codes are available at: https://github.com/Little0o0/FedRTS</description>
   <guid>oai:arXiv.org:2501.19122v1</guid>
   <category>cs.LG</category>
   <category>cs.AI</category>
   <pubdate>Mon, 03 Feb 2025 00:00:00 -0500</pubdate>
   <announce_type>cross</announce_type>
   <rights>http://creativecommons.org/licenses/by/4.0/</rights>
   <creator>Hong Huang, Hai Yang, Yuan Chen, Jiaxun Ye, Dapeng Wu</creator>
  </item>
 </channel>
</rss>
