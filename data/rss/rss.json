[{"title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents", "link": "https://arxiv.org/abs/2501.14654", "description": "Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging. To address this gap, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems. MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is still substantial space for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories. MedAgentBench establishes this and is publicly available at https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain.", "guid": "oai:arXiv.org:2501.14654v2", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, Jonathan H. Chen"}, {"title": "Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation", "link": "https://arxiv.org/abs/2502.07951", "description": "Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.", "guid": "oai:arXiv.org:2502.07951v1", "categories": ["cs.CV", "cs.DC", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xinyi Tan, Jiacheng Wang, Liansheng Wang"}, {"title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "link": "https://arxiv.org/abs/2502.05206", "description": "The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.", "guid": "oai:arXiv.org:2502.05206v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang"}, {"title": "An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models", "link": "https://arxiv.org/abs/2502.08008", "description": "Federated learning (FL) enhances privacy by keeping user data on local devices. However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data. This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications. Differential Privacy (DP) is considered the gold standard for safeguarding user data. However, DP guarantees are highly conservative, providing worst-case privacy guarantees. This can result in overestimating privacy needs, which may compromise the model's accuracy. Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts. This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem. In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility. Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate. To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL. We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions. As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon = 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage.", "guid": "oai:arXiv.org:2502.08008v1", "categories": ["cs.LG", "cs.CR"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Kasra Ahmadi, Rouzbeh Behnia, Reza Ebrahimi, Mehran Mozaffari Kermani, Jeremiah Birrell, Jason Pacheco, Attila A Yavuz"}, {"title": "Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning", "link": "https://arxiv.org/abs/2502.08024", "description": "Initializing with pre-trained models when learning on downstream tasks is becoming standard practice in machine learning. Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream training is performed at the edge clients with heterogeneous data distribution. These works show that starting from a pre-trained model can substantially reduce the adverse impact of data heterogeneity on the test performance of a model trained in a federated setting, with no changes to the standard FedAvg training algorithm. In this work, we provide a deeper theoretical understanding of this phenomenon. To do so, we study the class of two-layer convolutional neural networks (CNNs) and provide bounds on the training error convergence and test error of such a network trained with FedAvg. We introduce the notion of aligned and misaligned filters at initialization and show that the data heterogeneity only affects learning on misaligned filters. Starting with a pre-trained model typically results in fewer misaligned filters at initialization, thus producing a lower test error even when the model is trained in a federated setting with data heterogeneity. Experiments in synthetic settings and practical FL training on CNNs verify our theoretical findings.", "guid": "oai:arXiv.org:2502.08024v1", "categories": ["cs.LG", "cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Divyansh Jhunjhunwala, Pranay Sharma, Zheng Xu, Gauri Joshi"}, {"title": "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly", "link": "https://arxiv.org/abs/2502.08160", "description": "Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data. Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited. To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap. We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions. Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions. Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications.", "guid": "oai:arXiv.org:2502.08160v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Zhaomin Wu, Zhen Qin, Junyi Hou, Haodong Zhao, Qinbin Li, Bingsheng He, Lixin Fan"}, {"title": "Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency", "link": "https://arxiv.org/abs/2502.08206", "description": "Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date. Second, we observe that naively optimizing this metric leads us to slow down the system drastically by overemphazing staleness at the detriment of throughput. This motivates us to introduce an alternative metric that also takes system speed into account, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show that these optimizations enhance accuracy by 10% to 30%.", "guid": "oai:arXiv.org:2502.08206v1", "categories": ["cs.LG", "cs.PF", "math.OC", "math.PR"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Abdelkrim Alahyane (LAAS-SARA, LAAS-RISC, LAAS), C\\'eline Comte (CNRS, LAAS-SARA, LAAS-RISC, LAAS), Matthieu Jonckheere (CNRS, LAAS-SARA, LAAS-RISC, LAAS), \\'Eric Moulines (X)"}, {"title": "One-Shot Federated Learning with Classifier-Free Diffusion Models", "link": "https://arxiv.org/abs/2502.08488", "description": "Federated learning (FL) enables collaborative learning without data centralization but introduces significant communication costs due to multiple communication rounds between clients and the server. One-shot federated learning (OSFL) addresses this by forming a global model with a single communication round, often relying on the server's model distillation or auxiliary dataset generation - often through pre-trained diffusion models (DMs). Existing DM-assisted OSFL methods, however, typically employ classifier-guided DMs, which require training auxiliary classifier models at each client, introducing additional computation overhead. This work introduces OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses foundation models to devise category-specific data representations at each client, seamlessly integrated into a classifier-free diffusion model pipeline for server-side data generation. OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on four benchmarking datasets while reducing the communication load by at least 99%.", "guid": "oai:arXiv.org:2502.08488v1", "categories": ["cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Obaidullah Zaland, Shutong Jin, Florian T. Pokorny, Monowar Bhuyan"}, {"title": "FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices", "link": "https://arxiv.org/abs/2502.08518", "description": "Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups.", "guid": "oai:arXiv.org:2502.08518v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dezhong Yao, Yuexin Shi, Tongtong Liu, Zhiqiang Xu"}, {"title": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning", "link": "https://arxiv.org/abs/2502.08577", "description": "In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.", "guid": "oai:arXiv.org:2502.08577v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Davide Domini, Gianluca Aguzzi, Lukas Esterle, Mirko Viroli"}, {"title": "Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation", "link": "https://arxiv.org/abs/2502.07951", "description": "Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.", "guid": "oai:arXiv.org:2502.07951v1", "categories": ["cs.CV", "cs.DC", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xinyi Tan, Jiacheng Wang, Liansheng Wang"}, {"title": "Unveiling Client Privacy Leakage from Public Dataset Usage in Federated Distillation", "link": "https://arxiv.org/abs/2502.08001", "description": "Federated Distillation (FD) has emerged as a popular federated training framework, enabling clients to collaboratively train models without sharing private data. Public Dataset-Assisted Federated Distillation (PDA-FD), which leverages public datasets for knowledge sharing, has become widely adopted. Although PDA-FD enhances privacy compared to traditional Federated Learning, we demonstrate that the use of public datasets still poses significant privacy risks to clients' private training data. This paper presents the first comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious server. We show that the server can exploit clients' inference results on public datasets to extract two critical types of private information: label distributions and membership information of the private training dataset. To quantify these vulnerabilities, we introduce two novel attacks specifically designed for the PDA-FD setting: a label distribution inference attack and innovative membership inference methods based on Likelihood Ratio Attack (LiRA). Through extensive evaluation of three representative PDA-FD frameworks (FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance, with label distribution attacks reaching minimal KL-divergence and membership inference attacks maintaining high True Positive Rates under low False Positive Rate constraints. Our findings reveal significant privacy risks in current PDA-FD frameworks and emphasize the need for more robust privacy protection mechanisms in collaborative learning systems.", "guid": "oai:arXiv.org:2502.08001v1", "categories": ["cs.CR", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Haonan Shi, Tu Ouyang, An Wang"}, {"title": "SLVR: Securely Leveraging Client Validation for Robust Federated Learning", "link": "https://arxiv.org/abs/2502.08055", "description": "Federated Learning (FL) enables collaborative model training while keeping client data private. However, exposing individual client updates makes FL vulnerable to reconstruction attacks. Secure aggregation mitigates such privacy risks but prevents the server from verifying the validity of each client update, creating a privacy-robustness tradeoff. Recent efforts attempt to address this tradeoff by enforcing checks on client updates using zero-knowledge proofs, but they support limited predicates and often depend on public validation data. We propose SLVR, a general framework that securely leverages clients' private data through secure multi-party computation. By utilizing clients' data, SLVR not only eliminates the need for public validation data, but also enables a wider range of checks for robustness, including cross-client accuracy validation. It also adapts naturally to distribution shifts in client data as it can securely refresh its validation data up-to-date. Our empirical evaluations show that SLVR improves robustness against model poisoning attacks, particularly outperforming existing methods by up to 50% under adaptive attacks. Additionally, SLVR demonstrates effective adaptability and stable convergence under various distribution shift scenarios.", "guid": "oai:arXiv.org:2502.08055v1", "categories": ["cs.CR", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jihye Choi, Sai Rahul Rachuri, Ke Wang, Somesh Jha, Yizhen Wang"}, {"title": "Local Differential Privacy is Not Enough: A Sample Reconstruction Attack against Federated Learning with Local Differential Privacy", "link": "https://arxiv.org/abs/2502.08151", "description": "Reconstruction attacks against federated learning (FL) aim to reconstruct users' samples through users' uploaded gradients. Local differential privacy (LDP) is regarded as an effective defense against various attacks, including sample reconstruction in FL, where gradients are clipped and perturbed. Existing attacks are ineffective in FL with LDP since clipped and perturbed gradients obliterate most sample information for reconstruction. Besides, existing attacks embed additional sample information into gradients to improve the attack effect and cause gradient expansion, leading to a more severe gradient clipping in FL with LDP. In this paper, we propose a sample reconstruction attack against LDP-based FL with any target models to reconstruct victims' sensitive samples to illustrate that FL with LDP is not flawless. Considering gradient expansion in reconstruction attacks and noise in LDP, the core of the proposed attack is gradient compression and reconstructed sample denoising. For gradient compression, an inference structure based on sample characteristics is presented to reduce redundant gradients against LDP. For reconstructed sample denoising, we artificially introduce zero gradients to observe noise distribution and scale confidence interval to filter the noise. Theoretical proof guarantees the effectiveness of the proposed attack. Evaluations show that the proposed attack is the only attack that reconstructs victims' training samples in LDP-based FL and has little impact on the target model's accuracy. We conclude that LDP-based FL needs further improvements to defend against sample reconstruction attacks effectively.", "guid": "oai:arXiv.org:2502.08151v1", "categories": ["cs.CR", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Zhichao You, Xuewen Dong, Shujun Li, Ximeng Liu, Siqi Ma, Yulong Shen"}, {"title": "Graph Foundation Models for Recommendation: A Comprehensive Survey", "link": "https://arxiv.org/abs/2502.08346", "description": "Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.", "guid": "oai:arXiv.org:2502.08346v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi"}, {"title": "Federated Learning over Connected Modes", "link": "https://arxiv.org/abs/2403.03333", "description": "Statistical heterogeneity in federated learning poses two major challenges: slow global training due to conflicting gradient signals, and the need of personalization for local distributions. In this work, we tackle both challenges by leveraging recent advances in \\emph{linear mode connectivity} -- identifying a linearly connected low-loss region in the parameter space of neural networks, which we call solution simplex. We propose federated learning over connected modes (\\textsc{Floco}), where clients are assigned local subregions in this simplex based on their gradient signals, and together learn the shared global solution simplex. This allows personalization of the client models to fit their local distributions within the degrees of freedom in the solution simplex and homogenizes the update signals for the global simplex training. Our experiments show that \\textsc{Floco} accelerates the global training process, and significantly improves the local accuracy with minimal computational overhead in cross-silo federated learning settings.", "guid": "oai:arXiv.org:2403.03333v4", "categories": ["cs.LG", "cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dennis Grinwald, Philipp Wiesner, Shinichi Nakajima"}, {"title": "Federated Learning in Chemical Engineering: A Tutorial on a Framework for Privacy-Preserving Collaboration Across Distributed Data Sources", "link": "https://arxiv.org/abs/2411.16737", "description": "Federated Learning (FL) is a decentralized machine learning approach that has gained attention for its potential to enable collaborative model training across clients while protecting data privacy, making it an attractive solution for the chemical industry. This work aims to provide the chemical engineering community with an accessible introduction to the discipline. Supported by a hands-on tutorial and a comprehensive collection of examples, it explores the application of FL in tasks such as manufacturing optimization, multimodal data integration, and drug discovery while addressing the unique challenges of protecting proprietary information and managing distributed datasets. The tutorial was built using key frameworks such as $\\texttt{Flower}$ and $\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers with the right tools to adopt FL in their specific needs. We compare the performance of FL against centralized learning across three different datasets relevant to chemical engineering applications, demonstrating that FL will often maintain or improve classification performance, particularly for complex and heterogeneous data. We conclude with an outlook on the open challenges in federated learning to be tackled and current approaches designed to remediate and improve this framework.", "guid": "oai:arXiv.org:2411.16737v2", "categories": ["cs.LG", "cs.DC", "cs.NE"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Siddhant Dutta, Iago Leal de Freitas, Pedro Maciel Xavier, Claudio Miceli de Farias, David Esteban Bernal Neira"}, {"title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents", "link": "https://arxiv.org/abs/2501.14654", "description": "Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging. To address this gap, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems. MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is still substantial space for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories. MedAgentBench establishes this and is publicly available at https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain.", "guid": "oai:arXiv.org:2501.14654v2", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, Jonathan H. Chen"}, {"title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment", "link": "https://arxiv.org/abs/2502.07903", "description": "Disaggregating the prefill and decoding phases represents an effective new paradigm for generative inference of large language models (LLM), which eliminates prefill-decoding interference and optimizes resource allocation. However, it is still an open problem about how to deploy the disaggregated inference paradigm across a group of heterogeneous GPUs, which can be an economical alternative to deployment over homogeneous high-performance GPUs. Towards this end, we introduce HexGen-2, a distributed system for efficient and economical LLM serving on heterogeneous GPUs following the disaggregated paradigm. Built on top of HexGen, the core component of HexGen-2 is a scheduling algorithm that formalizes the allocation of disaggregated LLM inference computations and communications over heterogeneous GPUs and network connections as a constraint optimization problem. We leverage the graph partitioning and max-flow algorithms to co-optimize resource allocation, parallel strategies for distinct inference phases, and the efficiency of inter-phase key-value (KV) cache communications. We conduct extensive experiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models in various real-world settings, the results reveal that HexGen-2 delivers up to a 2.0 times and on average a 1.3 times improvement in serving throughput, reduces the average inference latency by 1.5 times compared with state-of-the-art systems given the same price budget, and achieves comparable inference performance with a 30% lower price budget.", "guid": "oai:arXiv.org:2502.07903v1", "categories": ["cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Youhe Jiang, Ran Yan, Binhang Yuan"}, {"title": "Memory Offloading for Large Language Model Inference with Latency SLO Guarantees", "link": "https://arxiv.org/abs/2502.08182", "description": "Offloading large language models (LLMs) state to host memory during inference promises to reduce operational costs by supporting larger models, longer inputs, and larger batch sizes. However, the design of existing memory offloading mechanisms does not take latency service-level objectives (SLOs) into consideration. As a result, they either lead to frequent SLO violations or underutilize host memory, thereby incurring economic loss and thus defeating the purpose of memory offloading.\n  This paper presents Select-N, a latency-SLO-aware memory offloading system for LLM serving. A key challenge in designing Select-N is to reconcile the tension between meeting SLOs and maximizing host memory usage. Select-N overcomes it by exploiting a unique characteristic of modern LLMs: during serving, the computation time of each decoder layer is deterministic. Leveraging this, Select-N introduces offloading interval, an internal tunable knob that captures the tradeoff between SLOs and host memory usage, thereby reducing the aforementioned challenge to pick an optimal offloading interval. With that, Select-N proposes a two-stage approach to automatically pick the offloading interval. The first stage is offline that generates the range of optimal offloading interval, while the second stage adjusts offloading interval at the granularity of inference iteration based on runtime hardware status. Our evaluation shows that Select-N consistently meets SLOs and improves the serving throughput over existing mechanisms by 1.85X due to maximizing the use of host memory.", "guid": "oai:arXiv.org:2502.08182v1", "categories": ["cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Chenxiang Ma, Zhisheng Ye, Hanyu Zhao, Zehua Yang, Tianhao Fu, Jiaxun Han, Jie Zhang, Yingwei Luo, Xiaolin Wang, Zhenlin Wang, Yong Li, Diyu Zhou"}, {"title": "Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation", "link": "https://arxiv.org/abs/2502.07951", "description": "Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.", "guid": "oai:arXiv.org:2502.07951v1", "categories": ["cs.CV", "cs.DC", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xinyi Tan, Jiacheng Wang, Liansheng Wang"}, {"title": "Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning", "link": "https://arxiv.org/abs/2502.08024", "description": "Initializing with pre-trained models when learning on downstream tasks is becoming standard practice in machine learning. Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream training is performed at the edge clients with heterogeneous data distribution. These works show that starting from a pre-trained model can substantially reduce the adverse impact of data heterogeneity on the test performance of a model trained in a federated setting, with no changes to the standard FedAvg training algorithm. In this work, we provide a deeper theoretical understanding of this phenomenon. To do so, we study the class of two-layer convolutional neural networks (CNNs) and provide bounds on the training error convergence and test error of such a network trained with FedAvg. We introduce the notion of aligned and misaligned filters at initialization and show that the data heterogeneity only affects learning on misaligned filters. Starting with a pre-trained model typically results in fewer misaligned filters at initialization, thus producing a lower test error even when the model is trained in a federated setting with data heterogeneity. Experiments in synthetic settings and practical FL training on CNNs verify our theoretical findings.", "guid": "oai:arXiv.org:2502.08024v1", "categories": ["cs.LG", "cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Divyansh Jhunjhunwala, Pranay Sharma, Zheng Xu, Gauri Joshi"}, {"title": "FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices", "link": "https://arxiv.org/abs/2502.08518", "description": "Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups.", "guid": "oai:arXiv.org:2502.08518v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dezhong Yao, Yuexin Shi, Tongtong Liu, Zhiqiang Xu"}, {"title": "FedOptimus: Optimizing Vertical Federated Learning for Scalability and Efficiency", "link": "https://arxiv.org/abs/2502.04243", "description": "Federated learning (FL) is a collaborative machine learning paradigm which ensures data privacy by training models across distributed datasets without centralizing sensitive information. Vertical Federated Learning (VFL), a kind of FL training method, facilitates collaboration among participants with each client having received a different feature space of a shared user set. VFL thus, proves invaluable in privacy-sensitive domains such as finance and healthcare. Despite its inherent advantages, VFL faced challenges including communication bottlenecks, computational inefficiency, and slow convergence due to non-IID data distributions. This paper introduces FedOptimus, a robust Multi-VFL framework integrating advanced techniques for improved model efficiency and scalability. FedOptimus leverages a Mutual Information (MI)-based client selection to prioritize high-contribution participants, reducing computational overhead. Further, it incorporates server-side momentum techniques like FedAvgM and SLOWMO to stabilize updates and accelerate convergence on heterogeneous data. Additionally, performing K-Step Averaging minimizes communication costs while maintaining model performance. FedOptimus proves to be superior in performance on benchmark datasets such as CIFAR-10, MNIST, and FMNIST, showcasing its scalability and effectiveness in real-world multi-server, multi-client settings. By unifying advanced optimization methods, FedOptimus sets a new standard for efficient and scalable Vertical Federated Learning frameworks, paving the way for broader adoption in complex, privacy-sensitive domains.", "guid": "oai:arXiv.org:2502.04243v2", "categories": ["cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Nikita Shrivastava, Drishya Uniyal, Bapi Chatterjee"}, {"title": "Federated Learning over Connected Modes", "link": "https://arxiv.org/abs/2403.03333", "description": "Statistical heterogeneity in federated learning poses two major challenges: slow global training due to conflicting gradient signals, and the need of personalization for local distributions. In this work, we tackle both challenges by leveraging recent advances in \\emph{linear mode connectivity} -- identifying a linearly connected low-loss region in the parameter space of neural networks, which we call solution simplex. We propose federated learning over connected modes (\\textsc{Floco}), where clients are assigned local subregions in this simplex based on their gradient signals, and together learn the shared global solution simplex. This allows personalization of the client models to fit their local distributions within the degrees of freedom in the solution simplex and homogenizes the update signals for the global simplex training. Our experiments show that \\textsc{Floco} accelerates the global training process, and significantly improves the local accuracy with minimal computational overhead in cross-silo federated learning settings.", "guid": "oai:arXiv.org:2403.03333v4", "categories": ["cs.LG", "cs.DC"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dennis Grinwald, Philipp Wiesner, Shinichi Nakajima"}, {"title": "Federated Learning in Chemical Engineering: A Tutorial on a Framework for Privacy-Preserving Collaboration Across Distributed Data Sources", "link": "https://arxiv.org/abs/2411.16737", "description": "Federated Learning (FL) is a decentralized machine learning approach that has gained attention for its potential to enable collaborative model training across clients while protecting data privacy, making it an attractive solution for the chemical industry. This work aims to provide the chemical engineering community with an accessible introduction to the discipline. Supported by a hands-on tutorial and a comprehensive collection of examples, it explores the application of FL in tasks such as manufacturing optimization, multimodal data integration, and drug discovery while addressing the unique challenges of protecting proprietary information and managing distributed datasets. The tutorial was built using key frameworks such as $\\texttt{Flower}$ and $\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers with the right tools to adopt FL in their specific needs. We compare the performance of FL against centralized learning across three different datasets relevant to chemical engineering applications, demonstrating that FL will often maintain or improve classification performance, particularly for complex and heterogeneous data. We conclude with an outlook on the open challenges in federated learning to be tackled and current approaches designed to remediate and improve this framework.", "guid": "oai:arXiv.org:2411.16737v2", "categories": ["cs.LG", "cs.DC", "cs.NE"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Siddhant Dutta, Iago Leal de Freitas, Pedro Maciel Xavier, Claudio Miceli de Farias, David Esteban Bernal Neira"}, {"title": "Satisfaction-Aware Incentive Scheme for Federated Learning in Industrial Metaverse: DRL-Based Stackbelberg Game Approach", "link": "https://arxiv.org/abs/2502.06909", "description": "Industrial Metaverse leverages the Industrial Internet of Things (IIoT) to integrate data from diverse devices, employing federated learning and meta-computing to train models in a distributed manner while ensuring data privacy. Achieving an immersive experience for industrial Metaverse necessitates maintaining a balance between model quality and training latency. Consequently, a primary challenge in federated learning tasks is optimizing overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency. Additionally, the satisfaction function is incorporated into the utility functions to incentivize node participation in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for industrial Metaverse. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves at least 23.7% utility compared to existing schemes without compromising model accuracy.", "guid": "oai:arXiv.org:2502.06909v1", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xiaohuan Li, Shaowen Qin, Xin Tang, Jiawen Kang, Jin Ye, Zhonghua Zhao, Dusit Niyato"}, {"title": "Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning", "link": "https://arxiv.org/abs/2502.06917", "description": "Federated Learning presents a nascent approach to machine learning, enabling collaborative model training across decentralized devices while safeguarding data privacy. However, its distributed nature renders it susceptible to adversarial attacks. Integrating blockchain technology with Federated Learning offers a promising avenue to enhance security and integrity. In this paper, we tackle the potential of blockchain in defending Federated Learning against adversarial attacks. First, we test Proof of Federated Learning, a well known consensus mechanism designed ad-hoc to federated contexts, as a defense mechanism demonstrating its efficacy against Byzantine and backdoor attacks when at least one miner remains uncompromised. Second, we propose Krum Federated Chain, a novel defense strategy combining Krum and Proof of Federated Learning, valid to defend against any configuration of Byzantine or backdoor attacks, even when all miners are compromised. Our experiments conducted on image classification datasets validate the effectiveness of our proposed approaches.", "guid": "oai:arXiv.org:2502.06917v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Mario Garc\\'ia-M\\'arquez, Nuria Rodr\\'iguez-Barroso, M. Victoria Luz\\'on, Francisco Herrera"}, {"title": "Federated Continual Learning: Concepts, Challenges, and Solutions", "link": "https://arxiv.org/abs/2502.07059", "description": "Federated Continual Learning (FCL) has emerged as a robust solution for collaborative model training in dynamic environments, where data samples are continuously generated and distributed across multiple devices. This survey provides a comprehensive review of FCL, focusing on key challenges such as heterogeneity, model stability, communication overhead, and privacy preservation. We explore various forms of heterogeneity and their impact on model performance. Solutions to non-IID data, resource-constrained platforms, and personalized learning are reviewed in an effort to show the complexities of handling heterogeneous data distributions. Next, we review techniques for ensuring model stability and avoiding catastrophic forgetting, which are critical in non-stationary environments. Privacy-preserving techniques are another aspect of FCL that have been reviewed in this work. This survey has integrated insights from federated learning and continual learning to present strategies for improving the efficacy and scalability of FCL systems, making it applicable to a wide range of real-world scenarios.", "guid": "oai:arXiv.org:2502.07059v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Parisa Hamedi, Roozbeh Razavi-Far, Ehsan Hallaji"}, {"title": "PFedDST: Personalized Federated Learning with Decentralized Selection Training", "link": "https://arxiv.org/abs/2502.07750", "description": "Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems.", "guid": "oai:arXiv.org:2502.07750v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Mengchen Fan, Keren Li, Tianyun Zhang, Qing Tian, Baocheng Geng"}, {"title": "Enhancing Security and Privacy in Federated Learning using Low-Dimensional Update Representation and Proximity-Based Defense", "link": "https://arxiv.org/abs/2405.18802", "description": "Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized. Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, particularly against curious or malicious adversaries. In this paper, we introduce a novel framework named \\underline{F}ederated \\underline{L}earning with Low-Dimensional \\underline{U}pdate \\underline{R}epresentation and \\underline{P}roximity-Based defense (FLURP), designed to address privacy preservation and resistance to Byzantine attacks in distributed learning environments. FLURP employs $\\mathsf{LinfSample}$ method, enabling clients to compute the $l_{\\infty}$ norm across sliding windows of updates, resulting in a Low-Dimensional Update Representation (LUR). Calculating the shared distance matrix among LURs, rather than updates, significantly reduces the overhead of Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and poisoned updates. Additionally, FLURP integrates a privacy-preserving proximity-based defense mechanism utilizing optimized SMPC protocols to minimize communication rounds. Our experiments demonstrate FLURP's effectiveness in countering Byzantine adversaries with low communication and runtime overhead. FLURP offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security.", "guid": "oai:arXiv.org:2405.18802v2", "categories": ["cs.CR", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Wenjie Li, Kai Fan, Jingyuan Zhang, Hui Li, Wei Yang Bryan Lim, Qiang Yang"}, {"title": "pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning", "link": "https://arxiv.org/abs/2409.05701", "description": "Federated Learning (FL) offers a decentralized approach to model training, where data remains local and only model parameters are shared between the clients and the central server. Traditional methods, such as Federated Averaging (FedAvg), linearly aggregate these parameters which are usually trained on heterogeneous data distributions, potentially overlooking the complex, high-dimensional nature of the parameter space. This can result in degraded performance of the aggregated model. While personalized FL approaches can mitigate the heterogeneous data issue to some extent, the limitation of linear aggregation remains unresolved. To alleviate this issue, we investigate the generative approach of diffusion model and propose a novel generative parameter aggregation framework for personalized FL, \\texttt{pFedGPA}. In this framework, we deploy a diffusion model on the server to integrate the diverse parameter distributions and propose a parameter inversion method to efficiently generate a set of personalized parameters for each client. This inversion method transforms the uploaded parameters into a latent code, which is then aggregated through denoising sampling to produce the final personalized parameters. By encoding the dependence of a client's model parameters on the specific data distribution using the high-capacity diffusion model, \\texttt{pFedGPA} can effectively decouple the complexity of the overall distribution of all clients' model parameters from the complexity of each individual client's parameter distribution. Our experimental results consistently demonstrate the superior performance of the proposed method across multiple datasets, surpassing baseline approaches.", "guid": "oai:arXiv.org:2409.05701v3", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jiahao Lai, Jiaqi Li, Jian Xu, Yanru Wu, Boshi Tang, Siqi Chen, Yongfeng Huang, Wenbo Ding, Yang Li"}, {"title": "LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild", "link": "https://arxiv.org/abs/2410.13919", "description": "Attacks powered by Large Language Model (LLM) agents represent a growing threat to modern cybersecurity. To address this concern, we present LLM Honeypot, a system designed to monitor autonomous AI hacking agents. By augmenting a standard SSH honeypot with prompt injection and time-based analysis techniques, our framework aims to distinguish LLM agents among all attackers. Over a trial deployment of about three months in a public environment, we collected 8,130,731 hacking attempts and 8 potential AI agents. Our work demonstrates the emergence of AI-driven threats and their current level of usage, serving as an early warning of malicious LLM agents in the wild.", "guid": "oai:arXiv.org:2410.13919v2", "categories": ["cs.CR", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": " Reworr, Dmitrii Volkov"}, {"title": "GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching", "link": "https://arxiv.org/abs/2501.00135", "description": "Quantum computing is an exciting non-Von Neumann paradigm, offering provable speedups over classical computing for specific problems. However, the practical limits of classical simulatability for quantum circuits remain unclear, especially with current noisy quantum devices. In this work, we explore the potential of leveraging Large Language Models (LLMs) to simulate the output of a quantum Turing machine using Grover's quantum circuits, known to provide quadratic speedups over classical counterparts. To this end, we developed GroverGPT, a specialized model based on LLaMA's 8-billion-parameter architecture, trained on over 15 trillion tokens. Unlike brute-force state-vector simulations, which demand substantial computational resources, GroverGPT employs pattern recognition to approximate quantum search algorithms without explicitly representing quantum states. Analyzing 97K quantum search instances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy), achieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on 4-qubit or larger datasets. It also demonstrated strong generalization, surpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to 6-qubit data. Analysis indicates GroverGPT captures quantum features of Grover's search rather than classical patterns, supported by novel prompting strategies to enhance performance. Although accuracy declines with increasing system size, these findings offer insights into the practical boundaries of classical simulatability. This work suggests task-specific LLMs can surpass general-purpose models like GPT-4o in quantum algorithm learning and serve as powerful tools for advancing quantum research.", "guid": "oai:arXiv.org:2501.00135v3", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Haoran Wang, Pingzhi Li, Min Chen, Jinglei Cheng, Junyu Liu, Tianlong Chen"}, {"title": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters", "link": "https://arxiv.org/abs/2502.04315", "description": "Recent advances in large language models (LLMs) have shown remarkable performance across diverse tasks. However, these models are typically deployed with fixed weights, which limits their ability to adapt dynamically to the variability inherent in real-world data during inference. This paper introduces ChameleonLLM, a novel framework that enables inference-time adaptation of LLMs by leveraging batch-aware clustering and on-the-fly generation of low-rank updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation (LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable masks), our method dynamically generates adaptive modifications to the decoder weights based on the aggregated statistics of clustered batches. By intelligently grouping similar inputs and computing context-aware low-rank updates via a hyper-network, ChameleonLLM achieves significant performance gains, outperforming conventional LoRA methods while eliminating the overhead of maintaining multiple expert models. Our experiments highlight the potential of our approach to serve as a versatile and highly adaptive solution for language model inference. ChameleonLLM is open-sourced to ensure the reproducibility of our experiments: https://anonymous.4open.science/r/ChamaleonLLM/", "guid": "oai:arXiv.org:2502.04315v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Kamer Ali Yuksel, Hassan Sawaf"}, {"title": "Salamandra Technical Report", "link": "https://arxiv.org/abs/2502.08489", "description": "This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and safety.With this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models.", "guid": "oai:arXiv.org:2502.08489v1", "categories": ["cs.CL"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Aitor Gonzalez-Agirre, Marc P\\`amies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo, Jos\\'e Javier Saiz, Ferran Espu\\~na, Jaume Prats, Javier Aula-Blasco, Mario Mina, Adri\\'an Rubio, Alexander Shvets, Anna Sall\\'es, I\\~naki Lacunza, I\\~nigo Pikabea, Jorge Palomar, J\\'ulia Falc\\~ao, Luc\\'ia Tormo, Luis Vasquez-Reina, Montserrat Marimon, Valle Ru\\'iz-Fern\\'andez, Marta Villegas"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/abs/2412.15151", "description": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.", "guid": "oai:arXiv.org:2412.15151v2", "categories": ["cs.CL", "cs.AI"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang"}, {"title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "link": "https://arxiv.org/abs/2502.05206", "description": "The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.", "guid": "oai:arXiv.org:2502.05206v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang"}, {"title": "Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency", "link": "https://arxiv.org/abs/2502.08206", "description": "Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date. Second, we observe that naively optimizing this metric leads us to slow down the system drastically by overemphazing staleness at the detriment of throughput. This motivates us to introduce an alternative metric that also takes system speed into account, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show that these optimizations enhance accuracy by 10% to 30%.", "guid": "oai:arXiv.org:2502.08206v1", "categories": ["cs.LG", "cs.PF", "math.OC", "math.PR"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Abdelkrim Alahyane (LAAS-SARA, LAAS-RISC, LAAS), C\\'eline Comte (CNRS, LAAS-SARA, LAAS-RISC, LAAS), Matthieu Jonckheere (CNRS, LAAS-SARA, LAAS-RISC, LAAS), \\'Eric Moulines (X)"}, {"title": "Graph Foundation Models for Recommendation: A Comprehensive Survey", "link": "https://arxiv.org/abs/2502.08346", "description": "Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.", "guid": "oai:arXiv.org:2502.08346v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi"}, {"title": "CODE-ACCORD: A Corpus of building regulatory data for rule generation towards automatic compliance checking", "link": "https://arxiv.org/abs/2403.02231", "description": "Automatic Compliance Checking (ACC) within the Architecture, Engineering, and Construction (AEC) sector necessitates automating the interpretation of building regulations to achieve its full potential. Converting textual rules into machine-readable formats is challenging due to the complexities of natural language and the scarcity of resources for advanced Machine Learning (ML). Addressing these challenges, we introduce CODE-ACCORD, a dataset of 862 sentences from the building regulations of England and Finland. Only the self-contained sentences, which express complete rules without needing additional context, were considered as they are essential for ACC. Each sentence was manually annotated with entities and relations by a team of 12 annotators to facilitate machine-readable rule generation, followed by careful curation to ensure accuracy. The final dataset comprises 4,297 entities and 4,329 relations across various categories, serving as a robust ground truth. CODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks, including text classification, entity recognition, and relation extraction. It enables applying recent trends, such as deep neural networks and large language models, to ACC.", "guid": "oai:arXiv.org:2403.02231v3", "categories": ["cs.IR"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Hansi Hettiarachchi, Amna Dridi, Mohamed Medhat Gaber, Pouyan Parsafard, Nicoleta Bocaneala, Katja Breitenfelder, Gon\\c{c}al Costa, Maria Hedblom, Mihaela Juganaru-Mathieu, Thamer Mecharnia, Sumee Park, He Tan, Abdel-Rahman H. Tawil, Edlira Vakaj"}, {"title": "Federated Learning in Chemical Engineering: A Tutorial on a Framework for Privacy-Preserving Collaboration Across Distributed Data Sources", "link": "https://arxiv.org/abs/2411.16737", "description": "Federated Learning (FL) is a decentralized machine learning approach that has gained attention for its potential to enable collaborative model training across clients while protecting data privacy, making it an attractive solution for the chemical industry. This work aims to provide the chemical engineering community with an accessible introduction to the discipline. Supported by a hands-on tutorial and a comprehensive collection of examples, it explores the application of FL in tasks such as manufacturing optimization, multimodal data integration, and drug discovery while addressing the unique challenges of protecting proprietary information and managing distributed datasets. The tutorial was built using key frameworks such as $\\texttt{Flower}$ and $\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers with the right tools to adopt FL in their specific needs. We compare the performance of FL against centralized learning across three different datasets relevant to chemical engineering applications, demonstrating that FL will often maintain or improve classification performance, particularly for complex and heterogeneous data. We conclude with an outlook on the open challenges in federated learning to be tackled and current approaches designed to remediate and improve this framework.", "guid": "oai:arXiv.org:2411.16737v2", "categories": ["cs.LG", "cs.DC", "cs.NE"], "pubdate": "Thu, 13 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Siddhant Dutta, Iago Leal de Freitas, Pedro Maciel Xavier, Claudio Miceli de Farias, David Esteban Bernal Neira"}]