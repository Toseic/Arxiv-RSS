[{"title": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display Advertisement Recommender System", "link": "https://arxiv.org/abs/2411.13789", "description": "Display advertising provides significant value to advertisers, publishers, and users. Traditional display advertising systems utilize a multi-stage architecture consisting of retrieval, coarse ranking, and final ranking. However, conventional retrieval methods rely on ID-based learning to rank mechanisms and fail to adequately utilize the content information of ads, which hampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world knowledge of LLMs. However, three key challenges arise when attempting to maximize the effectiveness of LLMs: \"How to capture user interests\", \"How to bridge the knowledge gap between LLMs and advertising system\", and \"How to efficiently deploy LLMs\". To overcome these challenges, we introduce a novel LLM-based framework called LLM Empowered Display ADvertisement REcommender system (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware Prompt Engineering introduces multi-faceted knowledge and designs intent-aware <Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users' personal interests. (2) The Advertising-Specific Knowledge Alignment incorporates auxiliary fine-tuning tasks and Direct Preference Optimization (DPO) to align LLMs with ad semantic and business value. (3) The Efficient System Deployment deploys LEADRE in an online environment by integrating both latency-tolerant and latency-sensitive service. Extensive offline experiments demonstrate the effectiveness of LEADRE and validate the contributions of individual modules. Online A/B test shows that LEADRE leads to a 1.57% and 1.17% GMV lift for serviced users on WeChat Channels and Moments separately. LEADRE has been deployed on both platforms, serving tens of billions of requests each day.", "guid": "oai:arXiv.org:2411.13789v3", "categories": ["cs.IR"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Fengxin Li, Yi Li, Yue Liu, Chao Zhou, Yuan Wang, Xiaoxiang Deng, Wei Xue, Dapeng Liu, Lei Xiao, Haijie Gu, Jie Jiang, Hongyan Liu, Biao Qin, Jun He"}, {"title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "link": "https://arxiv.org/abs/2507.12269", "description": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.", "guid": "oai:arXiv.org:2507.12269v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Sybelle Goedicke-Fritz (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Michelle Bous (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Annika Engel (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Matthias Flotho (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland), Pascal Hirsch (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Hannah Wittig (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Dino Milanovic (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Dominik Mohr (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Mathias Kaspar (Digital Medicine, University Hospital of Augsburg, Augsburg, Germany), Sogand Nemat (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Dorothea Kerner (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Arno B\\\"ucker (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Andreas Keller (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland, Pharma Science Hub), Sascha Meyer (Clinical Centre Karlsruhe, Franz-Lust Clinic for Paediatrics, Karlsruhe, Germany), Michael Zemlin (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Philipp Flotho (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland)"}, {"title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "link": "https://arxiv.org/abs/2507.11649", "description": "Federated Learning (FL) enables collaborative model training on decentralized data without exposing raw data. However, the evaluation phase in FL may leak sensitive information through shared performance metrics. In this paper, we propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to enable privacy-preserving and verifiable evaluation for FL. Instead of revealing raw loss values, clients generate a succinct proof asserting that their local loss is below a predefined threshold. Our approach is implemented without reliance on external APIs, using self-contained modules for federated learning simulation, ZKP circuit design, and experimental evaluation on both the MNIST and Human Activity Recognition (HAR) datasets. We focus on a threshold-based proof for a simple Convolutional Neural Network (CNN) model (for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate the approach in terms of computational overhead, communication cost, and verifiability.", "guid": "oai:arXiv.org:2507.11649v1", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel Commey, Benjamin Appiah, Griffith S. Klogo, Garth V. Crosby"}, {"title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "link": "https://arxiv.org/abs/2507.12127", "description": "Advancements in wireless and mobile technologies, including 5G advanced and the envisioned 6G, are driving exponential growth in wireless devices. However, this rapid expansion exacerbates spectrum scarcity, posing a critical challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and dynamically sharing spectrum--has emerged as an essential solution to address this issue. While machine learning (ML) models hold significant potential for improving spectrum sensing, their adoption in centralized ML-based DSA systems is limited by privacy concerns, bandwidth constraints, and regulatory challenges. To overcome these limitations, distributed ML-based approaches such as Federated Learning (FL) offer promising alternatives. This work addresses two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of labeled data for training FL models in practical spectrum sensing scenarios is tackled with a semi-supervised FL approach, combined with energy detection, enabling model training on unlabeled datasets. Second, we examine the security vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our analysis highlights the shortcomings of existing majority-based defenses in countering such attacks. To address these vulnerabilities, we propose a novel defense mechanism inspired by vaccination, which effectively mitigates data poisoning attacks without relying on majority-based assumptions. Extensive experiments on both synthetic and real-world datasets validate our solutions, demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets and maintain Byzantine robustness against both targeted and untargeted data poisoning attacks, even when a significant proportion of participants are malicious.", "guid": "oai:arXiv.org:2507.12127v1", "categories": ["cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Ngoc Duy Pham, Thusitha Dayaratne, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, Carsten Rudolph"}, {"title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "link": "https://arxiv.org/abs/2507.12439", "description": "Federated learning (FL) enables collaborative model training across decentralized clients while preserving data privacy. However, its open-participation nature exposes it to data-poisoning attacks, in which malicious actors submit corrupted model updates to degrade the global model. Existing defenses are often reactive, relying on statistical aggregation rules that can be computationally expensive and that typically assume an honest majority. This paper introduces a proactive, economic defense: a lightweight Bayesian incentive mechanism that makes malicious behavior economically irrational. Each training round is modeled as a Bayesian game of incomplete information in which the server, acting as the principal, uses a small, private validation dataset to verify update quality before issuing payments. The design satisfies Individual Rationality (IR) for benevolent clients, ensuring their participation is profitable, and Incentive Compatibility (IC), making poisoning an economically dominated strategy. Extensive experiments on non-IID partitions of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3 percentage points lower than in a scenario with 30% label-flipping adversaries. This outcome is 51.7 percentage points better than standard FedAvg, which collapses under the same 50% attack. The mechanism is computationally light, budget-bounded, and readily integrates into existing FL frameworks, offering a practical route to economically robust and sustainable FL ecosystems.", "guid": "oai:arXiv.org:2507.12439v1", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel Commey, Rebecca A. Sarpong, Griffith S. Klogo, Winful Bagyl-Bac, Garth V. Crosby"}, {"title": "The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models", "link": "https://arxiv.org/abs/2507.11544", "description": "Open-weight large language models (LLMs) unlock huge benefits in innovation, personalization, privacy, and democratization. However, their core advantage - modifiability - opens the door to systemic risks: bad actors can trivially subvert current safeguards, turning beneficial models into tools for harm. This leads to a 'safety gap': the difference in dangerous capabilities between a model with intact safeguards and one that has been stripped of those safeguards. We open-source a toolkit to estimate the safety gap for state-of-the-art open-weight models. As a case study, we evaluate biochemical and cyber capabilities, refusal rates, and generation quality of models from two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to 405B) using different safeguard removal techniques. Our experiments reveal that the safety gap widens as model scale increases and effective dangerous capabilities grow substantially when safeguards are removed. We hope that the Safety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve as an evaluation framework for common open-source models and as a motivation for developing and testing tamper-resistant safeguards. We welcome contributions to the toolkit from the community.", "guid": "oai:arXiv.org:2507.11544v1", "categories": ["cs.CY", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Ann-Kathrin Dombrowski, Dillon Bowen, Adam Gleave, Chris Cundy"}, {"title": "A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy", "link": "https://arxiv.org/abs/2507.12098", "description": "To mitigate privacy leakage and performance issues in personalized advertising, this paper proposes a framework that integrates federated learning and differential privacy. The system combines distributed feature extraction, dynamic privacy budget allocation, and robust model aggregation to balance model accuracy, communication overhead, and privacy protection. Multi-party secure computing and anomaly detection mechanisms further enhance system resilience against malicious attacks. Experimental results demonstrate that the framework achieves dual optimization of recommendation accuracy and system efficiency while ensuring privacy, providing both a practical solution and a theoretical foundation for applying privacy protection technologies in advertisement recommendation.", "guid": "oai:arXiv.org:2507.12098v1", "categories": ["cs.CR", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Xiang Li, Yifan Lin, Yuanzhe Zhang"}, {"title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "link": "https://arxiv.org/abs/2507.12269", "description": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.", "guid": "oai:arXiv.org:2507.12269v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Sybelle Goedicke-Fritz (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Michelle Bous (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Annika Engel (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Matthias Flotho (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland), Pascal Hirsch (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Hannah Wittig (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Dino Milanovic (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Dominik Mohr (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Mathias Kaspar (Digital Medicine, University Hospital of Augsburg, Augsburg, Germany), Sogand Nemat (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Dorothea Kerner (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Arno B\\\"ucker (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Andreas Keller (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland, Pharma Science Hub), Sascha Meyer (Clinical Centre Karlsruhe, Franz-Lust Clinic for Paediatrics, Karlsruhe, Germany), Michael Zemlin (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Philipp Flotho (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland)"}, {"title": "Holistic analysis on the sustainability of Federated Learning across AI product lifecycle", "link": "https://arxiv.org/abs/2312.14628", "description": "In light of emerging legal requirements and policies focused on privacy protection, there is a growing trend of companies across various industries adopting Federated Learning (FL). This decentralized approach involves multiple clients or silos, collaboratively training a global model under the coordination of a central server while utilizing their private local data. Unlike traditional methods that necessitate data sharing and transmission, Cross-Silo FL allows clients to share model updates rather than raw data, thereby enhancing privacy. Despite its growing adoption, the carbon impact associated with Cross-Silo FL remains poorly understood due to the limited research in this area. This study seeks to bridge this gap by evaluating the sustainability of Cross-Silo FL throughout the entire AI product lifecycle, extending the analysis beyond the model training phase alone. We systematically compare this decentralized method with traditional centralized approaches and present a robust quantitative framework for assessing the costs and CO2 emissions in real-world Cross-Silo FL environments. Our findings indicate that the energy consumption and costs of model training are comparable between Cross-Silo Federated Learning and Centralized Learning. However, the additional data transfer and storage requirements inherent in Centralized Learning can result in significant, often overlooked CO2 emissions. Moreover, we introduce an innovative data and application management system that integrates Cross-Silo FL and analytics, aiming at improving the sustainability and economic efficiency of IT enterprises.", "guid": "oai:arXiv.org:2312.14628v3", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Hongliu Cao"}, {"title": "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy", "link": "https://arxiv.org/abs/2501.13916", "description": "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL), a communication-efficient Vertical Federated Learning algorithm with Differential Privacy guarantees. PBM-VFL combines Secure Multi-Party Computation with the recently introduced Poisson Binomial Mechanism to protect parties' private datasets during model training. We define the novel concept of feature privacy and analyze end-to-end feature and sample privacy of our algorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We also provide the first theoretical characterization of the relationship between privacy budget, convergence error, and communication cost in differentially-private VFL. Finally, we empirically show that our model performs well with high levels of privacy.", "guid": "oai:arXiv.org:2501.13916v3", "categories": ["cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Linh Tran, Timothy Castiglia, Stacy Patterson, Ana Milanova"}, {"title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning", "link": "https://arxiv.org/abs/2503.17070", "description": "Federated learning (FL) allows collaborative machine learning (ML) model training among decentralized clients' information, ensuring data privacy. The decentralized nature of FL deals with non-independent and identically distributed (non-IID) data. This open problem has notable consequences, such as decreased model performance and more significant convergence times. Despite its importance, experimental studies systematically addressing all types of data heterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by assessing and quantifying the non-IID effect through a thorough empirical analysis. We use the Hellinger Distance (HD) to measure differences in distribution among clients. Our study benchmarks four state-of-the-art strategies for handling non-IID data, including label, feature, quantity, and spatiotemporal skewness, under realistic and controlled conditions. This is the first comprehensive analysis of the spatiotemporal skew effect in FL. Our findings highlight the significant impact of label and spatiotemporal skew non-IID types on FL model performance, with notable performance drops occurring at specific HD thresholds. Additionally, the FL performance is heavily affected mainly when the non-IIDness is extreme. Thus, we provide recommendations for FL research to tackle data heterogeneity effectively. Our work represents the most extensive examination of non-IIDness in FL, offering a robust foundation for future research.", "guid": "oai:arXiv.org:2503.17070v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti"}, {"title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "link": "https://arxiv.org/abs/2506.23210", "description": "Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and utilizes a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and clients' low computing cost.", "guid": "oai:arXiv.org:2506.23210v2", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Taehwan Yoon, Bongjun Choi"}, {"title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "link": "https://arxiv.org/abs/2507.10158", "description": "Federated Learning (FL) is a promising machine learning paradigm that enables participating devices to train privacy-preserved and collaborative models. FL has proven its benefits for robotic manipulation tasks. However, grasping tasks lack exploration in such settings where robots train a global model without moving data and ensuring data privacy. The main challenge is that each robot learns from data that is nonindependent and identically distributed (non-IID) and of low quantity. This exhibits performance degradation, particularly in robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL approach for robotic grasping, acknowledging the unique challenges posed by the non-IID data distribution across robots, including quantitative skewness. MTF-Grasp harnesses data quality and quantity across robots to select a set of \"top-level\" robots with better data distribution and higher sample count. It then utilizes top-level robots to train initial seed models and distribute them to the remaining \"low-level\" robots, reducing the risk of model performance degradation in low-level robots. Our approach outperforms the conventional FL setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping datasets.", "guid": "oai:arXiv.org:2507.10158v2", "categories": ["cs.LG", "cs.RO"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Obaidullah Zaland, Erik Elmroth, Monowar Bhuyan"}, {"title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning", "link": "https://arxiv.org/abs/2507.03220", "description": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the task specific parameters into adapters, which are a fraction of the size of the original base model. Popularity of PEFT technique for fine-tuning has led to creation of a large number of adapters for popular Large Language Models (LLMs). However, existing frameworks fall short in supporting inference or fine-tuning with multiple adapters in the following ways. 1) For fine-tuning, each job needs to deploy its dedicated base model instance, which results in excessive GPU memory consumption and poor GPU utilization. 2) While popular inference platforms can serve multiple PEFT adapters, they do not allow independent resource management or mixing of different PEFT methods. 3) They cannot share resources (such as base model instance) between inference and fine-tuning jobs. 4) They do not provide privacy to users who may not wish to expose their fine-tuned parameters to service providers. In Symbiosis, we address the above problems by enabling as-a-service deployment of base model. The base model layers can be shared across multiple inference or fine-tuning processes. Our split-execution technique decouples the execution of client-specific adapters and layers from the frozen base model layers offering them flexibility to manage their resources, to select their fine-tuning method, to achieve their performance goals. Our approach is transparent to models and works out-of-the-box for most models in the transformers library. Our evaluation on Llama2-13B shows the compared to baseline, Symbiosis can fine-tune 4X more adapters on the same set of GPUs in the same amount of time.", "guid": "oai:arXiv.org:2507.03220v2", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman"}, {"title": "Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM Serving", "link": "https://arxiv.org/abs/2507.06608", "description": "Monolithic serving with chunked prefill improves GPU utilization by batching prefill and decode together, but suffers from fine-grained phase interference. Engine-level prefill-decode (PD) disaggregation avoids interference but incurs higher hardware and coordination overhead. Prior intra-GPU disaggregation approaches multiplex prefill and decode within a single GPU, using SLO-based tuning guided by heuristics from offline profiling or reactive feedback loops. However, these methods respond reactively to performance issues rather than anticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts effectively to dynamic workloads? The key challenge lies in managing the conflicting resource demands of prefill and decode under varying conditions. We first show that GPU resources exhibit diminishing returns -- beyond a saturation point, more allocation yields minimal latency benefit. Second, we observe that memory bandwidth contention becomes a critical bottleneck. These insights motivate a design that dynamically partitions GPU resources across prefill and decode phases, while jointly considering compute capacity, memory footprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms SGLang by up to 2x; and matches or exceeds disaggregated vLLM.", "guid": "oai:arXiv.org:2507.06608v4", "categories": ["cs.DC", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xiaoxiang Shi, Colin Cai, Junjia Du"}, {"title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "link": "https://arxiv.org/abs/2507.11649", "description": "Federated Learning (FL) enables collaborative model training on decentralized data without exposing raw data. However, the evaluation phase in FL may leak sensitive information through shared performance metrics. In this paper, we propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to enable privacy-preserving and verifiable evaluation for FL. Instead of revealing raw loss values, clients generate a succinct proof asserting that their local loss is below a predefined threshold. Our approach is implemented without reliance on external APIs, using self-contained modules for federated learning simulation, ZKP circuit design, and experimental evaluation on both the MNIST and Human Activity Recognition (HAR) datasets. We focus on a threshold-based proof for a simple Convolutional Neural Network (CNN) model (for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate the approach in terms of computational overhead, communication cost, and verifiability.", "guid": "oai:arXiv.org:2507.11649v1", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel Commey, Benjamin Appiah, Griffith S. Klogo, Garth V. Crosby"}, {"title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning", "link": "https://arxiv.org/abs/2507.03220", "description": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the task specific parameters into adapters, which are a fraction of the size of the original base model. Popularity of PEFT technique for fine-tuning has led to creation of a large number of adapters for popular Large Language Models (LLMs). However, existing frameworks fall short in supporting inference or fine-tuning with multiple adapters in the following ways. 1) For fine-tuning, each job needs to deploy its dedicated base model instance, which results in excessive GPU memory consumption and poor GPU utilization. 2) While popular inference platforms can serve multiple PEFT adapters, they do not allow independent resource management or mixing of different PEFT methods. 3) They cannot share resources (such as base model instance) between inference and fine-tuning jobs. 4) They do not provide privacy to users who may not wish to expose their fine-tuned parameters to service providers. In Symbiosis, we address the above problems by enabling as-a-service deployment of base model. The base model layers can be shared across multiple inference or fine-tuning processes. Our split-execution technique decouples the execution of client-specific adapters and layers from the frozen base model layers offering them flexibility to manage their resources, to select their fine-tuning method, to achieve their performance goals. Our approach is transparent to models and works out-of-the-box for most models in the transformers library. Our evaluation on Llama2-13B shows the compared to baseline, Symbiosis can fine-tune 4X more adapters on the same set of GPUs in the same amount of time.", "guid": "oai:arXiv.org:2507.03220v2", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman"}, {"title": "Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM Serving", "link": "https://arxiv.org/abs/2507.06608", "description": "Monolithic serving with chunked prefill improves GPU utilization by batching prefill and decode together, but suffers from fine-grained phase interference. Engine-level prefill-decode (PD) disaggregation avoids interference but incurs higher hardware and coordination overhead. Prior intra-GPU disaggregation approaches multiplex prefill and decode within a single GPU, using SLO-based tuning guided by heuristics from offline profiling or reactive feedback loops. However, these methods respond reactively to performance issues rather than anticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts effectively to dynamic workloads? The key challenge lies in managing the conflicting resource demands of prefill and decode under varying conditions. We first show that GPU resources exhibit diminishing returns -- beyond a saturation point, more allocation yields minimal latency benefit. Second, we observe that memory bandwidth contention becomes a critical bottleneck. These insights motivate a design that dynamically partitions GPU resources across prefill and decode phases, while jointly considering compute capacity, memory footprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms SGLang by up to 2x; and matches or exceeds disaggregated vLLM.", "guid": "oai:arXiv.org:2507.06608v4", "categories": ["cs.DC", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xiaoxiang Shi, Colin Cai, Junjia Du"}, {"title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "link": "https://arxiv.org/abs/2506.23210", "description": "Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and utilizes a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and clients' low computing cost.", "guid": "oai:arXiv.org:2506.23210v2", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Taehwan Yoon, Bongjun Choi"}, {"title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "link": "https://arxiv.org/abs/2507.10158", "description": "Federated Learning (FL) is a promising machine learning paradigm that enables participating devices to train privacy-preserved and collaborative models. FL has proven its benefits for robotic manipulation tasks. However, grasping tasks lack exploration in such settings where robots train a global model without moving data and ensuring data privacy. The main challenge is that each robot learns from data that is nonindependent and identically distributed (non-IID) and of low quantity. This exhibits performance degradation, particularly in robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL approach for robotic grasping, acknowledging the unique challenges posed by the non-IID data distribution across robots, including quantitative skewness. MTF-Grasp harnesses data quality and quantity across robots to select a set of \"top-level\" robots with better data distribution and higher sample count. It then utilizes top-level robots to train initial seed models and distribute them to the remaining \"low-level\" robots, reducing the risk of model performance degradation in low-level robots. Our approach outperforms the conventional FL setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping datasets.", "guid": "oai:arXiv.org:2507.10158v2", "categories": ["cs.LG", "cs.RO"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Obaidullah Zaland, Erik Elmroth, Monowar Bhuyan"}, {"title": "ChipAlign: Instruction Alignment in Large Language Models for Chip Design via Geodesic Interpolation", "link": "https://arxiv.org/abs/2412.19819", "description": "Recent advancements in large language models (LLMs) have expanded their application across various domains, including chip design, where domain-adapted chip models like ChipNeMo have emerged. However, these models often struggle with instruction alignment, a crucial capability for LLMs that involves following explicit human directives. This limitation impedes the practical application of chip LLMs, including serving as assistant chatbots for hardware design engineers. In this work, we introduce ChipAlign, a novel approach that utilizes a training-free model merging strategy, combining the strengths of a general instruction-aligned LLM with a chip-specific LLM. By considering the underlying manifold in the weight space, ChipAlign employs geodesic interpolation to effectively fuse the weights of input LLMs, producing a merged model that inherits strong instruction alignment and chip expertise from the respective instruction and chip LLMs. Our results demonstrate that ChipAlign significantly enhances instruction-following capabilities of existing chip LLMs, achieving up to a 26.6% improvement on the IFEval benchmark, while maintaining comparable expertise in the chip domain. This improvement in instruction alignment also translates to notable gains in instruction-involved QA tasks, delivering performance enhancements of 3.9% on the OpenROAD QA benchmark and 8.25% on production-level chip QA benchmarks, surpassing state-of-the-art baselines.", "guid": "oai:arXiv.org:2412.19819v2", "categories": ["cs.AR", "cs.AI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Chenhui Deng, Yunsheng Bai, Haoxing Ren"}, {"title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "link": "https://arxiv.org/abs/2507.12269", "description": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.", "guid": "oai:arXiv.org:2507.12269v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Sybelle Goedicke-Fritz (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Michelle Bous (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Annika Engel (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Matthias Flotho (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland), Pascal Hirsch (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Hannah Wittig (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Dino Milanovic (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany), Dominik Mohr (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Mathias Kaspar (Digital Medicine, University Hospital of Augsburg, Augsburg, Germany), Sogand Nemat (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Dorothea Kerner (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Arno B\\\"ucker (Department of Radiology, and Interventional Radiology, University Hospital of Saarland, Homburg, Germany), Andreas Keller (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland, Pharma Science Hub), Sascha Meyer (Clinical Centre Karlsruhe, Franz-Lust Clinic for Paediatrics, Karlsruhe, Germany), Michael Zemlin (Department of General Pediatrics and Neonatology, Saarland University, Campus Homburg, Homburg/Saar, Germany), Philipp Flotho (Chair for Clinical Bioinformatics, Saarland Informatics Campus, Saarland University, Saarbr\\\"ucken, Germany, Helmholtz Institute for Pharmaceutical Research Saarland)"}, {"title": "Holistic analysis on the sustainability of Federated Learning across AI product lifecycle", "link": "https://arxiv.org/abs/2312.14628", "description": "In light of emerging legal requirements and policies focused on privacy protection, there is a growing trend of companies across various industries adopting Federated Learning (FL). This decentralized approach involves multiple clients or silos, collaboratively training a global model under the coordination of a central server while utilizing their private local data. Unlike traditional methods that necessitate data sharing and transmission, Cross-Silo FL allows clients to share model updates rather than raw data, thereby enhancing privacy. Despite its growing adoption, the carbon impact associated with Cross-Silo FL remains poorly understood due to the limited research in this area. This study seeks to bridge this gap by evaluating the sustainability of Cross-Silo FL throughout the entire AI product lifecycle, extending the analysis beyond the model training phase alone. We systematically compare this decentralized method with traditional centralized approaches and present a robust quantitative framework for assessing the costs and CO2 emissions in real-world Cross-Silo FL environments. Our findings indicate that the energy consumption and costs of model training are comparable between Cross-Silo Federated Learning and Centralized Learning. However, the additional data transfer and storage requirements inherent in Centralized Learning can result in significant, often overlooked CO2 emissions. Moreover, we introduce an innovative data and application management system that integrates Cross-Silo FL and analytics, aiming at improving the sustainability and economic efficiency of IT enterprises.", "guid": "oai:arXiv.org:2312.14628v3", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Hongliu Cao"}, {"title": "ChipAlign: Instruction Alignment in Large Language Models for Chip Design via Geodesic Interpolation", "link": "https://arxiv.org/abs/2412.19819", "description": "Recent advancements in large language models (LLMs) have expanded their application across various domains, including chip design, where domain-adapted chip models like ChipNeMo have emerged. However, these models often struggle with instruction alignment, a crucial capability for LLMs that involves following explicit human directives. This limitation impedes the practical application of chip LLMs, including serving as assistant chatbots for hardware design engineers. In this work, we introduce ChipAlign, a novel approach that utilizes a training-free model merging strategy, combining the strengths of a general instruction-aligned LLM with a chip-specific LLM. By considering the underlying manifold in the weight space, ChipAlign employs geodesic interpolation to effectively fuse the weights of input LLMs, producing a merged model that inherits strong instruction alignment and chip expertise from the respective instruction and chip LLMs. Our results demonstrate that ChipAlign significantly enhances instruction-following capabilities of existing chip LLMs, achieving up to a 26.6% improvement on the IFEval benchmark, while maintaining comparable expertise in the chip domain. This improvement in instruction alignment also translates to notable gains in instruction-involved QA tasks, delivering performance enhancements of 3.9% on the OpenROAD QA benchmark and 8.25% on production-level chip QA benchmarks, surpassing state-of-the-art baselines.", "guid": "oai:arXiv.org:2412.19819v2", "categories": ["cs.AR", "cs.AI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Chenhui Deng, Yunsheng Bai, Haoxing Ren"}, {"title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning", "link": "https://arxiv.org/abs/2503.17070", "description": "Federated learning (FL) allows collaborative machine learning (ML) model training among decentralized clients' information, ensuring data privacy. The decentralized nature of FL deals with non-independent and identically distributed (non-IID) data. This open problem has notable consequences, such as decreased model performance and more significant convergence times. Despite its importance, experimental studies systematically addressing all types of data heterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by assessing and quantifying the non-IID effect through a thorough empirical analysis. We use the Hellinger Distance (HD) to measure differences in distribution among clients. Our study benchmarks four state-of-the-art strategies for handling non-IID data, including label, feature, quantity, and spatiotemporal skewness, under realistic and controlled conditions. This is the first comprehensive analysis of the spatiotemporal skew effect in FL. Our findings highlight the significant impact of label and spatiotemporal skew non-IID types on FL model performance, with notable performance drops occurring at specific HD thresholds. Additionally, the FL performance is heavily affected mainly when the non-IIDness is extreme. Thus, we provide recommendations for FL research to tackle data heterogeneity effectively. Our work represents the most extensive examination of non-IIDness in FL, offering a robust foundation for future research.", "guid": "oai:arXiv.org:2503.17070v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti"}, {"title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "link": "https://arxiv.org/abs/2506.23210", "description": "Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and utilizes a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and clients' low computing cost.", "guid": "oai:arXiv.org:2506.23210v2", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Taehwan Yoon, Bongjun Choi"}, {"title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning", "link": "https://arxiv.org/abs/2507.03220", "description": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the task specific parameters into adapters, which are a fraction of the size of the original base model. Popularity of PEFT technique for fine-tuning has led to creation of a large number of adapters for popular Large Language Models (LLMs). However, existing frameworks fall short in supporting inference or fine-tuning with multiple adapters in the following ways. 1) For fine-tuning, each job needs to deploy its dedicated base model instance, which results in excessive GPU memory consumption and poor GPU utilization. 2) While popular inference platforms can serve multiple PEFT adapters, they do not allow independent resource management or mixing of different PEFT methods. 3) They cannot share resources (such as base model instance) between inference and fine-tuning jobs. 4) They do not provide privacy to users who may not wish to expose their fine-tuned parameters to service providers. In Symbiosis, we address the above problems by enabling as-a-service deployment of base model. The base model layers can be shared across multiple inference or fine-tuning processes. Our split-execution technique decouples the execution of client-specific adapters and layers from the frozen base model layers offering them flexibility to manage their resources, to select their fine-tuning method, to achieve their performance goals. Our approach is transparent to models and works out-of-the-box for most models in the transformers library. Our evaluation on Llama2-13B shows the compared to baseline, Symbiosis can fine-tune 4X more adapters on the same set of GPUs in the same amount of time.", "guid": "oai:arXiv.org:2507.03220v2", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman"}, {"title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "link": "https://arxiv.org/abs/2507.12439", "description": "Federated learning (FL) enables collaborative model training across decentralized clients while preserving data privacy. However, its open-participation nature exposes it to data-poisoning attacks, in which malicious actors submit corrupted model updates to degrade the global model. Existing defenses are often reactive, relying on statistical aggregation rules that can be computationally expensive and that typically assume an honest majority. This paper introduces a proactive, economic defense: a lightweight Bayesian incentive mechanism that makes malicious behavior economically irrational. Each training round is modeled as a Bayesian game of incomplete information in which the server, acting as the principal, uses a small, private validation dataset to verify update quality before issuing payments. The design satisfies Individual Rationality (IR) for benevolent clients, ensuring their participation is profitable, and Incentive Compatibility (IC), making poisoning an economically dominated strategy. Extensive experiments on non-IID partitions of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3 percentage points lower than in a scenario with 30% label-flipping adversaries. This outcome is 51.7 percentage points better than standard FedAvg, which collapses under the same 50% attack. The mechanism is computationally light, budget-bounded, and readily integrates into existing FL frameworks, offering a practical route to economically robust and sustainable FL ecosystems.", "guid": "oai:arXiv.org:2507.12439v1", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel Commey, Rebecca A. Sarpong, Griffith S. Klogo, Winful Bagyl-Bac, Garth V. Crosby"}, {"title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "link": "https://arxiv.org/abs/2507.11649", "description": "Federated Learning (FL) enables collaborative model training on decentralized data without exposing raw data. However, the evaluation phase in FL may leak sensitive information through shared performance metrics. In this paper, we propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to enable privacy-preserving and verifiable evaluation for FL. Instead of revealing raw loss values, clients generate a succinct proof asserting that their local loss is below a predefined threshold. Our approach is implemented without reliance on external APIs, using self-contained modules for federated learning simulation, ZKP circuit design, and experimental evaluation on both the MNIST and Human Activity Recognition (HAR) datasets. We focus on a threshold-based proof for a simple Convolutional Neural Network (CNN) model (for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate the approach in terms of computational overhead, communication cost, and verifiability.", "guid": "oai:arXiv.org:2507.11649v1", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Daniel Commey, Benjamin Appiah, Griffith S. Klogo, Garth V. Crosby"}, {"title": "The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models", "link": "https://arxiv.org/abs/2507.11544", "description": "Open-weight large language models (LLMs) unlock huge benefits in innovation, personalization, privacy, and democratization. However, their core advantage - modifiability - opens the door to systemic risks: bad actors can trivially subvert current safeguards, turning beneficial models into tools for harm. This leads to a 'safety gap': the difference in dangerous capabilities between a model with intact safeguards and one that has been stripped of those safeguards. We open-source a toolkit to estimate the safety gap for state-of-the-art open-weight models. As a case study, we evaluate biochemical and cyber capabilities, refusal rates, and generation quality of models from two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to 405B) using different safeguard removal techniques. Our experiments reveal that the safety gap widens as model scale increases and effective dangerous capabilities grow substantially when safeguards are removed. We hope that the Safety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve as an evaluation framework for common open-source models and as a motivation for developing and testing tamper-resistant safeguards. We welcome contributions to the toolkit from the community.", "guid": "oai:arXiv.org:2507.11544v1", "categories": ["cs.CY", "cs.LG"], "pubdate": "Thu, 17 Jul 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Ann-Kathrin Dombrowski, Dillon Bowen, Adam Gleave, Chris Cundy"}]