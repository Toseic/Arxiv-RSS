[{"title": "An Adaptive Data-Enabled Policy Optimization Approach for Autonomous Bicycle Control", "link": "https://arxiv.org/abs/2502.13676", "description": "This paper presents a unified control framework that integrates a Feedback Linearization (FL) controller in the inner loop with an adaptive Data-Enabled Policy Optimization (DeePO) controller in the outer loop to balance an autonomous bicycle. While the FL controller stabilizes and partially linearizes the inherently unstable and nonlinear system, its performance is compromised by unmodeled dynamics and time-varying characteristics. To overcome these limitations, the DeePO controller is introduced to enhance adaptability and robustness. The initial control policy of DeePO is obtained from a finite set of offline, persistently exciting input and state data. To improve stability and compensate for system nonlinearities and disturbances, a robustness-promoting regularizer refines the initial policy, while the adaptive section of the DeePO framework is enhanced with a forgetting factor to improve adaptation to time-varying dynamics. The proposed DeePO+FL approach is evaluated through simulations and real-world experiments on an instrumented autonomous bicycle. Results demonstrate its superiority over the FL-only approach, achieving more precise tracking of the reference lean angle and lean rate.", "guid": "oai:arXiv.org:2502.13676v1", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Niklas Persson, Feiran Zhao, Mojtaba Kaheni, Florian D\\\"orfler, Alessandro V. Papadopoulos"}, {"title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy", "link": "https://arxiv.org/abs/2502.05177", "description": "We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of 17M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. By leveraging our inference designs, Long-VITA models achieve a remarkable 2x prefill speedup and 4x context length extension in single node with 8 GPUs. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding.", "guid": "oai:arXiv.org:2502.05177v2", "categories": ["cs.CV"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Ran He, Caifeng Shan, Rongrong Ji, Xing Sun"}, {"title": "Where's the Bug? Attention Probing for Scalable Fault Localization", "link": "https://arxiv.org/abs/2502.13966", "description": "Ensuring code correctness remains a challenging problem even as large language models (LLMs) become increasingly capable at code-related tasks. While LLM-based program repair systems can propose bug fixes using only a user's bug report, their effectiveness is fundamentally limited by their ability to perform fault localization (FL), a challenging problem for both humans and LLMs. Existing FL approaches rely on executable test cases, require training on costly and often noisy line-level annotations, or demand resource-intensive LLMs. In this paper, we present Bug Attention Probe (BAP), a method which learns state-of-the-art fault localization without any direct localization labels, outperforming traditional FL baselines and prompting of large-scale LLMs. We evaluate our approach across a variety of code settings, including real-world Java bugs from the standard Defects4J dataset as well as seven other datasets which span a diverse set of bug types and languages. Averaged across all eight datasets, BAP improves by 34.6% top-1 accuracy compared to the strongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost.", "guid": "oai:arXiv.org:2502.13966v2", "categories": ["cs.SE", "cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Adam Stein, Arthur Wayne, Aaditya Naik, Mayur Naik, Eric Wong"}, {"title": "Smoothed Normalization for Efficient Distributed Private Optimization", "link": "https://arxiv.org/abs/2502.13482", "description": "Federated learning enables training machine learning models while preserving the privacy of participants. Surprisingly, there is no differentially private distributed method for smooth, non-convex optimization problems. The reason is that standard privacy techniques require bounding the participants' contributions, usually enforced via $\\textit{clipping}$ of the updates. Existing literature typically ignores the effect of clipping by assuming the boundedness of gradient norms or analyzes distributed algorithms with clipping but ignores DP constraints. In this work, we study an alternative approach via $\\textit{smoothed normalization}$ of the updates motivated by its favorable performance in the single-node setting. By integrating smoothed normalization with an error-feedback mechanism, we design a new distributed algorithm $\\alpha$-$\\sf NormEC$. We prove that our method achieves a superior convergence rate over prior works. By extending $\\alpha$-$\\sf NormEC$ to the DP setting, we obtain the first differentially private distributed optimization algorithm with provable convergence guarantees. Finally, our empirical results from neural network training indicate robust convergence of $\\alpha$-$\\sf NormEC$ across different parameter settings.", "guid": "oai:arXiv.org:2502.13482v1", "categories": ["cs.LG", "cs.CR", "cs.DC", "math.OC", "stat.ML"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Egor Shulgin, Sarit Khirirat, Peter Richt\\'arik"}, {"title": "Homophily Heterogeneity Matters in Graph Federated Learning: A Spectrum Sharing and Complementing Perspective", "link": "https://arxiv.org/abs/2502.13732", "description": "Since heterogeneity presents a fundamental challenge in graph federated learning, many existing methods are proposed to deal with node feature heterogeneity and structure heterogeneity. However, they overlook the critical homophily heterogeneity, which refers to the substantial variation in homophily levels across graph data from different clients. The homophily level represents the proportion of edges connecting nodes that belong to the same class. Due to adapting to their local homophily, local models capture inconsistent spectral properties across different clients, significantly reducing the effectiveness of collaboration. Specifically, local models trained on graphs with high homophily tend to capture low-frequency information, whereas local models trained on graphs with low homophily tend to capture high-frequency information. To effectively deal with homophily heterophily, we introduce the spectral Graph Neural Network (GNN) and propose a novel Federated learning method by mining Graph Spectral Properties (FedGSP). On one hand, our proposed FedGSP enables clients to share generic spectral properties (i.e., low-frequency information), allowing all clients to benefit through collaboration. On the other hand, inspired by our theoretical findings, our proposed FedGSP allows clients to complement non-generic spectral properties by acquiring the spectral properties they lack (i.e., high-frequency information), thereby obtaining additional information gain. Extensive experiments conducted on six homophilic and five heterophilic graph datasets, across both non-overlapping and overlapping settings, validate the superiority of our method over eleven state-of-the-art methods. Notably, our FedGSP outperforms the second-best method by an average margin of 3.28% on all heterophilic datasets.", "guid": "oai:arXiv.org:2502.13732v1", "categories": ["cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Wentao Yu"}, {"title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs", "link": "https://arxiv.org/abs/2502.13965", "description": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", "guid": "oai:arXiv.org:2502.13965v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica"}, {"title": "Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion", "link": "https://arxiv.org/abs/2502.13509", "description": "Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.", "guid": "oai:arXiv.org:2502.13509v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang"}, {"title": "Where's the Bug? Attention Probing for Scalable Fault Localization", "link": "https://arxiv.org/abs/2502.13966", "description": "Ensuring code correctness remains a challenging problem even as large language models (LLMs) become increasingly capable at code-related tasks. While LLM-based program repair systems can propose bug fixes using only a user's bug report, their effectiveness is fundamentally limited by their ability to perform fault localization (FL), a challenging problem for both humans and LLMs. Existing FL approaches rely on executable test cases, require training on costly and often noisy line-level annotations, or demand resource-intensive LLMs. In this paper, we present Bug Attention Probe (BAP), a method which learns state-of-the-art fault localization without any direct localization labels, outperforming traditional FL baselines and prompting of large-scale LLMs. We evaluate our approach across a variety of code settings, including real-world Java bugs from the standard Defects4J dataset as well as seven other datasets which span a diverse set of bug types and languages. Averaged across all eight datasets, BAP improves by 34.6% top-1 accuracy compared to the strongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost.", "guid": "oai:arXiv.org:2502.13966v2", "categories": ["cs.SE", "cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Adam Stein, Arthur Wayne, Aaditya Naik, Mayur Naik, Eric Wong"}, {"title": "FedSpaLLM: Federated Pruning of Large Language Models", "link": "https://arxiv.org/abs/2410.14852", "description": "Large Language Models (LLMs) achieve state-of-the-art performance but are challenging to deploy due to their high computational and storage demands. Pruning can reduce model size, yet existing methods assume public access to calibration data, which is impractical for privacy-sensitive applications. To address the challenge of pruning LLMs in privacy-preserving settings, we propose FedSpaLLM, the first federated learning framework designed specifically for pruning LLMs. FedSpaLLM enables clients to prune their models locally based on private data while accounting for system heterogeneity and maintaining communication efficiency. Our framework introduces several key innovations: (1) a novel $\\ell_0$-norm aggregation function that ensures only non-zero weights are averaged across clients, preserving important model parameters; (2) an adaptive mask expansion technique that meets global sparsity targets while accommodating client-specific pruning decisions; and (3) a layer sampling strategy that reduces communication overhead and personalizes the pruning process based on client resources. Extensive experiments show that FedSpaLLM improves pruning performance in diverse federated settings.", "guid": "oai:arXiv.org:2410.14852v2", "categories": ["cs.LG", "cs.CR"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, Kibaek Kim"}, {"title": "Towards Active Participant Centric Vertical Federated Learning: Some Representations May Be All You Need", "link": "https://arxiv.org/abs/2410.17648", "description": "Existing Vertical FL (VFL) methods often struggle with realistic and unaligned data partitions, and incur into high communication costs and significant operational complexity. This work introduces a novel approach to VFL, Active Participant Centric VFL (APC-VFL), that excels in scenarios when data samples among participants are partially aligned at training. Among its strengths, APC-VFL only requires a single communication step with the active participant. This is made possible through a local and unsupervised representation learning stage at each participant followed by a knowledge distillation step in the active participant. Compared to other VFL methods such as SplitNN or VFedTrans, APC-VFL consistently outperforms them across three popular VFL datasets in terms of F1, accuracy and communication costs as the ratio of aligned data is reduced.", "guid": "oai:arXiv.org:2410.17648v2", "categories": ["cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Jon Irureta, Jon Imaz, Aizea Lojo, Javier Fernandez-Marques, Marco Gonz\\'alez, I\\~nigo Perona"}, {"title": "PFedDST: Personalized Federated Learning with Decentralized Selection Training", "link": "https://arxiv.org/abs/2502.07750", "description": "Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems.", "guid": "oai:arXiv.org:2502.07750v2", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Mengchen Fan, Keren Li, Tianyun Zhang, Qing Tian, Baocheng Geng"}, {"title": "Smoothed Normalization for Efficient Distributed Private Optimization", "link": "https://arxiv.org/abs/2502.13482", "description": "Federated learning enables training machine learning models while preserving the privacy of participants. Surprisingly, there is no differentially private distributed method for smooth, non-convex optimization problems. The reason is that standard privacy techniques require bounding the participants' contributions, usually enforced via $\\textit{clipping}$ of the updates. Existing literature typically ignores the effect of clipping by assuming the boundedness of gradient norms or analyzes distributed algorithms with clipping but ignores DP constraints. In this work, we study an alternative approach via $\\textit{smoothed normalization}$ of the updates motivated by its favorable performance in the single-node setting. By integrating smoothed normalization with an error-feedback mechanism, we design a new distributed algorithm $\\alpha$-$\\sf NormEC$. We prove that our method achieves a superior convergence rate over prior works. By extending $\\alpha$-$\\sf NormEC$ to the DP setting, we obtain the first differentially private distributed optimization algorithm with provable convergence guarantees. Finally, our empirical results from neural network training indicate robust convergence of $\\alpha$-$\\sf NormEC$ across different parameter settings.", "guid": "oai:arXiv.org:2502.13482v1", "categories": ["cs.LG", "cs.CR", "cs.DC", "math.OC", "stat.ML"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Egor Shulgin, Sarit Khirirat, Peter Richt\\'arik"}, {"title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs", "link": "https://arxiv.org/abs/2502.13965", "description": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", "guid": "oai:arXiv.org:2502.13965v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica"}, {"title": "Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion", "link": "https://arxiv.org/abs/2502.13509", "description": "Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.", "guid": "oai:arXiv.org:2502.13509v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang"}, {"title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs", "link": "https://arxiv.org/abs/2502.13965", "description": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", "guid": "oai:arXiv.org:2502.13965v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica"}, {"title": "FedCC: Robust Federated Learning against Model Poisoning Attacks", "link": "https://arxiv.org/abs/2212.01976", "description": "Federated learning is a distributed framework designed to address privacy concerns. However, it introduces new attack surfaces, which are especially prone when data is non-Independently and Identically Distributed. Existing approaches fail to effectively mitigate the malicious influence in this setting; previous approaches often tackle non-IID data and poisoning attacks separately. To address both challenges simultaneously, we present FedCC, a simple yet effective novel defense algorithm against model poisoning attacks. It leverages the Centered Kernel Alignment similarity of Penultimate Layer Representations for clustering, allowing the identification and filtration of malicious clients, even in non-IID data settings. The penultimate layer representations are meaningful since the later layers are more sensitive to local data distributions, which allows better detection of malicious clients. The sophisticated utilization of layer-wise Centered Kernel Alignment similarity allows attack mitigation while leveraging useful knowledge obtained. Our extensive experiments demonstrate the effectiveness of FedCC in mitigating both untargeted model poisoning and targeted backdoor attacks. Compared to existing outlier detection-based and first-order statistics-based methods, FedCC consistently reduces attack confidence to zero. Specifically, it significantly minimizes the average degradation of global performance by 65.5\\%. We believe that this new perspective on aggregation makes it a valuable contribution to the field of FL model security and privacy. The code will be made available upon acceptance.", "guid": "oai:arXiv.org:2212.01976v3", "categories": ["cs.CR", "cs.AI"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Hyejun Jeong, Hamin Son, Seohu Lee, Jayun Hyun, Tai-Myoung Chung"}, {"title": "PFedDST: Personalized Federated Learning with Decentralized Selection Training", "link": "https://arxiv.org/abs/2502.07750", "description": "Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems.", "guid": "oai:arXiv.org:2502.07750v2", "categories": ["cs.LG", "cs.AI"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Mengchen Fan, Keren Li, Tianyun Zhang, Qing Tian, Baocheng Geng"}, {"title": "Supplementary File: Coded Cooperative Networks for Semi-Decentralized Federated Learning", "link": "https://arxiv.org/abs/2406.19002", "description": "To enhance straggler resilience in federated learning (FL) systems, a semi-decentralized approach has been recently proposed, enabling collaboration between clients. Unlike the existing semi-decentralized schemes, which adaptively adjust the collaboration weight according to the network topology, this letter proposes a deterministic coded network that leverages wireless diversity for semi-decentralized FL without requiring prior information about the entire network. Furthermore, the theoretical analyses of the outage and the convergence rate of the proposed scheme are provided. Finally, the superiority of our proposed method over benchmark methods is demonstrated through comprehensive simulations.", "guid": "oai:arXiv.org:2406.19002v4", "categories": ["cs.IT", "math.IT"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Shudi Weng, Ming Xiao, Chao Ren, Mikael Skoglund"}, {"title": "Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion", "link": "https://arxiv.org/abs/2502.13509", "description": "Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.", "guid": "oai:arXiv.org:2502.13509v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang"}, {"title": "MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models", "link": "https://arxiv.org/abs/2404.04990", "description": "The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters. Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning. To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German. MLaKE aggregates fact chains from Wikipedia across languages and utilizes LLMs to generate questions in both free-form and multiple-choice. We evaluate the multilingual knowledge editing generalization capabilities of existing methods on MLaKE. Existing knowledge editing methods demonstrate higher success rates in English samples compared to other languages. However, their generalization capabilities are limited in multi-language experiments. Notably, existing knowledge editing methods often show relatively high generalization for languages within the same language family compared to languages from different language families. These results underscore the imperative need for advancements in multilingual knowledge editing and we hope MLaKE can serve as a valuable resource for benchmarking and solution development.", "guid": "oai:arXiv.org:2404.04990v3", "categories": ["cs.CL"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Zihao Wei, Jingcheng Deng, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng"}, {"title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "link": "https://arxiv.org/abs/2502.13119", "description": "How should one judge whether a given large language model (LLM) can reliably perform economic reasoning? Most existing LLM benchmarks focus on specific applications and fail to present the model with a rich variety of economic tasks. A notable exception is Raman et al. [2024], who offer an approach for comprehensively benchmarking strategic decision-making; however, this approach fails to address the non-strategic settings prevalent in microeconomics, such as supply-and-demand analysis. We address this gap by taxonomizing microeconomic reasoning into $58$ distinct elements, focusing on the logic of supply and demand, each grounded in up to $10$ distinct domains, $5$ perspectives, and $3$ types. The generation of benchmark data across this combinatorial space is powered by a novel LLM-assisted data generation protocol that we dub auto-STEER, which generates a set of questions by adapting handwritten templates to target new domains and perspectives. Because it offers an automated way of generating fresh questions, auto-STEER mitigates the risk that LLMs will be trained to over-fit evaluation benchmarks; we thus hope that it will serve as a useful tool both for evaluating and fine-tuning models for years to come. We demonstrate the usefulness of our benchmark via a case study on $27$ LLMs, ranging from small open-source models to the current state of the art. We examined each model's ability to solve microeconomic problems across our whole taxonomy and present the results across a range of prompting strategies and scoring metrics.", "guid": "oai:arXiv.org:2502.13119v2", "categories": ["cs.CL"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Narun Raman, Taylor Lundy, Thiago Amin, Jesse Perla, Kevin Leyton-Brown"}, {"title": "LLM4Tag: Automatic Tagging System for Information Retrieval via Large Language Models", "link": "https://arxiv.org/abs/2502.13481", "description": "Tagging systems play an essential role in various information retrieval applications such as search engines and recommender systems. Recently, Large Language Models (LLMs) have been applied in tagging systems due to their extensive world knowledge, semantic understanding, and reasoning capabilities. Despite achieving remarkable performance, existing methods still have limitations, including difficulties in retrieving relevant candidate tags comprehensively, challenges in adapting to emerging domain-specific knowledge, and the lack of reliable tag confidence quantification. To address these three limitations above, we propose an automatic tagging system LLM4Tag. First, a graph-based tag recall module is designed to effectively and comprehensively construct a small-scale highly relevant candidate tag set. Subsequently, a knowledge-enhanced tag generation module is employed to generate accurate tags with long-term and short-term knowledge injection. Finally, a tag confidence calibration module is introduced to generate reliable tag confidence scores. Extensive experiments over three large-scale industrial datasets show that LLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag has been deployed online for content tagging to serve hundreds of millions of users.", "guid": "oai:arXiv.org:2502.13481v1", "categories": ["cs.IR"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Ruiming Tang, Chenxu Zhu, Bo Chen, Weipeng Zhang, Menghui Zhu, Xinyi Dai, Huifeng Guo"}, {"title": "An Adaptive Data-Enabled Policy Optimization Approach for Autonomous Bicycle Control", "link": "https://arxiv.org/abs/2502.13676", "description": "This paper presents a unified control framework that integrates a Feedback Linearization (FL) controller in the inner loop with an adaptive Data-Enabled Policy Optimization (DeePO) controller in the outer loop to balance an autonomous bicycle. While the FL controller stabilizes and partially linearizes the inherently unstable and nonlinear system, its performance is compromised by unmodeled dynamics and time-varying characteristics. To overcome these limitations, the DeePO controller is introduced to enhance adaptability and robustness. The initial control policy of DeePO is obtained from a finite set of offline, persistently exciting input and state data. To improve stability and compensate for system nonlinearities and disturbances, a robustness-promoting regularizer refines the initial policy, while the adaptive section of the DeePO framework is enhanced with a forgetting factor to improve adaptation to time-varying dynamics. The proposed DeePO+FL approach is evaluated through simulations and real-world experiments on an instrumented autonomous bicycle. Results demonstrate its superiority over the FL-only approach, achieving more precise tracking of the reference lean angle and lean rate.", "guid": "oai:arXiv.org:2502.13676v1", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "pubdate": "Thu, 20 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Niklas Persson, Feiran Zhao, Mojtaba Kaheni, Florian D\\\"orfler, Alessandro V. Papadopoulos"}]