[{"title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across Enterprise Workflows", "link": "https://arxiv.org/abs/2509.04198", "description": "The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.", "guid": "oai:arXiv.org:2509.04198v1", "categories": ["cs.CY", "cs.MA"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Petr Pr\\r{u}cha, Michaela Matou\\v{s}kov\\'a, Jan Strnad"}, {"title": "Semi-decentralized Federated Time Series Prediction with Client Availability Budgets", "link": "https://arxiv.org/abs/2509.03660", "description": "Federated learning (FL) effectively promotes collaborative training among distributed clients with privacy considerations in the Internet of Things (IoT) scenarios. Despite of data heterogeneity, FL clients may also be constrained by limited energy and availability budgets. Therefore, effective selection of clients participating in training is of vital importance for the convergence of the global model and the balance of client contributions. In this paper, we discuss the performance impact of client availability with time-series data on federated learning. We set up three different scenarios that affect the availability of time-series data and propose FedDeCAB, a novel, semi-decentralized client selection method applying probabilistic rankings of available clients. When a client is disconnected from the server, FedDeCAB allows obtaining partial model parameters from the nearest neighbor clients for joint optimization, improving the performance of offline models and reducing communication overhead. Experiments based on real-world large-scale taxi and vessel trajectory datasets show that FedDeCAB is effective under highly heterogeneous data distribution, limited communication budget, and dynamic client offline or rejoining.", "guid": "oai:arXiv.org:2509.03660v1", "categories": ["cs.LG", "cs.DC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Yunkai Bao, Reza Safarzadeh, Xin Wang, Steve Drew"}, {"title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline Co-Serving", "link": "https://arxiv.org/abs/2410.01228", "description": "Large language model (LLM) serving demands low latency and high throughput, but high load variability makes it challenging to achieve high GPU utilization. In this paper, we identify a synergetic but overlooked opportunity to co-serve latency-critical online requests alongside latency-tolerant offline tasks such as model benchmarking. While promising, existing serving systems fail to co-serve them efficiently, as their coarse-grained resource management at the request or iteration level cannot harvest millisecond-level GPU idle cycles without introducing interference that violates online latency objectives. ConServe is a new LLM co-serving system that achieves high throughput and strong online latency guarantees by managing resources at finer granularities. ConServe introduces three techniques: (1) a latency-aware token-level scheduler that precisely sizes offline batches and tokens to fit within online latency objectives; (2) sub-iteration, layer-wise preemption that allows offline tasks to yield to online load spikes; and (3) incremental KV cache management that enables preempting and resuming offline requests at near-zero cost. Evaluations with Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe delivers an average of 2.2$\\times$ higher throughput and reduces online serving tail latency by 2.9$\\times$ on average compared to state-of-the-art systems.", "guid": "oai:arXiv.org:2410.01228v2", "categories": ["cs.DC", "cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Yifan Qiao, Shu Anzai, Shan Yu, Haoran Ma, Shuo Yang, Yang Wang, Miryung Kim, Yongji Wu, Yang Zhou, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica, Harry Xu"}, {"title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems", "link": "https://arxiv.org/abs/2506.05138", "description": "Recently, federated learning frameworks such as Python TestBed for Federated Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms have emerged to tackle user privacy concerns and efficiency in embedded systems. Even more recently, an efficient federated anomaly detection algorithm, FLiForest, based on Isolation Forests has been developed, offering a low-resource, unsupervised method well-suited for edge deployment and continuous learning. In this paper, we present an application of Isolation Forest-based temperature anomaly detection, developed using the previously mentioned federated learning frameworks, aimed at small edge devices and IoT systems running MicroPython. The system has been experimentally evaluated, achieving over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in detecting anomalies across all tested configurations, while maintaining a memory usage below 160 KB during model training. These results highlight its suitability for resource-constrained environments and edge systems, while upholding federated learning principles of data privacy and collaborative learning.", "guid": "oai:arXiv.org:2506.05138v2", "categories": ["cs.LG", "cs.DC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Pavle Vasiljevic, Milica Matic, Miroslav Popovic"}, {"title": "InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching", "link": "https://arxiv.org/abs/2507.08523", "description": "Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.", "guid": "oai:arXiv.org:2507.08523v2", "categories": ["cs.SE"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng"}, {"title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "link": "https://arxiv.org/abs/2509.03647", "description": "Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from \"self-preference bias\": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.", "guid": "oai:arXiv.org:2509.03647v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dani Roytburg, Matthew Bozoukov, Matthew Nguyen, Jou Barzdukas, Simon Fu, Narmeen Oozeer"}, {"title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning", "link": "https://arxiv.org/abs/2509.04059", "description": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to interpret sheet music is a crucial step toward building AI musicians. However, current research lacks both evaluation benchmarks and training data for sheet music reasoning. To address this, we propose the idea of synthesizing sheet music problems grounded in music theory, which can serve both as evaluation benchmarks and as training data for reinforcement learning with verifiable rewards (RLVR). We introduce a data synthesis framework that generates verifiable sheet music questions in both textual and visual modalities, leading to the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on SSMR-Bench show the importance of models' reasoning abilities in interpreting sheet music. At the same time, the poor performance of Gemini 2.5-Pro highlights the challenges that MLLMs still face in interpreting sheet music in a visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and Qwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the trained Qwen3-8B-Base surpasses GPT-4 in overall performance on MusicTheoryBench and achieves reasoning performance comparable to GPT-4 with the strategies of Role play and Chain-of-Thought. Notably, its performance on math problems also improves relative to the original Qwen3-8B-Base. Furthermore, our results show that the enhanced reasoning ability can also facilitate music composition. In conclusion, we are the first to propose the idea of synthesizing sheet music problems based on music theory rules, and demonstrate its effectiveness not only in advancing model reasoning for sheet music understanding but also in unlocking new possibilities for AI-assisted music creation.", "guid": "oai:arXiv.org:2509.04059v1", "categories": ["cs.CL"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Zhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Haoran Zhang, Runzhe Zhan, Derek F. Wong, Jizhe Zhou, Yu Cheng"}, {"title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue", "link": "https://arxiv.org/abs/2509.04104", "description": "Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.", "guid": "oai:arXiv.org:2509.04104v1", "categories": ["cs.CL", "cs.HC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx"}, {"title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "link": "https://arxiv.org/abs/2508.00454", "description": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the \"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.", "guid": "oai:arXiv.org:2508.00454v2", "categories": ["cs.CL"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding"}, {"title": "Human Motion Video Generation: A Survey", "link": "https://arxiv.org/abs/2509.03883", "description": "Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "guid": "oai:arXiv.org:2509.03883v1", "categories": ["cs.CV", "cs.MM"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu"}, {"title": "FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity", "link": "https://arxiv.org/abs/2509.04107", "description": "Federated Learning (FL) provides decentralised model training, which effectively tackles problems such as distributed data and privacy preservation. However, the generalisation of global models frequently faces challenges from data heterogeneity among clients. This challenge becomes even more pronounced when datasets are limited in size and class imbalance. To address data heterogeneity, we propose a novel method, \\textit{FedQuad}, that explicitly optimises smaller intra-class variance and larger inter-class variance across clients, thereby decreasing the negative impact of model aggregation on the global model over client representations. Our approach minimises the distance between similar pairs while maximising the distance between negative pairs, effectively disentangling client data in the shared feature space. We evaluate our method on the CIFAR-10 and CIFAR-100 datasets under various data distributions and with many clients, demonstrating superior performance compared to existing approaches. Furthermore, we provide a detailed analysis of metric learning-based strategies within both supervised and federated learning paradigms, highlighting their efficacy in addressing representational learning challenges in federated settings.", "guid": "oai:arXiv.org:2509.04107v1", "categories": ["cs.LG", "cs.CV"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Ozgu Goksu, Nicolas Pugeault"}, {"title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across Enterprise Workflows", "link": "https://arxiv.org/abs/2509.04198", "description": "The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.", "guid": "oai:arXiv.org:2509.04198v1", "categories": ["cs.CY", "cs.MA"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Petr Pr\\r{u}cha, Michaela Matou\\v{s}kov\\'a, Jan Strnad"}, {"title": "Computational Basis of LLM's Decision Making in Social Simulation", "link": "https://arxiv.org/abs/2504.11671", "description": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "guid": "oai:arXiv.org:2504.11671v2", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Ji Ma"}, {"title": "Human Motion Video Generation: A Survey", "link": "https://arxiv.org/abs/2509.03883", "description": "Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "guid": "oai:arXiv.org:2509.03883v1", "categories": ["cs.CV", "cs.MM"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu"}, {"title": "Semi-decentralized Federated Time Series Prediction with Client Availability Budgets", "link": "https://arxiv.org/abs/2509.03660", "description": "Federated learning (FL) effectively promotes collaborative training among distributed clients with privacy considerations in the Internet of Things (IoT) scenarios. Despite of data heterogeneity, FL clients may also be constrained by limited energy and availability budgets. Therefore, effective selection of clients participating in training is of vital importance for the convergence of the global model and the balance of client contributions. In this paper, we discuss the performance impact of client availability with time-series data on federated learning. We set up three different scenarios that affect the availability of time-series data and propose FedDeCAB, a novel, semi-decentralized client selection method applying probabilistic rankings of available clients. When a client is disconnected from the server, FedDeCAB allows obtaining partial model parameters from the nearest neighbor clients for joint optimization, improving the performance of offline models and reducing communication overhead. Experiments based on real-world large-scale taxi and vessel trajectory datasets show that FedDeCAB is effective under highly heterogeneous data distribution, limited communication budget, and dynamic client offline or rejoining.", "guid": "oai:arXiv.org:2509.03660v1", "categories": ["cs.LG", "cs.DC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Yunkai Bao, Reza Safarzadeh, Xin Wang, Steve Drew"}, {"title": "From Federated Learning to $\\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks", "link": "https://arxiv.org/abs/2509.03709", "description": "We provide our perspective on $\\mathbb{X}$-Learning ($\\mathbb{X}$L), a novel distributed learning architecture that generalizes and extends the concept of decentralization. Our goal is to present a vision for $\\mathbb{X}$L, introducing its unexplored design considerations and degrees of freedom. To this end, we shed light on the intuitive yet non-trivial connections between $\\mathbb{X}$L, graph theory, and Markov chains. We also present a series of open research directions to stimulate further research.", "guid": "oai:arXiv.org:2509.03709v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Allan Salihovic, Payam Abdisarabshali, Michael Langberg, Seyyedali Hosseinalipour"}, {"title": "FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity", "link": "https://arxiv.org/abs/2509.04107", "description": "Federated Learning (FL) provides decentralised model training, which effectively tackles problems such as distributed data and privacy preservation. However, the generalisation of global models frequently faces challenges from data heterogeneity among clients. This challenge becomes even more pronounced when datasets are limited in size and class imbalance. To address data heterogeneity, we propose a novel method, \\textit{FedQuad}, that explicitly optimises smaller intra-class variance and larger inter-class variance across clients, thereby decreasing the negative impact of model aggregation on the global model over client representations. Our approach minimises the distance between similar pairs while maximising the distance between negative pairs, effectively disentangling client data in the shared feature space. We evaluate our method on the CIFAR-10 and CIFAR-100 datasets under various data distributions and with many clients, demonstrating superior performance compared to existing approaches. Furthermore, we provide a detailed analysis of metric learning-based strategies within both supervised and federated learning paradigms, highlighting their efficacy in addressing representational learning challenges in federated settings.", "guid": "oai:arXiv.org:2509.04107v1", "categories": ["cs.LG", "cs.CV"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Ozgu Goksu, Nicolas Pugeault"}, {"title": "Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit Objectives and Privacy Budget Allocation", "link": "https://arxiv.org/abs/2509.04232", "description": "Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially private deep learning by injecting noise into partitioned gradient vectors. However, existing methods often rely on heuristic noise allocation strategies, lacking a rigorous understanding of their theoretical grounding in connecting noise allocation to formal privacy-utility tradeoffs. In this paper, we present a unified analytical framework that systematically connects layer-wise noise injection strategies with their implicit optimization objectives and associated privacy budget allocations. Our analysis reveals that several existing approaches optimize ill-posed objectives -- either ignoring inter-layer signal-to-noise ratio (SNR) consistency or leading to inefficient use of the privacy budget. In response, we propose a SNR-Consistent noise allocation strategy that unifies both aspects, yielding a noise allocation scheme that achieves better signal preservation and more efficient privacy budget utilization. Extensive experiments in both centralized and federated learning settings demonstrate that our method consistently outperforms existing allocation strategies, achieving better privacy-utility tradeoffs. Our framework not only offers diagnostic insights into prior methods but also provides theoretical guidance for designing adaptive and effective noise injection schemes in deep models.", "guid": "oai:arXiv.org:2509.04232v1", "categories": ["cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Qifeng Tan (Xi'an Jiaotong University), Shusen Yang (Xi'an Jiaotong University), Xuebin Ren (Xi'an Jiaotong University), Yikai Zhang (Xi'an Jiaotong University)"}, {"title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "link": "https://arxiv.org/abs/2509.03647", "description": "Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from \"self-preference bias\": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.", "guid": "oai:arXiv.org:2509.03647v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dani Roytburg, Matthew Bozoukov, Matthew Nguyen, Jou Barzdukas, Simon Fu, Narmeen Oozeer"}, {"title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching", "link": "https://arxiv.org/abs/2501.12689", "description": "Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 70% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge transfer among requests. However, naively caching and reusing past responses leads to a big quality drop. In this paper, we introduce IC-Cache, a caching system that enables live LLM capability augmentation to improve serving efficiency: by leveraging historical request-response pairs from larger models as in-context examples, IC-Cache empowers small LLMs to imitate and even exceed the compositional abilities (e.g., reasoning) of their larger counterparts, enabling selective offloading of requests to reduce cost and latency. Achieving this live augmentation at scale introduces intricate trade-offs between response quality, latency, and system throughput. For a new request, IC-Cache efficiently selects similar, high-utility examples to prepend them to the new request's input. At scale, it adaptively routes requests across LLMs of varying capabilities, accounting for response quality and serving loads. IC-Cache employs a cost-aware cache replay mechanism that refines example quality offline to maximize online cache utility and efficiency. Evaluations on millions of realistic requests demonstrate that IC-Cache improves LLM serving throughput by 1.4-5.9x and reduces latency by 28-71% without hurting response quality.", "guid": "oai:arXiv.org:2501.12689v3", "categories": ["cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Yifan Yu, Yu Gan, Nikhil Sarda, Lillian Tsai, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler"}, {"title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems", "link": "https://arxiv.org/abs/2506.05138", "description": "Recently, federated learning frameworks such as Python TestBed for Federated Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms have emerged to tackle user privacy concerns and efficiency in embedded systems. Even more recently, an efficient federated anomaly detection algorithm, FLiForest, based on Isolation Forests has been developed, offering a low-resource, unsupervised method well-suited for edge deployment and continuous learning. In this paper, we present an application of Isolation Forest-based temperature anomaly detection, developed using the previously mentioned federated learning frameworks, aimed at small edge devices and IoT systems running MicroPython. The system has been experimentally evaluated, achieving over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in detecting anomalies across all tested configurations, while maintaining a memory usage below 160 KB during model training. These results highlight its suitability for resource-constrained environments and edge systems, while upholding federated learning principles of data privacy and collaborative learning.", "guid": "oai:arXiv.org:2506.05138v2", "categories": ["cs.LG", "cs.DC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Pavle Vasiljevic, Milica Matic, Miroslav Popovic"}, {"title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline Co-Serving", "link": "https://arxiv.org/abs/2410.01228", "description": "Large language model (LLM) serving demands low latency and high throughput, but high load variability makes it challenging to achieve high GPU utilization. In this paper, we identify a synergetic but overlooked opportunity to co-serve latency-critical online requests alongside latency-tolerant offline tasks such as model benchmarking. While promising, existing serving systems fail to co-serve them efficiently, as their coarse-grained resource management at the request or iteration level cannot harvest millisecond-level GPU idle cycles without introducing interference that violates online latency objectives. ConServe is a new LLM co-serving system that achieves high throughput and strong online latency guarantees by managing resources at finer granularities. ConServe introduces three techniques: (1) a latency-aware token-level scheduler that precisely sizes offline batches and tokens to fit within online latency objectives; (2) sub-iteration, layer-wise preemption that allows offline tasks to yield to online load spikes; and (3) incremental KV cache management that enables preempting and resuming offline requests at near-zero cost. Evaluations with Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe delivers an average of 2.2$\\times$ higher throughput and reduces online serving tail latency by 2.9$\\times$ on average compared to state-of-the-art systems.", "guid": "oai:arXiv.org:2410.01228v2", "categories": ["cs.DC", "cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Yifan Qiao, Shu Anzai, Shan Yu, Haoran Ma, Shuo Yang, Yang Wang, Miryung Kim, Yongji Wu, Yang Zhou, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica, Harry Xu"}, {"title": "Computational Basis of LLM's Decision Making in Social Simulation", "link": "https://arxiv.org/abs/2504.11671", "description": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "guid": "oai:arXiv.org:2504.11671v2", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Ji Ma"}, {"title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue", "link": "https://arxiv.org/abs/2509.04104", "description": "Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.", "guid": "oai:arXiv.org:2509.04104v1", "categories": ["cs.CL", "cs.HC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx"}, {"title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "link": "https://arxiv.org/abs/2509.03736", "description": "The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.", "guid": "oai:arXiv.org:2509.03736v1", "categories": ["cs.AI"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "James Mooney, Josef Woldense, Zheng Robert Jia, Shirley Anugrah Hayati, My Ha Nguyen, Vipul Raheja, Dongyeop Kang"}, {"title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "link": "https://arxiv.org/abs/2509.03647", "description": "Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from \"self-preference bias\": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.", "guid": "oai:arXiv.org:2509.03647v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Dani Roytburg, Matthew Bozoukov, Matthew Nguyen, Jou Barzdukas, Simon Fu, Narmeen Oozeer"}, {"title": "From Federated Learning to $\\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks", "link": "https://arxiv.org/abs/2509.03709", "description": "We provide our perspective on $\\mathbb{X}$-Learning ($\\mathbb{X}$L), a novel distributed learning architecture that generalizes and extends the concept of decentralization. Our goal is to present a vision for $\\mathbb{X}$L, introducing its unexplored design considerations and degrees of freedom. To this end, we shed light on the intuitive yet non-trivial connections between $\\mathbb{X}$L, graph theory, and Markov chains. We also present a series of open research directions to stimulate further research.", "guid": "oai:arXiv.org:2509.03709v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Allan Salihovic, Payam Abdisarabshali, Michael Langberg, Seyyedali Hosseinalipour"}, {"title": "Computational Basis of LLM's Decision Making in Social Simulation", "link": "https://arxiv.org/abs/2504.11671", "description": "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "guid": "oai:arXiv.org:2504.11671v2", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Ji Ma"}, {"title": "Efficient Item ID Generation for Large-Scale LLM-based Recommendation", "link": "https://arxiv.org/abs/2509.03746", "description": "Integrating product catalogs and user behavior into LLMs can enhance recommendations with broad world knowledge, but the scale of real-world item catalogs, often containing millions of discrete item identifiers (Item IDs), poses a significant challenge. This contrasts with the smaller, tokenized text vocabularies typically used in LLMs. The predominant view within the LLM-based recommendation literature is that it is infeasible to treat item ids as a first class citizen in the LLM and instead some sort of tokenization of an item into multiple tokens is required. However, this creates a key practical bottleneck in serving these models for real-time low-latency applications.\n  Our paper challenges this predominant practice and integrates item ids as first class citizens into the LLM. We provide simple, yet highly effective, novel training and inference modifications that enable single-token representations of items and single-step decoding. Our method shows improvements in recommendation quality (Recall and NDCG) over existing techniques on the Amazon shopping datasets while significantly improving inference efficiency by 5x-14x. Our work offers an efficiency perspective distinct from that of other popular approaches within LLM-based recommendation, potentially inspiring further research and opening up a new direction for integrating IDs into LLMs. Our code is available here https://drive.google.com/file/d/1cUMj37rV0Z1bCWMdhQ6i4q4eTRQLURtC", "guid": "oai:arXiv.org:2509.03746v1", "categories": ["cs.IR"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Anushya Subbiah, Vikram Aggarwal, James Pine, Steffen Rendle, Krishna Sayana, Kun Su"}, {"title": "SmartFLow: A Communication-Efficient SDN Framework for Cross-Silo Federated Learning", "link": "https://arxiv.org/abs/2509.00603", "description": "Cross-silo Federated Learning (FL) enables multiple institutions to collaboratively train machine learning models while preserving data privacy. In such settings, clients repeatedly exchange model weights with a central server, making the overall training time highly sensitive to network performance. However, conventional routing methods often fail to prevent congestion, leading to increased communication latency and prolonged training. Software-Defined Networking (SDN), which provides centralized and programmable control over network resources, offers a promising way to address this limitation. To this end, we propose SmartFLow, an SDN-based framework designed to enhance communication efficiency in cross-silo FL. SmartFLow dynamically adjusts routing paths in response to changing network conditions, thereby reducing congestion and improving synchronization efficiency. Experimental results show that SmartFLow decreases parameter synchronization time by up to 47% compared to shortest-path routing and 41% compared to capacity-aware routing. Furthermore, it achieves these gains with minimal computational overhead and scales effectively to networks of up to 50 clients, demonstrating its practicality for real-world FL deployments.", "guid": "oai:arXiv.org:2509.00603v2", "categories": ["cs.NI"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Osama Abu Hamdan, Hao Che, Engin Arslan, Md Arifuzzaman"}, {"title": "FLEET: A Federated Learning Emulation and Evaluation Testbed for Holistic Research", "link": "https://arxiv.org/abs/2509.00621", "description": "Federated Learning (FL) presents a robust paradigm for privacy-preserving, decentralized machine learning. However, a significant gap persists between the theoretical design of FL algorithms and their practical performance, largely because existing evaluation tools often fail to model realistic operational conditions. Many testbeds oversimplify the critical dynamics among algorithmic efficiency, client-level heterogeneity, and continuously evolving network infrastructure. To address this challenge, we introduce the Federated Learning Emulation and Evaluation Testbed (FLEET). This comprehensive platform provides a scalable and configurable environment by integrating a versatile, framework-agnostic learning component with a high-fidelity network emulator. FLEET supports diverse machine learning frameworks, customizable real-world network topologies, and dynamic background traffic generation. The testbed collects holistic metrics that correlate algorithmic outcomes with detailed network statistics. By unifying the entire experiment configuration, FLEET enables researchers to systematically investigate how network constraints, such as limited bandwidth, high latency, and packet loss, affect the convergence and efficiency of FL algorithms. This work provides the research community with a robust tool to bridge the gap between algorithmic theory and real-world network conditions, promoting the holistic and reproducible evaluation of federated learning systems.", "guid": "oai:arXiv.org:2509.00621v2", "categories": ["cs.NI"], "pubdate": "Fri, 05 Sep 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Osama Abu Hamdan, Hao Che, Engin Arslan, Md Arifuzzaman"}]