[{"title": "Right Reward Right Time for Federated Learning", "link": "https://arxiv.org/abs/2503.07869", "description": "Critical learning periods (CLPs) in federated learning (FL) refer to early stages during which low-quality contributions (e.g., sparse training data availability) can permanently impair the learning performance of the global model owned by the model owner (i.e., the cloud server). However, strategies to motivate clients with high-quality contributions to join the FL training process and share trained model updates during CLPs remain underexplored. Additionally, existing incentive mechanisms in FL treat all training periods equally, which consequently fails to motivate clients to participate early. Compounding this challenge is the cloud's limited knowledge of client training capabilities due to privacy regulations, leading to information asymmetry. Therefore, in this article, we propose a time-aware incentive mechanism, called Right Reward Right Time (R3T), to encourage client involvement, especially during CLPs, to maximize the utility of the cloud in FL. Specifically, the cloud utility function captures the trade-off between the achieved model performance and payments allocated for clients' contributions, while accounting for clients' time and system capabilities, efforts, joining time, and rewards. Then, we analytically derive the optimal contract for the cloud and devise a CLP-aware mechanism to incentivize early participation and efforts while maximizing cloud utility, even under information asymmetry. By providing the right reward at the right time, our approach can attract the highest-quality contributions during CLPs. Simulation and proof-of-concept studies show that R3T increases cloud utility and is more economically effective than benchmarks. Notably, our proof-of-concept results show up to a 47.6% reduction in the total number of clients and up to a 300% improvement in convergence time while reaching competitive test accuracies compared with incentive mechanism benchmarks.", "guid": "oai:arXiv.org:2503.07869v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.GT"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Thanh Linh Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham"}, {"title": "Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing", "link": "https://arxiv.org/abs/2503.07775", "description": "Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic algorithmic framework to estimate the PDF and CDF of any sub-Gaussian distribution while the samples from them arrive in a stream. We compute mergeable summaries of distributions from the stream of samples that require sublinear space w.r.t. the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two sub-Gaussian distributions while samples arrive in streams and from multiple sources (e.g. federated learning). Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities. In addition, we use the proposed estimators of Wasserstein and TV distances to audit the fairness and privacy of the ML algorithms. We empirically demonstrate the efficiency of the algorithms for estimating these distances and auditing using both synthetic and real-world datasets.", "guid": "oai:arXiv.org:2503.07775v1", "categories": ["cs.LG", "cs.CY", "cs.DS", "stat.CO"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Debabrota Basu, Debarshi Chanda"}, {"title": "Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing", "link": "https://arxiv.org/abs/2503.07775", "description": "Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic algorithmic framework to estimate the PDF and CDF of any sub-Gaussian distribution while the samples from them arrive in a stream. We compute mergeable summaries of distributions from the stream of samples that require sublinear space w.r.t. the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two sub-Gaussian distributions while samples arrive in streams and from multiple sources (e.g. federated learning). Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities. In addition, we use the proposed estimators of Wasserstein and TV distances to audit the fairness and privacy of the ML algorithms. We empirically demonstrate the efficiency of the algorithms for estimating these distances and auditing using both synthetic and real-world datasets.", "guid": "oai:arXiv.org:2503.07775v1", "categories": ["cs.LG", "cs.CY", "cs.DS", "stat.CO"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Debabrota Basu, Debarshi Chanda"}, {"title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework", "link": "https://arxiv.org/abs/2503.08461", "description": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ KV-cache compression to reduce memory footprint. However, existing compression methods introduce significant processing overhead and queuing delays, particularly in concurrent serving scenarios. We present \\texttt{FastCache}, a novel serving framework that effectively addresses these challenges through two key innovations: (1) a dynamic batching strategy that optimizes request scheduling across prefill, compression, and decode stages, and (2) an efficient KV-cache memory pool mechanism that eliminates memory fragmentation while maintaining high GPU utilization. Our comprehensive experiments on the GQA and MileBench datasets demonstrate that \\texttt{FastCache} achieves up to 19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$ improvement in throughput compared to state-of-the-art baselines. The system maintains stable performance under high-concurrency scenarios (up to 40 req/s) while reducing average memory consumption by 20\\%. These results establish \\texttt{FastCache} as an efficient solution for real-world LLM serving systems with KV-cache compression.", "guid": "oai:arXiv.org:2503.08461v1", "categories": ["cs.MM", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jianian Zhu, Hang Wu, Haojie Wang, Yinghui Li, Biao Hou, Ruixuan Li, Jidong Zhai"}, {"title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind People", "link": "https://arxiv.org/abs/2503.08221", "description": "We present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,210 videos that record the daily lives of real blind users from a first-person perspective. It also features 4,927 questions directly posed or generated and verified by blind individuals to reflect their needs for visual assistance under various scenarios. We provide each question with an average of 3 reference answers to alleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 15 leading MLLMs and find that all models struggle, with the best performers achieving accuracy around 56\\%, far behind human performance of 87.4\\%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and provide heuristic suggestions for improvement. With these efforts, we hope EgoBlind can serve as a valuable foundation for developing more effective AI assistants to enhance the independence of the blind individuals' lives.", "guid": "oai:arXiv.org:2503.08221v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Junbin Xiao, Nanxin Huang, Hao Qiu, Zhulin Tao, Xun Yang, Richang Hong, Meng Wang, Angela Yao"}, {"title": "AI-native Memory 2.0: Second Me", "link": "https://arxiv.org/abs/2503.08102", "description": "Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.", "guid": "oai:arXiv.org:2503.08102v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jiale Wei, Xiang Ying, Tao Gao, Felix Tao, Jingbo Shang"}, {"title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation", "link": "https://arxiv.org/abs/2503.08537", "description": "While machine learning algorithms have been shown to excel at specific chemical tasks, they have struggled to capture the strategic thinking that characterizes expert chemical reasoning, limiting their widespread adoption. Here we demonstrate that large language models (LLMs) can serve as powerful chemical reasoning engines when integrated with traditional search algorithms, enabling a new approach to computer-aided chemistry that mirrors human expert thinking. Rather than using LLMs to directly manipulate chemical structures, we leverage their ability to evaluate chemical strategies and guide search algorithms toward chemically meaningful solutions. We demonstrate this paradigm through two fundamental challenges: strategy-aware retrosynthetic planning and mechanism elucidation. In retrosynthetic planning, our method allows chemists to specify desired synthetic strategies in natural language to find routes that satisfy these constraints in vast searches. In mechanism elucidation, LLMs guide the search for plausible reaction mechanisms by combining chemical principles with systematic exploration. Our approach shows strong performance across diverse chemical tasks, with larger models demonstrating increasingly sophisticated chemical reasoning. Our approach establishes a new paradigm for computer-aided chemistry that combines the strategic understanding of LLMs with the precision of traditional chemical tools, opening possibilities for more intuitive and powerful chemical reasoning systems.", "guid": "oai:arXiv.org:2503.08537v1", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-sa/4.0/", "creator": "Andres M Bran, Theo A Neukomm, Daniel P Armstrong, Zlatko Jon\\v{c}ev, Philippe Schwaller"}, {"title": "BIPED: Pedagogically Informed Tutoring System for ESL Education", "link": "https://arxiv.org/abs/2406.03486", "description": "Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.", "guid": "oai:arXiv.org:2406.03486v1", "categories": ["cs.CL", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Soonwoo Kwon, Sojung Kim, Minju Park, Seunghyun Lee, Kyuseok Kim"}, {"title": "Right Reward Right Time for Federated Learning", "link": "https://arxiv.org/abs/2503.07869", "description": "Critical learning periods (CLPs) in federated learning (FL) refer to early stages during which low-quality contributions (e.g., sparse training data availability) can permanently impair the learning performance of the global model owned by the model owner (i.e., the cloud server). However, strategies to motivate clients with high-quality contributions to join the FL training process and share trained model updates during CLPs remain underexplored. Additionally, existing incentive mechanisms in FL treat all training periods equally, which consequently fails to motivate clients to participate early. Compounding this challenge is the cloud's limited knowledge of client training capabilities due to privacy regulations, leading to information asymmetry. Therefore, in this article, we propose a time-aware incentive mechanism, called Right Reward Right Time (R3T), to encourage client involvement, especially during CLPs, to maximize the utility of the cloud in FL. Specifically, the cloud utility function captures the trade-off between the achieved model performance and payments allocated for clients' contributions, while accounting for clients' time and system capabilities, efforts, joining time, and rewards. Then, we analytically derive the optimal contract for the cloud and devise a CLP-aware mechanism to incentivize early participation and efforts while maximizing cloud utility, even under information asymmetry. By providing the right reward at the right time, our approach can attract the highest-quality contributions during CLPs. Simulation and proof-of-concept studies show that R3T increases cloud utility and is more economically effective than benchmarks. Notably, our proof-of-concept results show up to a 47.6% reduction in the total number of clients and up to a 300% improvement in convergence time while reaching competitive test accuracies compared with incentive mechanism benchmarks.", "guid": "oai:arXiv.org:2503.07869v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.GT"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Thanh Linh Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham"}, {"title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind People", "link": "https://arxiv.org/abs/2503.08221", "description": "We present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,210 videos that record the daily lives of real blind users from a first-person perspective. It also features 4,927 questions directly posed or generated and verified by blind individuals to reflect their needs for visual assistance under various scenarios. We provide each question with an average of 3 reference answers to alleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 15 leading MLLMs and find that all models struggle, with the best performers achieving accuracy around 56\\%, far behind human performance of 87.4\\%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and provide heuristic suggestions for improvement. With these efforts, we hope EgoBlind can serve as a valuable foundation for developing more effective AI assistants to enhance the independence of the blind individuals' lives.", "guid": "oai:arXiv.org:2503.08221v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Junbin Xiao, Nanxin Huang, Hao Qiu, Zhulin Tao, Xun Yang, Richang Hong, Meng Wang, Angela Yao"}, {"title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data", "link": "https://arxiv.org/abs/2503.08325", "description": "Wind farms, typically in high-latitude regions, face a high risk of blade icing. Traditional centralized training methods raise serious privacy concerns. To enhance data privacy in detecting wind turbine blade icing, traditional federated learning (FL) is employed. However, data heterogeneity, resulting from collections across wind farms in varying environmental conditions, impacts the model's optimization capabilities. Moreover, imbalances in wind turbine data lead to models that tend to favor recognizing majority classes, thus neglecting critical icing anomalies. To tackle these challenges, we propose a federated prototype learning model for class-imbalanced data in heterogeneous environments to detect wind turbine blade icing. We also propose a contrastive supervised loss function to address the class imbalance problem. Experiments on real data from 20 turbines across two wind farms show our method outperforms five FL models and five class imbalance methods, with an average improvement of 19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best method, BiFL.", "guid": "oai:arXiv.org:2503.08325v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Lele Qi, Mengna Liu, Xu Cheng, Fan Shi, Xiufeng Liu, Shengyong Chen"}, {"title": "Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)", "link": "https://arxiv.org/abs/2311.10248", "description": "Federated Learning (FL) enables multiple parties to train machine learning models collaboratively without sharing the raw training data. However, the federated nature of FL enables malicious clients to influence a trained model by injecting error model updates via Byzantine or backdoor attacks. To detect malicious model updates, a typical approach is to measure the distance between each model update and a \\textit{ground-truth model update}. To find such \\textit{ground-truth model updates}, existing defenses either require a benign root dataset on the server (e.g., FLTrust) or simply use trimmed mean or median as the threshold for clipping (e.g., FLAME). However, such benign root datasets are impractical, and the trimmed mean or median may also eliminate contributions from these underrepresented datasets.\n  In this paper, we propose a generic solution, namely FedTruth, to defend against model poisoning attacks in FL, where the \\textit{ground-truth model update} (i.e., the global model update) will be estimated among all the model updates with dynamic aggregation weights. Specifically, FedTruth does not have specific assumptions on the benign or malicious data distribution or access to a benign root dataset. Moreover, FedTruth considers the potential contributions from all benign clients. Our empirical results show that FedTruth can reduce the impacts of poisoned model updates against both Byzantine and backdoor attacks, and is also efficient in large-scale FL systems.", "guid": "oai:arXiv.org:2311.10248v2", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Sheldon C. Ebron, Meiying Zhang, Kan Yang"}, {"title": "V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM", "link": "https://arxiv.org/abs/2411.00915", "description": "Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype VaLoRA on five popular vision tasks on three LMMs. Experiment results reveal that VaLoRA improves 24-62% of the accuracy compared to the original LMMs and reduces 20-89% of the latency compared to the state-of-the-art LoRA model serving systems.", "guid": "oai:arXiv.org:2411.00915v3", "categories": ["cs.CV", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang, Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen, Yunxin Liu"}, {"title": "PyGen: A Collaborative Human-AI Approach to Python Package Creation", "link": "https://arxiv.org/abs/2411.08932", "description": "The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. The findings of our work show that Pygen considerably enhances the researcher's productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the user's package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.\n  Our code and generated examples are open-sourced at [https://github.com/GitsSaikat/Pygen]", "guid": "oai:arXiv.org:2411.08932v3", "categories": ["cs.SE", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehenaz Khaled, Md. Shohrab Hossain"}, {"title": "Rationalization Models for Text-to-SQL", "link": "https://arxiv.org/abs/2502.06759", "description": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "guid": "oai:arXiv.org:2502.06759v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian"}, {"title": "Rationalization Models for Text-to-SQL", "link": "https://arxiv.org/abs/2502.06759", "description": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "guid": "oai:arXiv.org:2502.06759v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian"}, {"title": "Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed Resources on Massive Edge Devices", "link": "https://arxiv.org/abs/2503.08223", "description": "The remarkable success of foundation models has been driven by scaling laws, demonstrating that model performance improves predictably with increased training data and model size. However, this scaling trajectory faces two critical challenges: the depletion of high-quality public data, and the prohibitive computational power required for larger models, which have been monopolized by tech giants. These two bottlenecks pose significant obstacles to the further development of AI. In this position paper, we argue that leveraging massive distributed edge devices can break through these barriers. We reveal the vast untapped potential of data and computational resources on massive edge devices, and review recent technical advancements in distributed/federated learning that make this new paradigm viable. Our analysis suggests that by collaborating on edge devices, everyone can participate in training large language models with small edge devices. This paradigm shift towards distributed training on edge has the potential to democratize AI development and foster a more inclusive AI community.", "guid": "oai:arXiv.org:2503.08223v1", "categories": ["cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Tao Shen, Didi Zhu, Ziyu Zhao, Chao Wu, Fei Wu"}, {"title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM Inference", "link": "https://arxiv.org/abs/2503.08311", "description": "Large language models have been widely adopted across different tasks, but their auto-regressive generation nature often leads to inefficient resource utilization during inference. While batching is commonly used to increase throughput, performance gains plateau beyond a certain batch size, especially with smaller models, a phenomenon that existing literature typically explains as a shift to the compute-bound regime. In this paper, through an in-depth GPU-level analysis, we reveal that large-batch inference remains memory-bound, with most GPU compute capabilities underutilized due to DRAM bandwidth saturation as the primary bottleneck. To address this, we propose a Batching Configuration Advisor (BCA) that optimizes memory allocation, reducing GPU memory requirements with minimal impact on throughput. The freed memory and underutilized GPU compute capabilities can then be leveraged by concurrent workloads. Specifically, we use model replication to improve serving throughput and GPU utilization. Our findings challenge conventional assumptions about LLM inference, offering new insights and practical strategies for improving resource utilization, particularly for smaller language models.", "guid": "oai:arXiv.org:2503.08311v1", "categories": ["cs.DC", "cs.LG"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Pol G. Recasens, Ferran Agullo, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Jordi Torres, Josep Ll. Berral"}, {"title": "TokenSim: Enabling Hardware and Software Exploration for Large Language Model Inference Systems", "link": "https://arxiv.org/abs/2503.08415", "description": "The increasing demand for large language model (LLM) serving has necessitated significant advancements in the optimization and profiling of LLM inference systems. As these models become integral to a wide range of applications, the need for efficient and scalable serving solutions has grown exponentially. This work introduces TokenSim, a comprehensive hardware and software exploration system designed specifically for LLM inference. TokenSim is characterized by its support for extensible system optimizations including scheduling and memory management. We validate the results with systems running with realworld datasets, achieving an error rate of less than 1%. Furthermore, TokenSim facilitates various insightful explorations into the performance and optimization of LLM serving systems.", "guid": "oai:arXiv.org:2503.08415v1", "categories": ["cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Feiyang Wu, Zhuohang Bian, Guoyang Duan, Tianle Xu, Junchi Wu, Teng Ma, Yongqiang Yao, Ruihao Gong, Youwei Zhuo"}, {"title": "Right Reward Right Time for Federated Learning", "link": "https://arxiv.org/abs/2503.07869", "description": "Critical learning periods (CLPs) in federated learning (FL) refer to early stages during which low-quality contributions (e.g., sparse training data availability) can permanently impair the learning performance of the global model owned by the model owner (i.e., the cloud server). However, strategies to motivate clients with high-quality contributions to join the FL training process and share trained model updates during CLPs remain underexplored. Additionally, existing incentive mechanisms in FL treat all training periods equally, which consequently fails to motivate clients to participate early. Compounding this challenge is the cloud's limited knowledge of client training capabilities due to privacy regulations, leading to information asymmetry. Therefore, in this article, we propose a time-aware incentive mechanism, called Right Reward Right Time (R3T), to encourage client involvement, especially during CLPs, to maximize the utility of the cloud in FL. Specifically, the cloud utility function captures the trade-off between the achieved model performance and payments allocated for clients' contributions, while accounting for clients' time and system capabilities, efforts, joining time, and rewards. Then, we analytically derive the optimal contract for the cloud and devise a CLP-aware mechanism to incentivize early participation and efforts while maximizing cloud utility, even under information asymmetry. By providing the right reward at the right time, our approach can attract the highest-quality contributions during CLPs. Simulation and proof-of-concept studies show that R3T increases cloud utility and is more economically effective than benchmarks. Notably, our proof-of-concept results show up to a 47.6% reduction in the total number of clients and up to a 300% improvement in convergence time while reaching competitive test accuracies compared with incentive mechanism benchmarks.", "guid": "oai:arXiv.org:2503.07869v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.GT"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Thanh Linh Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham"}, {"title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection", "link": "https://arxiv.org/abs/2503.07978", "description": "The distributed nature of training makes Federated Learning (FL) vulnerable to backdoor attacks, where malicious model updates aim to compromise the global model's performance on specific tasks. Existing defense methods show limited efficacy as they overlook the inconsistency between benign and malicious model updates regarding both general and fine-grained directions. To fill this gap, we introduce AlignIns, a novel defense method designed to safeguard FL systems against backdoor attacks. AlignIns looks into the direction of each model update through a direction alignment inspection process. Specifically, it examines the alignment of model updates with the overall update direction and analyzes the distribution of the signs of their significant parameters, comparing them with the principle sign across all model updates. Model updates that exhibit an unusual degree of alignment are considered malicious and thus be filtered out. We provide the theoretical analysis of the robustness of AlignIns and its propagation error in FL. Our empirical results on both independent and identically distributed (IID) and non-IID datasets demonstrate that AlignIns achieves higher robustness compared to the state-of-the-art defense methods. The code is available at https://github.com/JiiahaoXU/AlignIns.", "guid": "oai:arXiv.org:2503.07978v1", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jiahao Xu, Zikai Zhang, Rui Hu"}, {"title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework", "link": "https://arxiv.org/abs/2503.08461", "description": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ KV-cache compression to reduce memory footprint. However, existing compression methods introduce significant processing overhead and queuing delays, particularly in concurrent serving scenarios. We present \\texttt{FastCache}, a novel serving framework that effectively addresses these challenges through two key innovations: (1) a dynamic batching strategy that optimizes request scheduling across prefill, compression, and decode stages, and (2) an efficient KV-cache memory pool mechanism that eliminates memory fragmentation while maintaining high GPU utilization. Our comprehensive experiments on the GQA and MileBench datasets demonstrate that \\texttt{FastCache} achieves up to 19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$ improvement in throughput compared to state-of-the-art baselines. The system maintains stable performance under high-concurrency scenarios (up to 40 req/s) while reducing average memory consumption by 20\\%. These results establish \\texttt{FastCache} as an efficient solution for real-world LLM serving systems with KV-cache compression.", "guid": "oai:arXiv.org:2503.08461v1", "categories": ["cs.MM", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jianian Zhu, Hang Wu, Haojie Wang, Yinghui Li, Biao Hou, Ruixuan Li, Jidong Zhai"}, {"title": "Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)", "link": "https://arxiv.org/abs/2311.10248", "description": "Federated Learning (FL) enables multiple parties to train machine learning models collaboratively without sharing the raw training data. However, the federated nature of FL enables malicious clients to influence a trained model by injecting error model updates via Byzantine or backdoor attacks. To detect malicious model updates, a typical approach is to measure the distance between each model update and a \\textit{ground-truth model update}. To find such \\textit{ground-truth model updates}, existing defenses either require a benign root dataset on the server (e.g., FLTrust) or simply use trimmed mean or median as the threshold for clipping (e.g., FLAME). However, such benign root datasets are impractical, and the trimmed mean or median may also eliminate contributions from these underrepresented datasets.\n  In this paper, we propose a generic solution, namely FedTruth, to defend against model poisoning attacks in FL, where the \\textit{ground-truth model update} (i.e., the global model update) will be estimated among all the model updates with dynamic aggregation weights. Specifically, FedTruth does not have specific assumptions on the benign or malicious data distribution or access to a benign root dataset. Moreover, FedTruth considers the potential contributions from all benign clients. Our empirical results show that FedTruth can reduce the impacts of poisoned model updates against both Byzantine and backdoor attacks, and is also efficient in large-scale FL systems.", "guid": "oai:arXiv.org:2311.10248v2", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Sheldon C. Ebron, Meiying Zhang, Kan Yang"}, {"title": "Harnessing Increased Client Participation with Cohort-Parallel Federated Learning", "link": "https://arxiv.org/abs/2405.15644", "description": "Federated learning (FL) is a machine learning approach where nodes collaboratively train a global model. As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes. In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts. We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using knowledge distillation. The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate. Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute resources. Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9x reduction in train time and a 1.3x reduction in resource usage, with a minimal drop in test accuracy.", "guid": "oai:arXiv.org:2405.15644v2", "categories": ["cs.LG", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires, Martijn de Vos"}, {"title": "Mobility-Aware Decentralized Federated Learning with Joint Optimization of Local Iteration and Leader Selection for Vehicular Networks", "link": "https://arxiv.org/abs/2503.06443", "description": "Federated learning (FL) emerges as a promising approach to empower vehicular networks, composed by intelligent connected vehicles equipped with advanced sensing, computing, and communication capabilities. While previous studies have explored the application of FL in vehicular networks, they have largely overlooked the intricate challenges arising from the mobility of vehicles and resource constraints. In this paper, we propose a framework of mobility-aware decentralized federated learning (MDFL) for vehicular networks. In this framework, nearby vehicles train an FL model collaboratively, yet in a decentralized manner. We formulate a local iteration and leader selection joint optimization problem (LSOP) to improve the training efficiency of MDFL. For problem solving, we first reformulate LSOP as a decentralized partially observable Markov decision process (Dec-POMDP), and then develop an effective optimization algorithm based on multi-agent proximal policy optimization (MAPPO) to solve Dec-POMDP. Finally, we verify the performance of the proposed algorithm by comparing it with other algorithms.", "guid": "oai:arXiv.org:2503.06443v2", "categories": ["cs.NI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Dongyu Chen, Tao Deng, Juncheng Jia, Siwei Feng, Di Yuan"}, {"title": "Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing", "link": "https://arxiv.org/abs/2503.07775", "description": "Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic algorithmic framework to estimate the PDF and CDF of any sub-Gaussian distribution while the samples from them arrive in a stream. We compute mergeable summaries of distributions from the stream of samples that require sublinear space w.r.t. the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two sub-Gaussian distributions while samples arrive in streams and from multiple sources (e.g. federated learning). Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities. In addition, we use the proposed estimators of Wasserstein and TV distances to audit the fairness and privacy of the ML algorithms. We empirically demonstrate the efficiency of the algorithms for estimating these distances and auditing using both synthetic and real-world datasets.", "guid": "oai:arXiv.org:2503.07775v1", "categories": ["cs.LG", "cs.CY", "cs.DS", "stat.CO"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Debabrota Basu, Debarshi Chanda"}, {"title": "Right Reward Right Time for Federated Learning", "link": "https://arxiv.org/abs/2503.07869", "description": "Critical learning periods (CLPs) in federated learning (FL) refer to early stages during which low-quality contributions (e.g., sparse training data availability) can permanently impair the learning performance of the global model owned by the model owner (i.e., the cloud server). However, strategies to motivate clients with high-quality contributions to join the FL training process and share trained model updates during CLPs remain underexplored. Additionally, existing incentive mechanisms in FL treat all training periods equally, which consequently fails to motivate clients to participate early. Compounding this challenge is the cloud's limited knowledge of client training capabilities due to privacy regulations, leading to information asymmetry. Therefore, in this article, we propose a time-aware incentive mechanism, called Right Reward Right Time (R3T), to encourage client involvement, especially during CLPs, to maximize the utility of the cloud in FL. Specifically, the cloud utility function captures the trade-off between the achieved model performance and payments allocated for clients' contributions, while accounting for clients' time and system capabilities, efforts, joining time, and rewards. Then, we analytically derive the optimal contract for the cloud and devise a CLP-aware mechanism to incentivize early participation and efforts while maximizing cloud utility, even under information asymmetry. By providing the right reward at the right time, our approach can attract the highest-quality contributions during CLPs. Simulation and proof-of-concept studies show that R3T increases cloud utility and is more economically effective than benchmarks. Notably, our proof-of-concept results show up to a 47.6% reduction in the total number of clients and up to a 300% improvement in convergence time while reaching competitive test accuracies compared with incentive mechanism benchmarks.", "guid": "oai:arXiv.org:2503.07869v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.GT"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Thanh Linh Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham"}, {"title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection", "link": "https://arxiv.org/abs/2503.07978", "description": "The distributed nature of training makes Federated Learning (FL) vulnerable to backdoor attacks, where malicious model updates aim to compromise the global model's performance on specific tasks. Existing defense methods show limited efficacy as they overlook the inconsistency between benign and malicious model updates regarding both general and fine-grained directions. To fill this gap, we introduce AlignIns, a novel defense method designed to safeguard FL systems against backdoor attacks. AlignIns looks into the direction of each model update through a direction alignment inspection process. Specifically, it examines the alignment of model updates with the overall update direction and analyzes the distribution of the signs of their significant parameters, comparing them with the principle sign across all model updates. Model updates that exhibit an unusual degree of alignment are considered malicious and thus be filtered out. We provide the theoretical analysis of the robustness of AlignIns and its propagation error in FL. Our empirical results on both independent and identically distributed (IID) and non-IID datasets demonstrate that AlignIns achieves higher robustness compared to the state-of-the-art defense methods. The code is available at https://github.com/JiiahaoXU/AlignIns.", "guid": "oai:arXiv.org:2503.07978v1", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jiahao Xu, Zikai Zhang, Rui Hu"}, {"title": "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated Generative Models", "link": "https://arxiv.org/abs/2503.08085", "description": "Despite recent advancements in federated learning (FL), the integration of generative models into FL has been limited due to challenges such as high communication costs and unstable training in heterogeneous data environments. To address these issues, we propose PRISM, a FL framework tailored for generative models that ensures (i) stable performance in heterogeneous data distributions and (ii) resource efficiency in terms of communication cost and final model size. The key of our method is to search for an optimal stochastic binary mask for a random network rather than updating the model weights, identifying a sparse subnetwork with high generative performance; i.e., a ``strong lottery ticket''. By communicating binary masks in a stochastic manner, PRISM minimizes communication overhead. This approach, combined with the utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic moving average aggregation method (MADA) on the server side, facilitates stable and strong generative capabilities by mitigating local divergence in FL scenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a lightweight model without extra pruning or quantization, making it ideal for environments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and CIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining privacy with minimal communication costs. PRISM is the first to successfully generate images under challenging non-IID and privacy-preserving FL environments on complex datasets, where previous methods have struggled.", "guid": "oai:arXiv.org:2503.08085v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Kyeongkook Seo, Dong-Jun Han, Jaejun Yoo"}, {"title": "Scaling Probabilistic Circuits via Data Partitioning", "link": "https://arxiv.org/abs/2503.08141", "description": "Probabilistic circuits (PCs) enable us to learn joint distributions over a set of random variables and to perform various probabilistic queries in a tractable fashion. Though the tractability property allows PCs to scale beyond non-tractable models such as Bayesian Networks, scaling training and inference of PCs to larger, real-world datasets remains challenging. To remedy the situation, we show how PCs can be learned across multiple machines by recursively partitioning a distributed dataset, thereby unveiling a deep connection between PCs and federated learning (FL). This leads to federated circuits (FCs) -- a novel and flexible federated learning (FL) framework that (1) allows one to scale PCs on distributed learning environments (2) train PCs faster and (3) unifies for the first time horizontal, vertical, and hybrid FL in one framework by re-framing FL as a density estimation problem over distributed datasets. We demonstrate FC's capability to scale PCs on various large-scale datasets. Also, we show FC's versatility in handling horizontal, vertical, and hybrid FL within a unified framework on multiple classification tasks.", "guid": "oai:arXiv.org:2503.08141v1", "categories": ["cs.LG"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jonas Seng, Florian Peter Busch, Pooja Prasad, Devendra Singh Dhami, Martin Mundt, Kristian Kersting"}, {"title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data", "link": "https://arxiv.org/abs/2503.08325", "description": "Wind farms, typically in high-latitude regions, face a high risk of blade icing. Traditional centralized training methods raise serious privacy concerns. To enhance data privacy in detecting wind turbine blade icing, traditional federated learning (FL) is employed. However, data heterogeneity, resulting from collections across wind farms in varying environmental conditions, impacts the model's optimization capabilities. Moreover, imbalances in wind turbine data lead to models that tend to favor recognizing majority classes, thus neglecting critical icing anomalies. To tackle these challenges, we propose a federated prototype learning model for class-imbalanced data in heterogeneous environments to detect wind turbine blade icing. We also propose a contrastive supervised loss function to address the class imbalance problem. Experiments on real data from 20 turbines across two wind farms show our method outperforms five FL models and five class imbalance methods, with an average improvement of 19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best method, BiFL.", "guid": "oai:arXiv.org:2503.08325v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Lele Qi, Mengna Liu, Xu Cheng, Fan Shi, Xiufeng Liu, Shengyong Chen"}, {"title": "Extra Clients at No Extra Cost: Overcome Data Heterogeneity in Federated Learning with Filter Decomposition", "link": "https://arxiv.org/abs/2503.08652", "description": "Data heterogeneity is one of the major challenges in federated learning (FL), which results in substantial client variance and slow convergence. In this study, we propose a novel solution: decomposing a convolutional filter in FL into a linear combination of filter subspace elements, i.e., filter atoms. This simple technique transforms global filter aggregation in FL into aggregating filter atoms and their atom coefficients. The key advantage here involves mathematically generating numerous cross-terms by expanding the product of two weighted sums from filter atom and atom coefficient. These cross-terms effectively emulate many additional latent clients, significantly reducing model variance, which is validated by our theoretical analysis and empirical observation. Furthermore, our method permits different training schemes for filter atoms and atom coefficients for highly adaptive model personalization and communication efficiency. Empirical results on benchmark datasets demonstrate that our filter decomposition technique substantially improves the accuracy of FL methods, confirming its efficacy in addressing data heterogeneity.", "guid": "oai:arXiv.org:2503.08652v1", "categories": ["cs.LG"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Wei Chen, Qiang Qiu"}, {"title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM Inference", "link": "https://arxiv.org/abs/2503.08311", "description": "Large language models have been widely adopted across different tasks, but their auto-regressive generation nature often leads to inefficient resource utilization during inference. While batching is commonly used to increase throughput, performance gains plateau beyond a certain batch size, especially with smaller models, a phenomenon that existing literature typically explains as a shift to the compute-bound regime. In this paper, through an in-depth GPU-level analysis, we reveal that large-batch inference remains memory-bound, with most GPU compute capabilities underutilized due to DRAM bandwidth saturation as the primary bottleneck. To address this, we propose a Batching Configuration Advisor (BCA) that optimizes memory allocation, reducing GPU memory requirements with minimal impact on throughput. The freed memory and underutilized GPU compute capabilities can then be leveraged by concurrent workloads. Specifically, we use model replication to improve serving throughput and GPU utilization. Our findings challenge conventional assumptions about LLM inference, offering new insights and practical strategies for improving resource utilization, particularly for smaller language models.", "guid": "oai:arXiv.org:2503.08311v1", "categories": ["cs.DC", "cs.LG"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Pol G. Recasens, Ferran Agullo, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Jordi Torres, Josep Ll. Berral"}, {"title": "Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)", "link": "https://arxiv.org/abs/2311.10248", "description": "Federated Learning (FL) enables multiple parties to train machine learning models collaboratively without sharing the raw training data. However, the federated nature of FL enables malicious clients to influence a trained model by injecting error model updates via Byzantine or backdoor attacks. To detect malicious model updates, a typical approach is to measure the distance between each model update and a \\textit{ground-truth model update}. To find such \\textit{ground-truth model updates}, existing defenses either require a benign root dataset on the server (e.g., FLTrust) or simply use trimmed mean or median as the threshold for clipping (e.g., FLAME). However, such benign root datasets are impractical, and the trimmed mean or median may also eliminate contributions from these underrepresented datasets.\n  In this paper, we propose a generic solution, namely FedTruth, to defend against model poisoning attacks in FL, where the \\textit{ground-truth model update} (i.e., the global model update) will be estimated among all the model updates with dynamic aggregation weights. Specifically, FedTruth does not have specific assumptions on the benign or malicious data distribution or access to a benign root dataset. Moreover, FedTruth considers the potential contributions from all benign clients. Our empirical results show that FedTruth can reduce the impacts of poisoned model updates against both Byzantine and backdoor attacks, and is also efficient in large-scale FL systems.", "guid": "oai:arXiv.org:2311.10248v2", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Sheldon C. Ebron, Meiying Zhang, Kan Yang"}, {"title": "Harnessing Increased Client Participation with Cohort-Parallel Federated Learning", "link": "https://arxiv.org/abs/2405.15644", "description": "Federated learning (FL) is a machine learning approach where nodes collaboratively train a global model. As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes. In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts. We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using knowledge distillation. The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate. Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute resources. Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9x reduction in train time and a 1.3x reduction in resource usage, with a minimal drop in test accuracy.", "guid": "oai:arXiv.org:2405.15644v2", "categories": ["cs.LG", "cs.DC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires, Martijn de Vos"}, {"title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates", "link": "https://arxiv.org/abs/2503.07216", "description": "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private client parameters are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.", "guid": "oai:arXiv.org:2503.07216v2", "categories": ["cs.LG"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Sangwoo Park, Seanie Lee, Byungjoo Kim, Sung Ju Hwang"}, {"title": "Minimax Estimation for Personalized Federated Learning: An Alternative between FedAvg and Local Training?", "link": "https://arxiv.org/abs/2103.01901", "description": "A widely recognized difficulty in federated learning arises from the statistical heterogeneity among clients: local datasets often originate from distinct yet not entirely unrelated probability distributions, and personalization is, therefore, necessary to achieve optimal results from each individual's perspective. In this paper, we show how the excess risks of personalized federated learning using a smooth, strongly convex loss depend on data heterogeneity from a minimax point of view, with a focus on the FedAvg algorithm (McMahan et al., 2017) and pure local training (i.e., clients solve empirical risk minimization problems on their local datasets without any communication). Our main result reveals an approximate alternative between these two baseline algorithms for federated learning: the former algorithm is minimax rate optimal over a collection of instances when data heterogeneity is small, whereas the latter is minimax rate optimal when data heterogeneity is large, and the threshold is sharp up to a constant.\n  As an implication, our results show that from a worst-case point of view, a dichotomous strategy that makes a choice between the two baseline algorithms is rate-optimal. Another implication is that the popular FedAvg following by local fine tuning strategy is also minimax optimal under additional regularity conditions. Our analysis relies on a new notion of algorithmic stability that takes into account the nature of federated learning.", "guid": "oai:arXiv.org:2103.01901v2", "categories": ["stat.ML", "cs.LG"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Shuxiao Chen, Qinqing Zheng, Qi Long, Weijie J. Su"}, {"title": "PyGen: A Collaborative Human-AI Approach to Python Package Creation", "link": "https://arxiv.org/abs/2411.08932", "description": "The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. The findings of our work show that Pygen considerably enhances the researcher's productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the user's package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.\n  Our code and generated examples are open-sourced at [https://github.com/GitsSaikat/Pygen]", "guid": "oai:arXiv.org:2411.08932v3", "categories": ["cs.SE", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehenaz Khaled, Md. Shohrab Hossain"}, {"title": "AI-native Memory 2.0: Second Me", "link": "https://arxiv.org/abs/2503.08102", "description": "Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.", "guid": "oai:arXiv.org:2503.08102v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jiale Wei, Xiang Ying, Tao Gao, Felix Tao, Jingbo Shang"}, {"title": "AI-native Memory 2.0: Second Me", "link": "https://arxiv.org/abs/2503.08102", "description": "Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.", "guid": "oai:arXiv.org:2503.08102v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Jiale Wei, Xiang Ying, Tao Gao, Felix Tao, Jingbo Shang"}, {"title": "Rationalization Models for Text-to-SQL", "link": "https://arxiv.org/abs/2502.06759", "description": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "guid": "oai:arXiv.org:2502.06759v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian"}, {"title": "A Survey on Wi-Fi Sensing Generalizability: Taxonomy, Techniques, Datasets, and Future Research Prospects", "link": "https://arxiv.org/abs/2503.08008", "description": "Wi-Fi sensing has emerged as a transformative technology that leverages ubiquitous wireless signals to enable a variety of applications ranging from activity and gesture recognition to indoor localization and health monitoring. However, the inherent dependency of Wi-Fi signals on environmental conditions introduces significant generalization challenges,variations in surroundings, human positions, and orientations often lead to inconsistent signal features, impeding robust action recognition. In this survey, we review over 200 studies on Wi-Fi sensing generalization, categorizing them along the entire sensing pipeline: device deployment, signal processing, feature learning, and model deployment. We systematically analyze state-of-the-art techniques, which are employed to mitigate the adverse effects of environmental variability. Moreover, we provide a comprehensive overview of open-source datasets such as Widar3.0, XRF55, and XRFv2, highlighting their unique characteristics and applicability for multimodal fusion and cross-modal tasks. Finally, we discuss emerging research directions, such as multimodal approaches and the integration of large language models,to inspire future advancements in this rapidly evolving field. Our survey aims to serve as a valuable resource for researchers, offering insights into current methodologies, available datasets, and promising avenues for further investigation.", "guid": "oai:arXiv.org:2503.08008v1", "categories": ["cs.CV"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Fei Wang, Tingting Zhang, Bintong Zhao, Libao Xing, Tiantian Wang, Han Ding, Tony Xiao Han"}, {"title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind People", "link": "https://arxiv.org/abs/2503.08221", "description": "We present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,210 videos that record the daily lives of real blind users from a first-person perspective. It also features 4,927 questions directly posed or generated and verified by blind individuals to reflect their needs for visual assistance under various scenarios. We provide each question with an average of 3 reference answers to alleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 15 leading MLLMs and find that all models struggle, with the best performers achieving accuracy around 56\\%, far behind human performance of 87.4\\%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and provide heuristic suggestions for improvement. With these efforts, we hope EgoBlind can serve as a valuable foundation for developing more effective AI assistants to enhance the independence of the blind individuals' lives.", "guid": "oai:arXiv.org:2503.08221v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Junbin Xiao, Nanxin Huang, Hao Qiu, Zhulin Tao, Xun Yang, Richang Hong, Meng Wang, Angela Yao"}, {"title": "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated Generative Models", "link": "https://arxiv.org/abs/2503.08085", "description": "Despite recent advancements in federated learning (FL), the integration of generative models into FL has been limited due to challenges such as high communication costs and unstable training in heterogeneous data environments. To address these issues, we propose PRISM, a FL framework tailored for generative models that ensures (i) stable performance in heterogeneous data distributions and (ii) resource efficiency in terms of communication cost and final model size. The key of our method is to search for an optimal stochastic binary mask for a random network rather than updating the model weights, identifying a sparse subnetwork with high generative performance; i.e., a ``strong lottery ticket''. By communicating binary masks in a stochastic manner, PRISM minimizes communication overhead. This approach, combined with the utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic moving average aggregation method (MADA) on the server side, facilitates stable and strong generative capabilities by mitigating local divergence in FL scenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a lightweight model without extra pruning or quantization, making it ideal for environments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and CIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining privacy with minimal communication costs. PRISM is the first to successfully generate images under challenging non-IID and privacy-preserving FL environments on complex datasets, where previous methods have struggled.", "guid": "oai:arXiv.org:2503.08085v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Kyeongkook Seo, Dong-Jun Han, Jaejun Yoo"}, {"title": "V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM", "link": "https://arxiv.org/abs/2411.00915", "description": "Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype VaLoRA on five popular vision tasks on three LMMs. Experiment results reveal that VaLoRA improves 24-62% of the accuracy compared to the original LMMs and reduces 20-89% of the latency compared to the state-of-the-art LoRA model serving systems.", "guid": "oai:arXiv.org:2411.00915v3", "categories": ["cs.CV", "cs.AI"], "pubdate": "Wed, 12 Mar 2025 00:00:00 -0400", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang, Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen, Yunxin Liu"}]