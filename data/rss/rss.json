[{"title": "Rationalization Models for Text-to-SQL", "link": "https://arxiv.org/abs/2502.06759", "description": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "guid": "oai:arXiv.org:2502.06759v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian"}, {"title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method", "link": "https://arxiv.org/abs/2401.17460", "description": "Cross-device federated learning (FL) is a growing machine learning setting whereby multiple edge devices collaborate to train a model without disclosing their raw data. With the great number of mobile devices participating in more FL applications via the wireless environment, the practical implementation of these applications will be hindered due to the limited uplink capacity of devices, causing critical bottlenecks. In this work, we propose a novel doubly communication-efficient zero-order (ZO) method with a one-point gradient estimator that replaces communicating long vectors with scalar values and that harnesses the nature of the wireless communication channel, overcoming the need to know the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. We then offer a thorough analysis of the proposed zero-order federated learning (ZOFL) framework and prove that our method converges \\textit{almost surely}, which is a novel result in nonconvex ZO optimization. We further prove a convergence rate of $O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the potential of our algorithm with experimental results.", "guid": "oai:arXiv.org:2401.17460v3", "categories": ["cs.LG", "cs.DC", "cs.MA", "math.OC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Elissa Mhanna, Mohamad Assaad"}, {"title": "LLMs Meet Library Evolution: Evaluating Deprecated API Usage in LLM-based Code Completion", "link": "https://arxiv.org/abs/2406.09834", "description": "Large language models (LLMs), pre-trained or fine-tuned on large code corpora, have shown effectiveness in generating code completions. However, in LLM-based code completion, LLMs may struggle to use correct and up-to-date Application Programming Interfaces (APIs) due to the rapid and continuous evolution of libraries. While existing studies have highlighted issues with predicting incorrect APIs, the specific problem of deprecated API usage in LLM-based code completion has not been thoroughly investigated. To address this gap, we conducted the first evaluation study on deprecated API usage in LLM-based code completion. This study involved seven advanced LLMs, 145 API mappings from eight popular Python libraries, and 28,125 completion prompts. The study results reveal the status quo (i.e., API usage plausibility and deprecated usage rate) of deprecated API and replacing API usage in LLM-based code completion from the perspectives of model, prompt, and library, and indicate the root causes behind. Based on these findings, we propose two lightweight fixing approaches, REPLACEAPI and INSERTPROMPT, which can serve as baseline approaches for future research on mitigating deprecated API usage in LLM-based completion. Additionally, we provide implications for future research on integrating library evolution with LLM-driven software development.", "guid": "oai:arXiv.org:2406.09834v3", "categories": ["cs.SE"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, Xin Peng"}, {"title": "PLayer-FL: A Principled Approach to Personalized Layer-wise Cross-Silo Federated Learning", "link": "https://arxiv.org/abs/2502.08829", "description": "Non-identically distributed data is a major challenge in Federated Learning (FL). Personalized FL tackles this by balancing local model adaptation with global model consistency. One variant, partial FL, leverages the observation that early layers learn more transferable features by federating only early layers. However, current partial FL approaches use predetermined, architecture-specific rules to select layers, limiting their applicability. We introduce Principled Layer-wise-FL (PLayer-FL), which uses a novel federation sensitivity metric to identify layers that benefit from federation. This metric, inspired by model pruning, quantifies each layer's contribution to cross-client generalization after the first training epoch, identifying a transition point in the network where the benefits of federation diminish. We first demonstrate that our federation sensitivity metric shows strong correlation with established generalization measures across diverse architectures. Next, we show that PLayer-FL outperforms existing FL algorithms on a range of tasks, also achieving more uniform performance improvements across clients.", "guid": "oai:arXiv.org:2502.08829v1", "categories": ["cs.LG"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Ahmed Elhussein, Gamze G\\\"ursoy"}, {"title": "One-shot Federated Learning Methods: A Practical Guide", "link": "https://arxiv.org/abs/2502.09104", "description": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs). However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an innovative categorization method and analyze the trade-offs of various techniques. Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and insights for future research.", "guid": "oai:arXiv.org:2502.09104v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xiang Liu, Zhenheng Tang, Xia Li, Yijun Song, Sijie Ji, Zemin Liu, Bo Han, Linshan Jiang, Jialin Li"}, {"title": "Vertical Federated Continual Learning via Evolving Prototype Knowledge", "link": "https://arxiv.org/abs/2502.09152", "description": "Vertical Federated Learning (VFL) has garnered significant attention as a privacy-preserving machine learning framework for sample-aligned feature federation. However, traditional VFL approaches do not address the challenges of class and feature continual learning, resulting in catastrophic forgetting of knowledge from previous tasks. To address the above challenge, we propose a novel vertical federated continual learning method, named Vertical Federated Continual Learning via Evolving Prototype Knowledge (V-LETO), which primarily facilitates the transfer of knowledge from previous tasks through the evolution of prototypes. Specifically, we propose an evolving prototype knowledge method, enabling the global model to retain both previous and current task knowledge. Furthermore, we introduce a model optimization technique that mitigates the forgetting of previous task knowledge by restricting updates to specific parameters of the local model, thereby enhancing overall performance. Extensive experiments conducted in both CIL and FIL settings demonstrate that our method, V-LETO, outperforms the other state-of-the-art methods. For example, our method outperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL tasks, respectively. Our code is available at https://anonymous.4open.science/r/V-LETO-0108/README.md.", "guid": "oai:arXiv.org:2502.09152v1", "categories": ["cs.LG", "cs.NE"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Shuo Wang, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu"}, {"title": "Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation: A Stagewise Decision-Making Methodology", "link": "https://arxiv.org/abs/2502.09303", "description": "Federated Learning (FL) offers a pioneering distributed learning paradigm that enables devices/clients to build a shared global model. This global model is obtained through frequent model transmissions between clients and a central server, which may cause high latency, energy consumption, and congestion over backhaul links. To overcome these drawbacks, Hierarchical Federated Learning (HFL) has emerged, which organizes clients into multiple clusters and utilizes edge nodes (e.g., edge servers) for intermediate model aggregations between clients and the central server. Current research on HFL mainly focus on enhancing model accuracy, latency, and energy consumption in scenarios with a stable/fixed set of clients. However, addressing the dynamic availability of clients -- a critical aspect of real-world scenarios -- remains underexplored. This study delves into optimizing client selection and client-to-edge associations in HFL under intermittent client participation so as to minimize overall system costs (i.e., delay and energy), while achieving fast model convergence. We unveil that achieving this goal involves solving a complex NP-hard problem. To tackle this, we propose a stagewise methodology that splits the solution into two stages, referred to as Plan A and Plan B. Plan A focuses on identifying long-term clients with high chance of participation in subsequent model training rounds. Plan B serves as a backup, selecting alternative clients when long-term clients are unavailable during model training rounds. This stagewise methodology offers a fresh perspective on client selection that can enhance both HFL and conventional FL via enabling low-overhead decision-making processes. Through evaluations on MNIST and CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks in terms of model accuracy and system costs.", "guid": "oai:arXiv.org:2502.09303v1", "categories": ["cs.LG", "cs.DC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Minghong Wu, Minghui Liwang, Yuhan Su, Li Li, Seyyedali Hosseinalipour, Xianbin Wang, Huaiyu Dai, Zhenzhen Jiao"}, {"title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method", "link": "https://arxiv.org/abs/2401.17460", "description": "Cross-device federated learning (FL) is a growing machine learning setting whereby multiple edge devices collaborate to train a model without disclosing their raw data. With the great number of mobile devices participating in more FL applications via the wireless environment, the practical implementation of these applications will be hindered due to the limited uplink capacity of devices, causing critical bottlenecks. In this work, we propose a novel doubly communication-efficient zero-order (ZO) method with a one-point gradient estimator that replaces communicating long vectors with scalar values and that harnesses the nature of the wireless communication channel, overcoming the need to know the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. We then offer a thorough analysis of the proposed zero-order federated learning (ZOFL) framework and prove that our method converges \\textit{almost surely}, which is a novel result in nonconvex ZO optimization. We further prove a convergence rate of $O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the potential of our algorithm with experimental results.", "guid": "oai:arXiv.org:2401.17460v3", "categories": ["cs.LG", "cs.DC", "cs.MA", "math.OC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Elissa Mhanna, Mohamad Assaad"}, {"title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models", "link": "https://arxiv.org/abs/2501.13904", "description": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank factorization scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks.", "guid": "oai:arXiv.org:2501.13904v3", "categories": ["cs.LG"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova"}, {"title": "Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed Computing", "link": "https://arxiv.org/abs/2501.17392", "description": "Federated learning (FL) has gained attention as a distributed learning paradigm for its data privacy benefits and accelerated convergence through parallel computation. Traditional FL relies on a server-client (SC) architecture, where a central server coordinates multiple clients to train a global model, but this approach faces scalability challenges due to server communication bottlenecks. To overcome this, the ring-all-reduce (RAR) architecture has been introduced, eliminating the central server and achieving bandwidth optimality. However, the tightly coupled nature of RAR's ring topology exposes it to unique Byzantine attack risks not present in SC-based FL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms remains an open problem. To address this gap, we propose BRACE (Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve both Byzantine robustness and communication efficiency. We provide theoretical guarantees for the convergence of BRACE under Byzantine attacks, demonstrate its bandwidth efficiency, and validate its practical effectiveness through experiments. Our work offers a foundational understanding of Byzantine-robust RAR-based FL design.", "guid": "oai:arXiv.org:2501.17392v2", "categories": ["cs.CR", "cs.LG"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Minghong Fang, Zhuqing Liu, Xuecen Zhao, Jia Liu"}, {"title": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments", "link": "https://arxiv.org/abs/2502.09334", "description": "Recent developments in large language models (LLMs) have demonstrated their remarkable proficiency in a range of tasks. Compared to in-house homogeneous GPU clusters, deploying LLMs in cloud environments with diverse types of GPUs is crucial for addressing the GPU shortage problem and being more cost-effective. However, the diversity of network environments and various GPU types on the cloud bring difficulties to achieving high-performance serving. In this work, we propose ThunderServe, a high-performance and cost-efficient LLM serving system for heterogeneous cloud environments. We introduce a novel scheduling algorithm, which optimizes the deployment plan of LLM serving to accommodate the heterogeneous resource and network bandwidth conditions in cloud environments. Furthermore, we propose a lightweight re-scheduling mechanism, designed to adapt to fluctuating online conditions (e.g., node failures, workload shifts) without the need for costly restarts of ongoing services. Empirical results in both heterogeneous cloud and homogeneous in-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and on average a $1.7\\times$ increase in throughput and achieves up to a 2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines compared with state-of-the-art systems given the same price budget, suggesting opting for cloud services provides a more cost-efficient solution.", "guid": "oai:arXiv.org:2502.09334v1", "categories": ["cs.DC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Youhe Jiang, Fangcheng Fu, Xiaozhe Yao, Taiyi Wang, Bin Cui, Ana Klimovic, Eiko Yoneki"}, {"title": "Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation: A Stagewise Decision-Making Methodology", "link": "https://arxiv.org/abs/2502.09303", "description": "Federated Learning (FL) offers a pioneering distributed learning paradigm that enables devices/clients to build a shared global model. This global model is obtained through frequent model transmissions between clients and a central server, which may cause high latency, energy consumption, and congestion over backhaul links. To overcome these drawbacks, Hierarchical Federated Learning (HFL) has emerged, which organizes clients into multiple clusters and utilizes edge nodes (e.g., edge servers) for intermediate model aggregations between clients and the central server. Current research on HFL mainly focus on enhancing model accuracy, latency, and energy consumption in scenarios with a stable/fixed set of clients. However, addressing the dynamic availability of clients -- a critical aspect of real-world scenarios -- remains underexplored. This study delves into optimizing client selection and client-to-edge associations in HFL under intermittent client participation so as to minimize overall system costs (i.e., delay and energy), while achieving fast model convergence. We unveil that achieving this goal involves solving a complex NP-hard problem. To tackle this, we propose a stagewise methodology that splits the solution into two stages, referred to as Plan A and Plan B. Plan A focuses on identifying long-term clients with high chance of participation in subsequent model training rounds. Plan B serves as a backup, selecting alternative clients when long-term clients are unavailable during model training rounds. This stagewise methodology offers a fresh perspective on client selection that can enhance both HFL and conventional FL via enabling low-overhead decision-making processes. Through evaluations on MNIST and CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks in terms of model accuracy and system costs.", "guid": "oai:arXiv.org:2502.09303v1", "categories": ["cs.LG", "cs.DC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Minghong Wu, Minghui Liwang, Yuhan Su, Li Li, Seyyedali Hosseinalipour, Xianbin Wang, Huaiyu Dai, Zhenzhen Jiao"}, {"title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method", "link": "https://arxiv.org/abs/2401.17460", "description": "Cross-device federated learning (FL) is a growing machine learning setting whereby multiple edge devices collaborate to train a model without disclosing their raw data. With the great number of mobile devices participating in more FL applications via the wireless environment, the practical implementation of these applications will be hindered due to the limited uplink capacity of devices, causing critical bottlenecks. In this work, we propose a novel doubly communication-efficient zero-order (ZO) method with a one-point gradient estimator that replaces communicating long vectors with scalar values and that harnesses the nature of the wireless communication channel, overcoming the need to know the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. We then offer a thorough analysis of the proposed zero-order federated learning (ZOFL) framework and prove that our method converges \\textit{almost surely}, which is a novel result in nonconvex ZO optimization. We further prove a convergence rate of $O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the potential of our algorithm with experimental results.", "guid": "oai:arXiv.org:2401.17460v3", "categories": ["cs.LG", "cs.DC", "cs.MA", "math.OC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Elissa Mhanna, Mohamad Assaad"}, {"title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions", "link": "https://arxiv.org/abs/2502.08657", "description": "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing methods heavily depend on the manual annotation of high-quality positive samples, while contending with issues such as noisy labels and minimal distinctions between preferred and dispreferred response data. However, readily available toxic samples with clear safety distinctions are often filtered out, removing valuable negative references that could aid LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety self-alignment approach that minimizes human supervision by automatically refining positive and toxic samples and performing fine-grained dual instruction tuning. Positive samples are harmless responses, while toxic samples deliberately contain extremely harmful content, serving as a new supervisory signals. Specifically, we utilize LLM itself to iteratively generate and refine training instances by only exploring fewer than 50 human annotations. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance the LLM's safety. The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples. Conversely, the fine-grained UT loss guides the LLM to minimize the output of harmful words based on negative samples at the token-level, thereby guiding the model to decouple safety from effectiveness, directing it toward safer fine-tuning objectives, and increasing the likelihood of generating helpful and reliable content. Experiments on 9 popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for safety alignment, while maintaining comparable levels of helpfulness and usefulness.", "guid": "oai:arXiv.org:2502.08657v1", "categories": ["cs.CL", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Jingxin Xu, Guoshun Nan, Sheng Guan, Sicong Leng, Yilian Liu, Zixiao Wang, Yuyang Ma, Zhili Zhou, Yanzhao Hou, Xiaofeng Tao"}, {"title": "AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society", "link": "https://arxiv.org/abs/2502.08691", "description": "Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.", "guid": "oai:arXiv.org:2502.08691v1", "categories": ["cs.SI", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, Yong Li"}, {"title": "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency Detection in Privacy-Preserving Federated Learning", "link": "https://arxiv.org/abs/2502.08989", "description": "Federated Learning (FL) allows users to collaboratively train a global machine learning model by sharing local model only, without exposing their private data to a central server. This distributed learning is particularly appealing in scenarios where data privacy is crucial, and it has garnered substantial attention from both industry and academia. However, studies have revealed privacy vulnerabilities in FL, where adversaries can potentially infer sensitive information from the shared model parameters. In this paper, we present an efficient masking-based secure aggregation scheme utilizing lightweight cryptographic primitives to mitigate privacy risks. Our scheme offers several advantages over existing methods. First, it requires only a single setup phase for the entire FL training session, significantly reducing communication overhead. Second, it minimizes user-side overhead by eliminating the need for user-to-user interactions, utilizing an intermediate server layer and a lightweight key negotiation method. Third, the scheme is highly resilient to user dropouts, and the users can join at any FL round. Fourth, it can detect and defend against malicious server activities, including recently discovered model inconsistency attacks. Finally, our scheme ensures security in both semi-honest and malicious settings. We provide security analysis to formally prove the robustness of our approach. Furthermore, we implemented an end-to-end prototype of our scheme. We conducted comprehensive experiments and comparisons, which show that it outperforms existing solutions in terms of communication and computation overhead, functionality, and security.", "guid": "oai:arXiv.org:2502.08989v1", "categories": ["cs.CR", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Nazatul H. Sultan, Yan Bo, Yansong Gao, Seyit Camtepe, Arash Mahboubi, Hang Thanh Bui, Aufeef Chauhan, Hamed Aboutorab, Michael Bewong, Praveen Gauravaram, Rafiqul Islam, Sharif Abuadbba"}, {"title": "One-shot Federated Learning Methods: A Practical Guide", "link": "https://arxiv.org/abs/2502.09104", "description": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs). However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an innovative categorization method and analyze the trade-offs of various techniques. Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and insights for future research.", "guid": "oai:arXiv.org:2502.09104v1", "categories": ["cs.LG", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Xiang Liu, Zhenheng Tang, Xia Li, Yijun Song, Sijie Ji, Zemin Liu, Bo Han, Linshan Jiang, Jialin Li"}, {"title": "Federated Data-Driven Kalman Filtering for State Estimation", "link": "https://arxiv.org/abs/2411.05847", "description": "This paper proposes a novel localization framework based on collaborative training or federated learning paradigm, for highly accurate localization of autonomous vehicles. More specifically, we build on the standard approach of KalmanNet, a recurrent neural network aiming to estimate the underlying system uncertainty of traditional Extended Kalman Filtering, and reformulate it by the adapt-then-combine concept to FedKalmanNet. The latter is trained in a distributed manner by a group of vehicles (or clients), with local training datasets consisting of vehicular location and velocity measurements, through a global server aggregation operation. The FedKalmanNet is then used by each vehicle to localize itself, by estimating the associated system uncertainty matrices (i.e, Kalman gain). Our aim is to actually demonstrate the benefits of collaborative training for state estimation in autonomous driving, over collaborative decision-making which requires rich V2X communication resources for measurement exchange and sensor fusion under real-time constraints. An extensive experimental and evaluation study conducted in CARLA autonomous driving simulator highlights the superior performance of FedKalmanNet over state-of-the-art collaborative decision-making approaches, in localizing vehicles without the need of real-time V2X communication.", "guid": "oai:arXiv.org:2411.05847v2", "categories": ["cs.RO", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Nikos Piperigkos, Alexandros Gkillas, Christos Anagnostopoulos, Aris S. Lalos"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/abs/2412.15151", "description": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting a ceiling on the performance of LLMs. To address this issue, we propose a novel paradigm named LANCE (LANguage models as Continuous self-Evolving data engineers) that enables LLMs to train themselves by autonomously generating, cleaning, reviewing, and annotating data with preference information. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction. Through iterative fine-tuning on Qwen2 series models, we validate the effectiveness of LANCE across various tasks, showing that it can maintain high-quality data generation and continuously improve model performance. Across multiple benchmark dimensions, LANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities. Codes are available at: https://github.com/Control-derek/LANCE.", "guid": "oai:arXiv.org:2412.15151v3", "categories": ["cs.CL", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song"}, {"title": "Rationalization Models for Text-to-SQL", "link": "https://arxiv.org/abs/2502.06759", "description": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "guid": "oai:arXiv.org:2502.06759v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian"}, {"title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions", "link": "https://arxiv.org/abs/2502.08657", "description": "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing methods heavily depend on the manual annotation of high-quality positive samples, while contending with issues such as noisy labels and minimal distinctions between preferred and dispreferred response data. However, readily available toxic samples with clear safety distinctions are often filtered out, removing valuable negative references that could aid LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety self-alignment approach that minimizes human supervision by automatically refining positive and toxic samples and performing fine-grained dual instruction tuning. Positive samples are harmless responses, while toxic samples deliberately contain extremely harmful content, serving as a new supervisory signals. Specifically, we utilize LLM itself to iteratively generate and refine training instances by only exploring fewer than 50 human annotations. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance the LLM's safety. The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples. Conversely, the fine-grained UT loss guides the LLM to minimize the output of harmful words based on negative samples at the token-level, thereby guiding the model to decouple safety from effectiveness, directing it toward safer fine-tuning objectives, and increasing the likelihood of generating helpful and reliable content. Experiments on 9 popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for safety alignment, while maintaining comparable levels of helpfulness and usefulness.", "guid": "oai:arXiv.org:2502.08657v1", "categories": ["cs.CL", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Jingxin Xu, Guoshun Nan, Sheng Guan, Sicong Leng, Yilian Liu, Zixiao Wang, Yuyang Ma, Zhili Zhou, Yanzhao Hou, Xiaofeng Tao"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/abs/2412.15151", "description": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting a ceiling on the performance of LLMs. To address this issue, we propose a novel paradigm named LANCE (LANguage models as Continuous self-Evolving data engineers) that enables LLMs to train themselves by autonomously generating, cleaning, reviewing, and annotating data with preference information. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction. Through iterative fine-tuning on Qwen2 series models, we validate the effectiveness of LANCE across various tasks, showing that it can maintain high-quality data generation and continuously improve model performance. Across multiple benchmark dimensions, LANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities. Codes are available at: https://github.com/Control-derek/LANCE.", "guid": "oai:arXiv.org:2412.15151v3", "categories": ["cs.CL", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song"}, {"title": "Rationalization Models for Text-to-SQL", "link": "https://arxiv.org/abs/2502.06759", "description": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "guid": "oai:arXiv.org:2502.06759v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian"}, {"title": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties", "link": "https://arxiv.org/abs/2502.07058", "description": "A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms, such as Booking.com, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland Mandarin). To prove this concept, we constructed a contextually aligned dataset comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs in a sentiment analysis task. Our results show that LLMs consistently underperform in Taiwan Mandarin.", "guid": "oai:arXiv.org:2502.07058v2", "categories": ["cs.CL", "cs.HC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Zixin Tang, Chieh-Yang Huang, Tsung-Chi Li, Ho Yin Sam Ng, Hen-Hsen Huang, Ting-Hao 'Kenneth' Huang"}, {"title": "Salamandra Technical Report", "link": "https://arxiv.org/abs/2502.08489", "description": "This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and safety.With this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models.", "guid": "oai:arXiv.org:2502.08489v2", "categories": ["cs.CL"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Aitor Gonzalez-Agirre, Marc P\\`amies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo, Jos\\'e Javier Saiz, Ferran Espu\\~na, Jaume Prats, Javier Aula-Blasco, Mario Mina, I\\~nigo Pikabea, Adri\\'an Rubio, Alexander Shvets, Anna Sall\\'es, I\\~naki Lacunza, Jorge Palomar, J\\'ulia Falc\\~ao, Luc\\'ia Tormo, Luis Vasquez-Reina, Montserrat Marimon, Oriol Pareras, Valle Ruiz-Fern\\'andez, Marta Villegas"}, {"title": "Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI Suggestion", "link": "https://arxiv.org/abs/2502.09155", "description": "This demo paper presents AirSense-R, a privacy-preserving mobile application that provides real-time, pollution-aware recommendations for points of interest (POIs) in urban environments. By combining real-time air quality monitoring data with user preferences, the proposed system aims to help users make health-conscious decisions about the locations they visit. The application utilizes collaborative filtering for personalized suggestions, and federated learning for privacy protection, and integrates air pollutant readings from AirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland. Additionally, the AirSENCE prediction engine can be employed to detect anomaly readings and interpolate for air quality readings in areas with sparse sensor coverage. This system offers a promising, health-oriented POI recommendation solution that adapts dynamically to current urban air quality conditions while safeguarding user privacy. The code of AirTOWN and a demonstration video is made available at the following repo: https://github.com/AirtownApp/Airtown-Application.git.", "guid": "oai:arXiv.org:2502.09155v1", "categories": ["cs.IR"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Giuseppe Fasano, Yashar Deldjoo, Tommaso di Noia, Bianca Lau, Sina Adham-Khiabani, Eric Morris, Xia Liu, Ganga Chinna Rao Devarapu, Liam O'Faolain"}, {"title": "KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG", "link": "https://arxiv.org/abs/2502.09304", "description": "Graph-RAG constructs a knowledge graph from text chunks to improve retrieval in Large Language Model (LLM)-based question answering. It is particularly useful in domains such as biomedicine, law, and political science, where retrieval often requires multi-hop reasoning over proprietary documents. Some existing Graph-RAG systems construct KNN graphs based on text chunk relevance, but this coarse-grained approach fails to capture entity relationships within texts, leading to sub-par retrieval and generation quality. To address this, recent solutions leverage LLMs to extract entities and relationships from text chunks, constructing triplet-based knowledge graphs. However, this approach incurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small set of key text chunks and leverages an LLM to construct a knowledge graph skeleton. It then builds a text-keyword bipartite graph from all text chunks, serving as a lightweight alternative to a full knowledge graph. During retrieval, KET-RAG searches both structures: it follows the local search strategy of existing Graph-RAG systems on the skeleton while mimicking this search on the bipartite graph to improve retrieval quality. We evaluate eight solutions on two real-world datasets, demonstrating that KET-RAG outperforms all competitors in indexing cost, retrieval effectiveness, and generation quality. Notably, it achieves comparable or superior retrieval quality to Microsoft's Graph-RAG while reducing indexing costs by over an order of magnitude. Additionally, it improves the generation quality by up to 32.4% while lowering indexing costs by around 20%.", "guid": "oai:arXiv.org:2502.09304v1", "categories": ["cs.IR"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-sa/4.0/", "creator": "Yiqian Huang, Shiqi Zhang, Xiaokui Xiao"}, {"title": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice", "link": "https://arxiv.org/abs/2411.03137", "description": "Creative writing is a deeply human craft, yet AI systems using large language models (LLMs) offer the automation of significant parts of the writing process. So why do some creative writers choose to use AI? Through interviews and observed writing sessions with 18 creative writers who already use AI regularly in their writing practice, we find that creative writers are intentional about how they incorporate AI, making many deliberate decisions about when and how to engage AI based on their core values, such as authenticity and craftsmanship. We characterize the interplay between writers' values, their fluid relationships with AI, and specific integration strategies -- ultimately enabling writers to create new AI workflows without compromising their creative values. We provide insight for writing communities, AI developers and future researchers on the importance of supporting transparency of these emerging writing processes and rethinking what AI features can best serve writers.", "guid": "oai:arXiv.org:2411.03137v2", "categories": ["cs.HC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "creator": "Alicia Guo, Shreya Sathyanarayanan, Leijie Wang, Jeffrey Heer, Amy Zhang"}, {"title": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties", "link": "https://arxiv.org/abs/2502.07058", "description": "A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms, such as Booking.com, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland Mandarin). To prove this concept, we constructed a contextually aligned dataset comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs in a sentiment analysis task. Our results show that LLMs consistently underperform in Taiwan Mandarin.", "guid": "oai:arXiv.org:2502.07058v2", "categories": ["cs.CL", "cs.HC"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace-cross", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Zixin Tang, Chieh-Yang Huang, Tsung-Chi Li, Ho Yin Sam Ng, Hen-Hsen Huang, Ting-Hao 'Kenneth' Huang"}, {"title": "Vertical Federated Continual Learning via Evolving Prototype Knowledge", "link": "https://arxiv.org/abs/2502.09152", "description": "Vertical Federated Learning (VFL) has garnered significant attention as a privacy-preserving machine learning framework for sample-aligned feature federation. However, traditional VFL approaches do not address the challenges of class and feature continual learning, resulting in catastrophic forgetting of knowledge from previous tasks. To address the above challenge, we propose a novel vertical federated continual learning method, named Vertical Federated Continual Learning via Evolving Prototype Knowledge (V-LETO), which primarily facilitates the transfer of knowledge from previous tasks through the evolution of prototypes. Specifically, we propose an evolving prototype knowledge method, enabling the global model to retain both previous and current task knowledge. Furthermore, we introduce a model optimization technique that mitigates the forgetting of previous task knowledge by restricting updates to specific parameters of the local model, thereby enhancing overall performance. Extensive experiments conducted in both CIL and FIL settings demonstrate that our method, V-LETO, outperforms the other state-of-the-art methods. For example, our method outperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL tasks, respectively. Our code is available at https://anonymous.4open.science/r/V-LETO-0108/README.md.", "guid": "oai:arXiv.org:2502.09152v1", "categories": ["cs.LG", "cs.NE"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "cross", "rights": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "creator": "Shuo Wang, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu"}, {"title": "Federated Data-Driven Kalman Filtering for State Estimation", "link": "https://arxiv.org/abs/2411.05847", "description": "This paper proposes a novel localization framework based on collaborative training or federated learning paradigm, for highly accurate localization of autonomous vehicles. More specifically, we build on the standard approach of KalmanNet, a recurrent neural network aiming to estimate the underlying system uncertainty of traditional Extended Kalman Filtering, and reformulate it by the adapt-then-combine concept to FedKalmanNet. The latter is trained in a distributed manner by a group of vehicles (or clients), with local training datasets consisting of vehicular location and velocity measurements, through a global server aggregation operation. The FedKalmanNet is then used by each vehicle to localize itself, by estimating the associated system uncertainty matrices (i.e, Kalman gain). Our aim is to actually demonstrate the benefits of collaborative training for state estimation in autonomous driving, over collaborative decision-making which requires rich V2X communication resources for measurement exchange and sensor fusion under real-time constraints. An extensive experimental and evaluation study conducted in CARLA autonomous driving simulator highlights the superior performance of FedKalmanNet over state-of-the-art collaborative decision-making approaches, in localizing vehicles without the need of real-time V2X communication.", "guid": "oai:arXiv.org:2411.05847v2", "categories": ["cs.RO", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "replace", "rights": "http://creativecommons.org/licenses/by/4.0/", "creator": "Nikos Piperigkos, Alexandros Gkillas, Christos Anagnostopoulos, Aris S. Lalos"}, {"title": "AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society", "link": "https://arxiv.org/abs/2502.08691", "description": "Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.", "guid": "oai:arXiv.org:2502.08691v1", "categories": ["cs.SI", "cs.AI"], "pubdate": "Fri, 14 Feb 2025 00:00:00 -0500", "announce_type": "new", "rights": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "creator": "Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, Yong Li"}]